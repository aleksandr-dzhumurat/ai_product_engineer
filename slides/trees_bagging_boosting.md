# References

* [youtube: gradient boosting](https://www.youtube.com/watch?v=G9d2likA-_E)
* [Jupyter Notebook](../jupyter_notebooks/vol_00_pre_requirements_02_machine_learning_classification.ipynb)

# Bagging vs boosting

| Характеристика | Bagging | Boosting |
| --- | --- | --- |
| Модели | Обучаем параллельно, независимые | Последовательно, зависимые |
| Фокус | Снижение variance | Снижение bias |
| Пример | Random Forest | XGBoost, LightGBM |
| Переобучение | Меньше | Может быть, требует регуляризации |
| Стратегия | Усреднение | Взвешивание ошибок |

# Gradient boosting

Основная формула обновления модели

$$F_m(x) = F_{m-1}(x) + \nu \cdot h_m(x, \Theta_m)$$

**где:**
- $F_m(x)$ - предсказание ансамбля после $m$ итераций
- $F_{m-1}(x)$ - предсказание предыдущей итерации
- $\nu$ (или $\eta$) - learning rate (shrinkage parameter)
- $h_m(x, \Theta_m)$ - новое дерево решений с параметрами $\Theta_m$
- $\Theta_m$ - параметры дерева на итерации $m$ (структура + веса листьев)

Почему именно деревья?

- Хорошо аппроксимируют сложные нелинейности
- Не требуют масштабирования признаков
- Устойчивы к выбросам
- Легко регуляризуются (глубина, min samples, learning rate)

| Параметр | Boosting |
| --- | --- |
| Bias | ✅ Сильно уменьшает (по шагам исправляем ошибки) |
| Variance | ⚠ Может увеличивать (модели становятся зависимыми, переобучение возможно) |

Регуляризация в градиентном бустинге — это набор техник, которые предотвращают **переобучение** и улучшают **обобщающую способность** модели. Без регуляризации модель может слишком точно подстроиться под обучающие данные и плохо работать на новых данных.

---

### 1. Shrinkage (Learning Rate, η)

**Самый важный параметр регуляризации!** Каждое новое дерево добавляется к модели не полностью, а с весом η:

$$F_m(x) = F_{m-1}(x) + \eta \cdot h_m(x)$$

где:

- $F_m(x)$ — предсказание после добавления m-го дерева
- $h_m(x)$ — предсказание нового дерева
- $\eta$ — learning rate (скорость обучения)

Как работает?

- **Значение**: обычно $\eta < 1$ (типично 0.01–0.3)
- **Эффект**: замедляет обучение, делая шаги более осторожными
- **Компромисс**:
    - Меньшее η → требуется больше деревьев, но **выше точность**
    - Большее η → быстрее обучение, но **риск переобучения**

**Пример**

Если модель ошибается на 50 единиц:

- При η = 1.0: новое дерево исправит ошибку полностью (+50)
- При η = 0.1: новое дерево исправит только (+5), остальное — следующим деревьям

---

### 2. Ограничения на структуру деревьев

Max Depth (Максимальная глубина)

Обычно: 3–8 уровней

- **Мелкие деревья** (3–5) = сильная регуляризация
    - Предотвращают запоминание шума
    - Используют только важные закономерности
- **Глубокие деревья** (6–10) = слабая регуляризация
    - Могут улавливать сложные взаимодействия
    - Рискуют переобучиться

**Min Samples per Leaf (Минимум объектов в листе)**

Запрещает создавать листья под единичные примеры:

- Если в листе меньше N объектов → разбиение не происходит
- Предотвращает подстройку под выбросы

**Max Features (Подмножество признаков)**

Для каждого дерева используется только часть признаков:

- Добавляет случайность
- Уменьшает корреляцию между деревьями
- Аналогично Random Forest

---

### 3. Stochastic Gradient Boosting (Subsampling)

**Subsampling строк (0.5–0.8)**

Каждое дерево обучается на **случайной подвыборке** данных:

- Если subsample = 0.8 → используется 80% случайных объектов
- **Эффект**: уменьшает дисперсию и ускоряет обучение
- **Аналогия**: как голосование разных экспертов, видевших разные данные

**Subsampling колонок**

Для каждого дерева или каждого разбиения используется подмножество признаков:

- Увеличивает разнообразие деревьев
- Уменьшает переобучение

### 4. Early Stopping (Ранняя остановка)

**Принцип**

Остановка обучения при **отсутствии улучшения** на валидационной выборке:

1. Отслеживается метрика на holdout-выборке
2. Если N итераций подряд метрика не улучшается → обучение останавливается
3. Используется модель с лучшей валидационной метрикой

**Преимущества**

- Автоматически находит оптимальное количество деревьев
- Предотвращает бессмысленное добавление деревьев
- Экономит время обучения

Типичный пайплайн настройки

1. **Начать с консервативных параметров**
    - learning_rate = 0.1
    - max_depth = 3
    - n_estimators = 100
2. **Настроить глубину деревьев**
    - Попробовать 3, 5, 7, 9
    - Выбрать по валидации
3. **Подобрать learning rate**
    - Уменьшить до 0.01–0.05
    - Увеличить n_estimators соответственно
4. **Добавить subsampling**
    - Попробовать 0.6, 0.8, 1.0
    - Для строк и колонок
5. **Точная настройка λ/α** (если XGBoost/LightGBM)
    - Добавить L2: lambda = 1, 10, 100
    - Или L1: alpha = 1, 10, 100

---

## Обучение бустинга

Инициализация (m = 0):

$$F_0(x) = \arg\min_{\gamma} \sum_{i=1}^{n} L(y_i, \gamma)$$

Для MSE: $F_0(x) = \bar{y}$ (среднее значение)

Для итерации m = 1, 2, ..., M:

**Шаг 1:** Вычислить псевдо-остатки (антиградиенты):

$$r_{im} = -\left[\frac{\partial L(y_i, F(x_i))}{\partial F(x_i)}\right]_{F(x)=F_{m-1}(x)}$$

**Шаг 2:** Обучить дерево $h_m(x)$ на $(x_i, r_{im})$

**Шаг 3:** Для каждого листа $j$ найти оптимальный вес $\gamma_{jm}$:

$$\gamma_{jm} = \arg\min_{\gamma} \sum_{x_i \in R_{jm}} L(y_i, F_{m-1}(x_i) + \gamma)$$

где $R_{jm}$ - множество объектов в листе $j$ дерева $m$

**Шаг 4:** Обновить модель:

$$F_m(x) = F_{m-1}(x) + \nu \sum_{j=1}^{J_m} \gamma_{jm} \mathbb{I}(x \in R_{jm})$$

---

## Параметры $\Theta_m$

$$\Theta_m = \{R_{jm}, \gamma_{jm}\}_{j=1}^{J_m}$$

**где:**
- $R_{jm}$ - регионы (листья) дерева $m$
- $\gamma_{jm}$ - веса листьев
- $J_m$ - количество листьев в дереве $m$

Интерпретация:

$\Theta_m$ включает:
1. **Структуру дерева** - правила разбиения, определяющие регионы $R_{jm}$
2. **Веса листьев** - значения $\gamma_{jm}$, которые добавляются к предсказаниям

---

## Для разных функций потерь

### MSE (L2 Loss):

$$L(y, F(x)) = \frac{1}{2}(y - F(x))^2$$

- Антиградиент: $r_{im} = y_i - F_{m-1}(x_i)$
- Вес листа: $\gamma_{jm} = \text{mean}(r_{im})$ для $x_i \in R_{jm}$

### MAE (L1 Loss):

$$L(y, F(x)) = |y - F(x)|$$

- Антиградиент: $r_{im} = \text{sign}(y_i - F_{m-1}(x_i))$
- Вес листа: $\gamma_{jm} = \text{median}(y_i - F_{m-1}(x_i))$ для $x_i \in R_{jm}$

### Log Loss (Бинарная классификация):

$$L(y, F(x)) = -y \log(p) - (1-y)\log(1-p)$$

где $p = \frac{1}{1 + e^{-F(x)}}$

- Антиградиент: $r_{im} = y_i - p_i$

---

## XGBoost формула с регуляризацией

### Оптимизируемая функция:

$$\Theta_m^* = \arg\min_{\Theta} \sum_{i=1}^{n} L(y_i, F_{m-1}(x_i) + h_m(x_i, \Theta)) + \Omega(h_m)$$

### Регуляризация:

$$\Omega(h) = \gamma T + \frac{1}{2}\lambda \sum_{j=1}^{T} \omega_j^2$$

**где:**
- $T$ - количество листьев
- $\omega_j$ - вес листа $j$
- $\gamma$ - штраф за количество листьев (минимальная потеря для split)
- $\lambda$ - L2 регуляризация весов

### Оптимальный вес листа в XGBoost:

$$\omega_j^* = -\frac{\sum_{i \in I_j} g_i}{\sum_{i \in I_j} h_i + \lambda}$$

где:
- $g_i = \frac{\partial L(y_i, F_{m-1}(x_i))}{\partial F_{m-1}(x_i)}$ - первая производная (градиент)
- $h_i = \frac{\partial^2 L(y_i, F_{m-1}(x_i))}{\partial F_{m-1}(x_i)^2}$ - вторая производная (гессиан)
- $I_j$ - множество объектов в листе $j$

---

## Финальная модель

После $M$ итераций:

$$F_M(x) = F_0(x) + \nu \sum_{m=1}^{M} h_m(x, \Theta_m)$$

или в развернутом виде:

$$F_M(x) = F_0(x) + \nu \sum_{m=1}^{M} \sum_{j=1}^{J_m} \gamma_{jm} \mathbb{I}(x \in R_{jm})$$

---

## Пример расчета

### Дано:
- Данные: $y = [10, 20, 15, 25]$
- $F_0(x) = 17.5$ (среднее)
- $\nu = 0.1$
- MSE loss

### Итерация 1:

**Шаг 1:** Остатки
$$r_i = y_i - F_0(x_i) = [-7.5, 2.5, -2.5, 7.5]$$

**Шаг 2:** Обучаем дерево, получаем 2 листа:
- Лист 1: объекты 0,2 → $\gamma_1 = \text{mean}(-7.5, -2.5) = -5$
- Лист 2: объекты 1,3 → $\gamma_2 = \text{mean}(2.5, 7.5) = 5$

**Шаг 3:** Обновление
$$F_1(x) = F_0(x) + 0.1 \times \gamma_j = 17.5 + 0.1 \times \{\text{-5 или 5}\}$$

Результат: $F_1 = [17.0, 18.0, 17.0, 18.0]$

# Bagging & Random forrest

**Bagging vs Random Forest** — что это не одно и то же, есть нюанс.

**Bagging**

Основная идея:

- Берём **одну модель** (например, Decision Tree)
- Строим **множество копий на случайных подвыборках данных с возвращением** (bootstrap sampling)
- Усредняем прогнозы (для регрессии) или берём голосование (для классификации)

Характеристики:

- Модели **обучаются параллельно**
- Каждая модель использует **все признаки**
- Основная цель — **уменьшить variance**
- Bagging **устраняет переобучение**, делает предсказания стабильнее
- Особенно полезно для **нестабильных моделей**, типа деревьев

| Параметр | Bagging |
| --- | --- |
| Bias | ✅ Не меняет сильно (каждая модель по-прежнему слабая) |
| Variance | ✅ Сильно снижает (усреднение уменьшает разброс) |

**Random Forest**

Основная идея:

- **Это Bagging + случайный выбор признаков на каждом сплите дерева**
- Отличие: при построении дерева на каждом узле выбирается **только случайный поднабор признаков** (feature subsampling)

Почему это важно:

- **Устраняет корреляцию между деревьями**
- Деревья в обычном Bagging могут быть сильно похожи, если есть сильные признаки → усреднение не так эффективно
- Random Forest снижает **variance ещё сильнее** за счёт “разнообразия деревьев”

| Характеристика            | Bagging                          | Random Forest                                             |
|---------------------------|----------------------------------|-----------------------------------------------------------|
| Используемые признаки     | Все признаки на каждом узле      | Случайный поднабор признаков на узле                      |
| Разнообразие деревьев     | Меньше                           | Больше                                                    |
| Variance                  | Снижает                          | Снижает сильнее                                          |
| Bias                      | Не меняет сильно                 | Немного выше (из-за ограничения признаков)               |
| Когда использовать        | Любая модель, нестабильная       | Деревья, когда есть сильные коррелирующие признаки       |
