# References

* [youtube: gradient boosting](https://www.youtube.com/watch?v=G9d2likA-_E)
* [Jupyter Notebook](../jupyter_notebooks/vol_00_pre_requirements_02_machine_learning_classification.ipynb)

# Bagging vs boosting

| Характеристика | Bagging | Boosting |
| --- | --- | --- |
| Модели | Обучаем параллельно, независимые | Последовательно, зависимые |
| Фокус | Снижение variance | Снижение bias |
| Пример | Random Forest | XGBoost, LightGBM |
| Переобучение | Меньше | Может быть, требует регуляризации |
| Стратегия | Усреднение | Взвешивание ошибок |

# Gradient boosting

Основная формула обновления модели

$$F_m(x) = F_{m-1}(x) + \nu \cdot h_m(x, \Theta_m)$$

**где:**
- $F_m(x)$ - предсказание ансамбля после $m$ итераций
- $F_{m-1}(x)$ - предсказание предыдущей итерации
- $\nu$ (или $\eta$) - learning rate (shrinkage parameter)
- $h_m(x, \Theta_m)$ - новое дерево решений с параметрами $\Theta_m$
- $\Theta_m$ - параметры дерева на итерации $m$ (структура + веса листьев)

Почему именно деревья?

- Хорошо аппроксимируют сложные нелинейности
- Не требуют масштабирования признаков
- Устойчивы к выбросам
- Легко регуляризуются (глубина, min samples, learning rate)

| Параметр | Boosting |
| --- | --- |
| Bias | ✅ Сильно уменьшает (по шагам исправляем ошибки) |
| Variance | ⚠ Может увеличивать (модели становятся зависимыми, переобучение возможно) |

Регуляризация в градиентном бустинге — это набор техник, которые предотвращают **переобучение** и улучшают **обобщающую способность** модели. Без регуляризации модель может слишком точно подстроиться под обучающие данные и плохо работать на новых данных.

---

### 1. Shrinkage (Learning Rate, η)

**Самый важный параметр регуляризации!** Каждое новое дерево добавляется к модели не полностью, а с весом η:

$$F_m(x) = F_{m-1}(x) + \eta \cdot h_m(x)$$

где:

- $F_m(x)$ — предсказание после добавления m-го дерева
- $h_m(x)$ — предсказание нового дерева
- $\eta$ — learning rate (скорость обучения)

Как работает?

- **Значение**: обычно $\eta < 1$ (типично 0.01–0.3)
- **Эффект**: замедляет обучение, делая шаги более осторожными
- **Компромисс**:
    - Меньшее η → требуется больше деревьев, но **выше точность**
    - Большее η → быстрее обучение, но **риск переобучения**

**Пример**

Если модель ошибается на 50 единиц:

- При η = 1.0: новое дерево исправит ошибку полностью (+50)
- При η = 0.1: новое дерево исправит только (+5), остальное — следующим деревьям

---

### 2. Ограничения на структуру деревьев

Max Depth (Максимальная глубина)

Обычно: 3–8 уровней

- **Мелкие деревья** (3–5) = сильная регуляризация
    - Предотвращают запоминание шума
    - Используют только важные закономерности
- **Глубокие деревья** (6–10) = слабая регуляризация
    - Могут улавливать сложные взаимодействия
    - Рискуют переобучиться

**Min Samples per Leaf (Минимум объектов в листе)**

Запрещает создавать листья под единичные примеры:

- Если в листе меньше N объектов → разбиение не происходит
- Предотвращает подстройку под выбросы

**Max Features (Подмножество признаков)**

Для каждого дерева используется только часть признаков:

- Добавляет случайность
- Уменьшает корреляцию между деревьями
- Аналогично Random Forest

---

### 3. Stochastic Gradient Boosting (Subsampling)

**Subsampling строк (0.5–0.8)**

Каждое дерево обучается на **случайной подвыборке** данных:

- Если subsample = 0.8 → используется 80% случайных объектов
- **Эффект**: уменьшает дисперсию и ускоряет обучение
- **Аналогия**: как голосование разных экспертов, видевших разные данные

**Subsampling колонок**

Для каждого дерева или каждого разбиения используется подмножество признаков:

- Увеличивает разнообразие деревьев
- Уменьшает переобучение

### 4. Early Stopping (Ранняя остановка)

**Принцип**

Остановка обучения при **отсутствии улучшения** на валидационной выборке:

1. Отслеживается метрика на holdout-выборке
2. Если N итераций подряд метрика не улучшается → обучение останавливается
3. Используется модель с лучшей валидационной метрикой

**Преимущества**

- Автоматически находит оптимальное количество деревьев
- Предотвращает бессмысленное добавление деревьев
- Экономит время обучения

Типичный пайплайн настройки

1. **Начать с консервативных параметров**
    - learning_rate = 0.1
    - max_depth = 3
    - n_estimators = 100
2. **Настроить глубину деревьев**
    - Попробовать 3, 5, 7, 9
    - Выбрать по валидации
3. **Подобрать learning rate**
    - Уменьшить до 0.01–0.05
    - Увеличить n_estimators соответственно
4. **Добавить subsampling**
    - Попробовать 0.6, 0.8, 1.0
    - Для строк и колонок
5. **Точная настройка λ/α** (если XGBoost/LightGBM)
    - Добавить L2: lambda = 1, 10, 100
    - Или L1: alpha = 1, 10, 100

---

## Обучение бустинга

Инициализация (m = 0):

$$F_0(x) = \arg\min_{\gamma} \sum_{i=1}^{n} L(y_i, \gamma)$$

Для MSE: $F_0(x) = \bar{y}$ (среднее значение)

Для итерации m = 1, 2, ..., M:

**Шаг 1:** Вычислить псевдо-остатки (антиградиенты):

$$r_{im} = -\left[\frac{\partial L(y_i, F(x_i))}{\partial F(x_i)}\right]_{F(x)=F_{m-1}(x)}$$

**Шаг 2:** Обучить дерево $h_m(x)$ на $(x_i, r_{im})$

**Шаг 3:** Для каждого листа $j$ найти оптимальный вес $\gamma_{jm}$:

$$\gamma_{jm} = \arg\min_{\gamma} \sum_{x_i \in R_{jm}} L(y_i, F_{m-1}(x_i) + \gamma)$$

где $R_{jm}$ - множество объектов в листе $j$ дерева $m$

**Шаг 4:** Обновить модель:

$$F_m(x) = F_{m-1}(x) + \nu \sum_{j=1}^{J_m} \gamma_{jm} \mathbb{I}(x \in R_{jm})$$

---

## Параметры $\Theta_m$

$$\Theta_m = \{R_{jm}, \gamma_{jm}\}_{j=1}^{J_m}$$

**где:**
- $R_{jm}$ - регионы (листья) дерева $m$
- $\gamma_{jm}$ - веса листьев
- $J_m$ - количество листьев в дереве $m$

Интерпретация:

$\Theta_m$ включает:
1. **Структуру дерева** - правила разбиения, определяющие регионы $R_{jm}$
2. **Веса листьев** - значения $\gamma_{jm}$, которые добавляются к предсказаниям

---

## Для разных функций потерь

### MSE (L2 Loss):

$$L(y, F(x)) = \frac{1}{2}(y - F(x))^2$$

- Антиградиент: $r_{im} = y_i - F_{m-1}(x_i)$
- Вес листа: $\gamma_{jm} = \text{mean}(r_{im})$ для $x_i \in R_{jm}$

### MAE (L1 Loss):

$$L(y, F(x)) = |y - F(x)|$$

- Антиградиент: $r_{im} = \text{sign}(y_i - F_{m-1}(x_i))$
- Вес листа: $\gamma_{jm} = \text{median}(y_i - F_{m-1}(x_i))$ для $x_i \in R_{jm}$

### Log Loss (Бинарная классификация):

$$L(y, F(x)) = -y \log(p) - (1-y)\log(1-p)$$

где $p = \frac{1}{1 + e^{-F(x)}}$

- Антиградиент: $r_{im} = y_i - p_i$

---

## XGBoost формула с регуляризацией

### Оптимизируемая функция:

$$\Theta_m^* = \arg\min_{\Theta} \sum_{i=1}^{n} L(y_i, F_{m-1}(x_i) + h_m(x_i, \Theta)) + \Omega(h_m)$$

### Регуляризация:

$$\Omega(h) = \gamma T + \frac{1}{2}\lambda \sum_{j=1}^{T} \omega_j^2$$

**где:**
- $T$ - количество листьев
- $\omega_j$ - вес листа $j$
- $\gamma$ - штраф за количество листьев (минимальная потеря для split)
- $\lambda$ - L2 регуляризация весов

### Оптимальный вес листа в XGBoost:

$$\omega_j^* = -\frac{\sum_{i \in I_j} g_i}{\sum_{i \in I_j} h_i + \lambda}$$

где:
- $g_i = \frac{\partial L(y_i, F_{m-1}(x_i))}{\partial F_{m-1}(x_i)}$ - первая производная (градиент)
- $h_i = \frac{\partial^2 L(y_i, F_{m-1}(x_i))}{\partial F_{m-1}(x_i)^2}$ - вторая производная (гессиан)
- $I_j$ - множество объектов в листе $j$

---

## Финальная модель

После $M$ итераций:

$$F_M(x) = F_0(x) + \nu \sum_{m=1}^{M} h_m(x, \Theta_m)$$

или в развернутом виде:

$$F_M(x) = F_0(x) + \nu \sum_{m=1}^{M} \sum_{j=1}^{J_m} \gamma_{jm} \mathbb{I}(x \in R_{jm})$$

---

## Пример расчета

### Дано:
- Данные: $y = [10, 20, 15, 25]$
- $F_0(x) = 17.5$ (среднее)
- $\nu = 0.1$
- MSE loss

### Итерация 1:

**Шаг 1:** Остатки
$$r_i = y_i - F_0(x_i) = [-7.5, 2.5, -2.5, 7.5]$$

**Шаг 2:** Обучаем дерево, получаем 2 листа:
- Лист 1: объекты 0,2 → $\gamma_1 = \text{mean}(-7.5, -2.5) = -5$
- Лист 2: объекты 1,3 → $\gamma_2 = \text{mean}(2.5, 7.5) = 5$

**Шаг 3:** Обновление
$$F_1(x) = F_0(x) + 0.1 \times \gamma_j = 17.5 + 0.1 \times \{\text{-5 или 5}\}$$

Результат: $F_1 = [17.0, 18.0, 17.0, 18.0]$

# Bagging & Random forrest

**Bagging vs Random Forest** — что это не одно и то же, есть нюанс.

**Bagging**

Основная идея:

- Берём **одну модель** (например, Decision Tree)
- Строим **множество копий на случайных подвыборках данных с возвращением** (bootstrap sampling)
- Усредняем прогнозы (для регрессии) или берём голосование (для классификации)

Характеристики:

- Модели **обучаются параллельно**
- Каждая модель использует **все признаки**
- Основная цель — **уменьшить variance**
- Bagging **устраняет переобучение**, делает предсказания стабильнее
- Особенно полезно для **нестабильных моделей**, типа деревьев

| Параметр | Bagging |
| --- | --- |
| Bias | ✅ Не меняет сильно (каждая модель по-прежнему слабая) |
| Variance | ✅ Сильно снижает (усреднение уменьшает разброс) |

**Random Forest**

Основная идея:

- **Это Bagging + случайный выбор признаков на каждом сплите дерева**
- Отличие: при построении дерева на каждом узле выбирается **только случайный поднабор признаков** (feature subsampling)

Почему это важно:

- **Устраняет корреляцию между деревьями**
- Деревья в обычном Bagging могут быть сильно похожи, если есть сильные признаки → усреднение не так эффективно
- Random Forest снижает **variance ещё сильнее** за счёт “разнообразия деревьев”

| Характеристика            | Bagging                          | Random Forest                                             |
|---------------------------|----------------------------------|-----------------------------------------------------------|
| Используемые признаки     | Все признаки на каждом узле      | Случайный поднабор признаков на узле                      |
| Разнообразие деревьев     | Меньше                           | Больше                                                    |
| Variance                  | Снижает                          | Снижает сильнее                                          |
| Bias                      | Не меняет сильно                 | Немного выше (из-за ограничения признаков)               |
| Когда использовать        | Любая модель, нестабильная       | Деревья, когда есть сильные коррелирующие признаки       |

### Сильная регуляризация (защита от переобучения)

```python
params = {
    'learning_rate': 0.01-0.05,    # малый learning rate
    'max_depth': 3-5,               # мелкие деревья
    'subsample': 0.8-0.9,           # большой subsample
    'colsample_bytree': 0.8-0.9     # большой subsample признаков
}

```

**Когда использовать**: маленькие датасеты, много шума, риск переобучения

### Слабая регуляризация (больше гибкости)

```python
params = {
    'learning_rate': 0.1-0.3,       # большой learning rate
    'max_depth': 6-10,              # глубокие деревья
    'subsample': 0.5-0.7,           # меньший subsample
    'colsample_bytree': 0.6-0.8
}

```

**Когда использовать**: большие датасеты, сложные зависимости, мало шума

Типичный пайплайн настройки

1. **Начать с консервативных параметров**
    - learning_rate = 0.1
    - max_depth = 3
    - n_estimators = 100
2. **Настроить глубину деревьев**
    - Попробовать 3, 5, 7, 9
    - Выбрать по валидации
3. **Подобрать learning rate**
    - Уменьшить до 0.01–0.05
    - Увеличить n_estimators соответственно
4. **Добавить subsampling**
    - Попробовать 0.6, 0.8, 1.0
    - Для строк и колонок
5. **Точная настройка λ/α** (если XGBoost/LightGBM)
    - Добавить L2: lambda = 1, 10, 100
    - Или L1: alpha = 1, 10, 100

---

Задача: предсказать цену квартиры

**Ситуация:**

- Текущее предсказание модели: $F(x) = 100$ тыс. руб
- Реальная цена: $y = 150$ тыс. руб
- Градиент (ошибка): $150 - 100 = 50$ тыс. руб

**В один лист попали 10 квартир** с градиентами:

```
[45, 50, 48, 52, 49, 51, 47, 50, 48, 50]

```

**Вес этого листа:**
$$w = \text{среднее}([45, 50, 48, 52, 49, 51, 47, 50, 48, 50])$$
$$w = 49 \text{ тыс. руб}$$

**Результат:** Новое дерево будет добавлять **+49 тыс. руб** к предсказанию для объектов, попавших в этот лист.

---

**Регуляризация весов листьев:** как работает штраф

Когда мы штрафуем за веса листьев (L2):

$$\text{Штраф} = \lambda \cdot \sum_{j=1}^{T} w_j^2$$

Мы **уменьшаем абсолютные значения весов**, делая предсказания каждого дерева более консервативными.

Эффект на вес

- **Без регуляризации**: $w = 49$
- **С регуляризацией** ($\lambda = 1$): $w \approx 30\text{–}35$

**Зачем?** Это предотвращает слишком **агрессивные корректировки** модели и переобучение.

---

Визуализация дерева

```
        [Корень]
       /        \
   возраст>30   возраст≤30
     /              \
  [Лист 1]       [Лист 2]
   w = +20        w = -15

```

**Интерпретация:**

- Объекты с возрастом > 30 → получат прибавку **+20** к предсказанию
- Объекты с возрастом ≤ 30 → получат вычитание **15** от предсказания

---

**Ключевые выводы**

1. **Регуляризация критична** для предотвращения переобучения в градиентном бустинге
2. **Learning rate** — самый важный параметр регуляризации
3. **Структура деревьев** (глубина, размер листьев) сильно влияет на переобучение
4. **Subsampling** добавляет случайность и уменьшает дисперсию
5. **L1/L2 штрафы** делают веса листьев более консервативными
6. **Early stopping** автоматически находит оптимальное количество деревьев
7. **Вес листа** — это оптимальное значение, минимизирующее функцию потерь для объектов в листе

---

**Совет:** Начинайте с сильной регуляризации и постепенно ослабляйте её, наблюдая за метриками на валидации!

## XGBoost params

XGBoost hyperparameter and its role:

eta (learning rate):

- Controls how much weight is given to new trees when added to the model
- Typically between 0.01-0.3
- Lower values make the model more conservative/robust but require more trees
- Also called 'learning_rate'

gamma (minimum loss reduction):

- Minimum loss reduction required to make a new split
- Controls whether a node should be split further
- Higher values = more conservative splitting
- Helps prevent overfitting by requiring splits to provide meaningful gain
- Default is 0

alpha (L1 regularization):

- Controls L1 regularization on leaf weights
- Similar to Lasso regression
- Higher values = more regularization = simpler model
- Useful for making model sparse and handling high-dimensional data
- Default is 0

lambda (L2 regularization):

- Controls L2 regularization on leaf weights
- Similar to Ridge regression
- Higher values = more regularization = more conservative model
- Helps prevent overfitting by keeping leaf weights small
- Default is 1