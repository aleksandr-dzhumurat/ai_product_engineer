
[![–õ–µ–∫—Ü–∏—è 03 vol 2. –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ Word2Vec Transformers](http://img.youtube.com/vi/csqW3HF_3p8/0.jpg)](http://www.youtube.com/watch?v=csqW3HF_3p8 "–õ–µ–∫—Ü–∏—è 03 vol 2. –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ Word2Vec Transformers")

1Ô∏è‚É£ –ù–∞—á–∞—Ç—å –º–æ–∂–Ω–æ —Å –ø—Ä–∞–∫—Ç–∏–∫–∏ ‚Äî –ø—Ä–æ–¥—É–∫—Ç–æ–≤—ã–µ –∫–µ–π—Å—ã –æ—Ç —Ä–µ–±—è—Ç –∏–∑ FinAI (–¥–µ–ª–∞—é—Ç –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –¥–ª—è –∫–æ–º–∞–Ω–¥—ã —Å–∞–ø–ø–æ—Ä—Ç–∞).
–ë–ª–æ–≥ —Å–≤–µ–∂–∏–π, –º–Ω–æ–≥–æ –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã—Ö –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–π: –≤ –æ—Å–Ω–æ–≤–Ω–æ–º RAG, –Ω–µ–º–Ω–æ–≥–æ –ø—Ä–æ –æ–±—É—á–µ–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö —É–∑–∫–æ—Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö SLM.
–ü–∏—à—É—Ç –æ–±–æ –≤—Å—ë–º ‚Äî –æ—Ç –∏–Ω—Ñ—Ä—ã –¥–æ –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ –ø—Ä–æ–¥—É–∫—Ç–æ–≤—ã—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤.
üëâ fin.ai/research (https://fin.ai/research/)

–î–∞–ª—å—à–µ –∏–¥—É—Ç —É–≥–ª—É–±–ª–µ–Ω–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã

2Ô∏è‚É£ –û—Å–Ω–æ–≤–∞ –∏ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç ‚Äî (https://cme295.stanford.edu/syllabus/) —Å–≤–µ–∂–∏–π –∫—É—Ä—Å CME295 –æ—Ç –°—Ç—ç–Ω—Ñ–æ—Ä–¥–∞.
–ö—É—Ä—Å –ø—Ä–æ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã: –ª–µ–∫—Ü–∏–∏, –≤–∏–¥–æ—Å—ã, –Ω–æ—Ä–º–∞–ª—å–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –º–∞—Ç–µ—Ä–∏–∞–ª–∞.
–î–ª—è YouTube —Ä–æ–ª–∏–∫–æ–≤ —è –∏—Å–ø–æ–ª—å–∑—É—é notebooklm (https://notebooklm.google.com/) ‚Äî —É–¥–æ–±–Ω–æ –≤—ã—Ç–∞—Å–∫–∏–≤–∞—Ç—å –∫–æ–Ω—Å–ø–µ–∫—Ç—ã –∏ –¥–µ–ª–∞—Ç—å –∏–Ω—Ñ–æ–≥—Ä–∞—Ñ–∏–∫—É.

3Ô∏è‚É£ –í–æ–ø—Ä–æ—Å—ã —ç–∫–∑–∞–º–µ–Ω–∞ –ø–æ CME295 (https://cme295.stanford.edu/exams/midterm.pdf) –∏–¥–µ–∞–ª—å–Ω—ã –¥–ª—è —Å–æ–±–µ—Å–æ–≤.
–•–æ—Ä–æ—à–∞—è –ø–æ–¥–±–æ—Ä–∫–∞: –≤—Å—ë —Ä–∞–∑–±–∏—Ç–æ –ø–æ –ø—É–Ω–∫—Ç–∞–º, –ø–æ–∫—Ä—ã–≤–∞–µ—Ç –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ –≤–µ—Å—å –∫—É—Ä—Å. –û—Ç–ª–∏—á–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏.

4Ô∏è‚É£ –ß—Ç–æ–±—ã –ø—Ä–∏–∑–µ–º–ª–∏—Ç—å —Ç–µ–æ—Ä–∏—é ‚Äî –∫—É—Ä—Å –ø–æ LLM (https://huggingface.co/learn/llm-course/en/chapter6/1) –æ—Ç HuggingFace, –æ—Å–æ–±–µ–Ω–Ω–æ –∫–ª–∞—Å—Å–Ω–∞—è —á–∞—Å—Ç—å –ø—Ä–æ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä—ã + –º–Ω–æ–≥–æ –ø—Ä–∞–∫—Ç–∏–∫–∏ –ø—Ä–æ –¥–µ–ø–ª–æ–π –¥–∂–æ–±–æ–≤ –≤ HuggingFace Cloud.

5Ô∏è‚É£ –ï—Å–ª–∏ —Ö–æ—á–µ—Ç—Å—è –ø–æ–±–æ–ª—å—à–µ –∏–Ω–∂–µ–Ω–µ—Ä–Ω—ã—Ö –¥–µ—Ç–∞–ª–µ–π:
* Nebius LLM Engineering (https://github.com/Nebius-Academy/LLM-Engineering-Essentials) ‚Äî —É–ø–æ—Ä –Ω–∞ –º–µ—Ç—Ä–∏–∫–∏
* LLMOps Essential (https://github.com/Nebius-Academy/LLMOps-Essentials) ‚Äî –ø–æ–ª–µ–∑–Ω—ã–π –º–∞—Ç–µ—Ä–∏–∞–ª ( –ø—Ä–∏–∫–æ–ª—å–Ω—ã–µ —à—Ç—É–∫–∏ —Ç–∏–ø–∞ –¥–µ–ø–ª–æ—è –≤ Kubernetes)
* Unsloth [fine-tuning guide](https://unsloth.ai/docs/get-started/fine-tuning-llms-guide)

6Ô∏è‚É£ –ï—Å–ª–∏ –≤–∫–∞—Ç—ã–≤–∞–Ω–∏ –≤ LLM —Ç—è–∂–µ–ª–æ –∏–¥–µ—Ç, –Ω–∞—á–Ω–∏—Ç–µ —Å –±–∞–∑—ã.
–°—Ç—ç–Ω—Ñ–æ—Ä–¥ –≤—ã–ª–æ–∂–∏–ª CS230 ‚Äî –æ—Ç–ª–∏—á–Ω—ã–π —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π –∫—É—Ä—Å –ø–æ DL
* —Å–ª–∞–π–¥—ã (https://cs230.stanford.edu/syllabus/)
* –≤–∏–¥–æ—Å—ã (https://www.youtube.com/playlist?list=PLoROMvodv4rNRRGdS0rBbXOUGA0wjdh1X)

---

–ú–∞—Ç–µ—Ä–∏–∞–ª–∞ —Ö–≤–∞—Ç–∞–µ—Ç –º–∏–Ω–∏–º—É–º –Ω–∞ –ø–∞—Ä—É –Ω–µ–¥–µ–ª—å –ø–ª–æ—Ç–Ω–æ–≥–æ –ø–æ–≥—Ä—É–∂–µ–Ω–∏—è.
–ò–¥–µ–∞–ª—å–Ω–æ –¥–ª—è –Ω–æ–≤–æ–≥–æ–¥–Ω–∏—Ö –∫–∞–Ω–∏–∫—É–ª üéÑ‚ú®


Transformers
* video explanation https://youtu.be/ECR4oAwocjs 
* blog post  https://poloclub.github.io/transformer-explainer/


–û–±—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –Ω–∞ —Å–≤–æ–∏—Ö –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –¥–∞–Ω–Ω—ã—Ö

* –ø–æ—á–µ–º—É —ç—Ç–æ –≤–∞–∂–Ω–æ https://fin.ai/research/finetuning-retrieval-for-fin/
* –∫–∞–∫ –æ–±—É—á–∞—Ç—å https://arxiv.org/abs/2512.21021

# Word2Vec: "bank" –≤—Å–µ–≥–¥–∞ –æ–¥–∏–Ω–∞–∫–æ–≤—ã–π –≤–µ–∫—Ç–æ—Ä
"I went to the bank" ‚Üí [0.23, -0.45, 0.67, ...]
"River bank is beautiful" ‚Üí [0.23, -0.45, 0.67, ...]  # –¢–æ—Ç –∂–µ!

# BERT: "bank" –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
```
"I went to the bank" ‚Üí [0.12, 0.89, -0.34, ...]  # –§–∏–Ω–∞–Ω—Å—ã
"River bank is beautiful" ‚Üí [-0.45, 0.23, 0.91, ...]  # –ë–µ—Ä–µ–≥
```

üìö **–ò—Å—Ç–æ—á–Ω–∏–∫–∏:**
- [Efficient Estimation of Word Representations in Vector Space (Mikolov et al., 2013)](https://arxiv.org/abs/1301.3781) ‚Äî Word2Vec
- [GloVe: Global Vectors for Word Representation (Pennington et al., 2014)](https://nlp.stanford.edu/pubs/glove.pdf)
- [BERT: Pre-training of Deep Bidirectional Transformers (Devlin et al., 2018)](https://arxiv.org/abs/1810.04805)

---

## 3. Transformer Architecture: Encoder vs Decoder

### –§—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ —Ä–∞–∑–ª–∏—á–∏–µ

**BERT (Encoder-only)** vs **GPT (Decoder-only)** ‚Äî —ç—Ç–æ –Ω–µ –ø—Ä–æ—Å—Ç–æ "—Ä–∞–∑–Ω—ã–µ –º–æ–¥–µ–ª–∏", —ç—Ç–æ —Ä–∞–∑–Ω—ã–µ —Ñ–∏–ª–æ—Å–æ—Ñ–∏–∏.

#### BERT: Bidirectional Attention

```
Input: "The cat [MASK] on the mat"

Attention pattern (–≤—Å–µ –≤–∏–¥—è—Ç –≤—Å—ë):
     The  cat  [MASK]  on  the  mat
The   ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚óè
cat   ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚óè
[MASK]‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚óè  ‚Üê –í–∏–¥–∏—Ç –í–ï–°–¨ –∫–æ–Ω—Ç–µ–∫—Å—Ç!
on    ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚óè
the   ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚óè
mat   ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚óè

–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ [MASK] –∏—Å–ø–æ–ª—å–∑—É–µ—Ç:
- –õ–µ–≤—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç: "The cat"
- –ü—Ä–∞–≤—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç: "on the mat"
```

#### GPT: Causal Attention

```
Input: "The cat sits on the"

Attention pattern (—Ç–æ–ª—å–∫–æ –ø—Ä–æ—à–ª–æ–µ):
         The  cat  sits  on   the
The      ‚óè    ‚úó    ‚úó    ‚úó    ‚úó
cat      ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚óè    ‚úó    ‚úó    ‚úó
sits     ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚óè    ‚úó    ‚úó
on       ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚óè    ‚úó
the      ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚óè

–ü—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Å–ª–æ–≤–∞ –≤–∏–¥–∏—Ç —Ç–æ–ª—å–∫–æ –ø—Ä–µ–¥—ã–¥—É—â–∏–µ!
```

**–ö–æ–≥–¥–∞ —á—Ç–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:**

| –ó–∞–¥–∞—á–∞ | BERT | GPT |
|--------|------|-----|
| Classification | ‚úÖ –û—Ç–ª–∏—á–Ω–æ | ‚ö†Ô∏è –ú–æ–∂–Ω–æ |
| NER | ‚úÖ –û—Ç–ª–∏—á–Ω–æ | ‚ùå –ü–ª–æ—Ö–æ |
| Text Generation | ‚ùå –ù–µ–≤–æ–∑–º–æ–∂–Ω–æ | ‚úÖ –û—Ç–ª–∏—á–Ω–æ |
| Q&A (extractive) | ‚úÖ –û—Ç–ª–∏—á–Ω–æ | ‚ö†Ô∏è –ú–æ–∂–Ω–æ |
| Q&A (generative) | ‚ùå | ‚úÖ –û—Ç–ª–∏—á–Ω–æ |

üìö **–ò—Å—Ç–æ—á–Ω–∏–∫–∏:**
- [Attention Is All You Need (Vaswani et al., 2017)](https://arxiv.org/abs/1706.03762) ‚Äî –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π Transformer
- [BERT: Pre-training of Deep Bidirectional Transformers](https://arxiv.org/abs/1810.04805)
- [Improving Language Understanding by Generative Pre-Training (Radford et al., 2018)](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf) ‚Äî GPT-1
- [The Illustrated Transformer by Jay Alammar](http://jalammar.github.io/illustrated-transformer/) ‚Äî –æ—Ç–ª–∏—á–Ω–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è

---

## 4. Attention Mechanisms: Self vs Cross

### –¢—Ä–∏ —Ç–∏–ø–∞ Attention

–ú–Ω–æ–≥–∏–µ –ø—É—Ç–∞—é—Ç bidirectional attention –∏ cross-attention. –≠—Ç–æ **—Ä–∞–∑–Ω—ã–µ** –º–µ—Ö–∞–Ω–∏–∑–º—ã!

#### 1. Self-Attention (Bidirectional) ‚Äî BERT

```
Q, K, V –≤—Å–µ –∏–∑ –û–î–ù–û–ô –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

Input: "The cat sits"
Query –∏–∑: "The cat sits"
Key –∏–∑: "The cat sits"
Value –∏–∑: "The cat sits"
```

#### 2. Self-Attention (Causal) ‚Äî GPT

```
Q, K, V –∏–∑ –æ–¥–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ + causal mask

–¢—Ä–µ—É–≥–æ–ª—å–Ω–∞—è –º–∞—Å–∫–∞ –∑–∞–ø—Ä–µ—â–∞–µ—Ç —Å–º–æ—Ç—Ä–µ—Ç—å –≤ –±—É–¥—É—â–µ–µ
```

#### 3. Cross-Attention ‚Äî Encoder-Decoder

```
Query –∏–∑ DECODER
Key, Value –∏–∑ ENCODER (–¥—Ä—É–≥–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å!)

Encoder input: "The cat sits" ‚Üí Encoder outputs
                                       ‚Üì
Decoder: "Le chat" ‚Üí Cross-Attention ‚Üí —Å–º–æ—Ç—Ä–∏—Ç –Ω–∞ Encoder outputs
```

**–°—Ä–∞–≤–Ω–µ–Ω–∏–µ:**

| –¢–∏–ø | Q –∏–∑ | K, V –∏–∑ | –ú–∞—Å–∫–∞ | –ú–æ–¥–µ–ª–∏ |
|-----|------|---------|-------|--------|
| Bidirectional Self | –¢–æ–π –∂–µ seq | –¢–æ–π –∂–µ seq | –ù–µ—Ç | BERT |
| Causal Self | –¢–æ–π –∂–µ seq | –¢–æ–π –∂–µ seq | –¢—Ä–µ—É–≥–æ–ª—å–Ω–∞—è | GPT |
| Cross | Decoder | Encoder | –ù–µ—Ç | T5, BART |



- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) ‚Äî –æ–ø–∏—Å–∞–Ω–∏–µ –≤—Å–µ—Ö —Ç—Ä—ë—Ö —Ç–∏–ø–æ–≤
- [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html) ‚Äî –∫–æ–¥ —Å –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è–º–∏
- [Transformers explained](https://www.linkedin.com/posts/nicole-koenigstein_transformers-the-definitive-guide-activity-7411413196646846466-PZpQ?utm_source=share&utm_medium=member_ios&rcm=ACoAABHcLTkB9ZRrPOB4NW-jmLGXwC1oz0SS_hY)
- [CS25 Transformers intro](https://youtu.be/XfpMkf4rD6E?si=A0ckxe7ZkndQxWEe)
- [NLP interview questions](https://www.linkedin.com/posts/sumanth077_top-50-llm-interview-questions-a-comprehensive-activity-7400863663253028864-2oPM)
- [encoders vs decoders](https://www.linkedin.com/posts/mary-newhauser_not-all-llms-generate-text-most-people-share-7402121282898739201-mOSi/)
- [self-attention-from-scratch](https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html?utm_source=substack&utm_medium=email)
- [Self attention](https://youtu.be/Bg8Y5q1OiP0)
- [Transformers cheatsheet](https://github.com/afshinea/stanford-cme-295-transformers-large-language-models/blob/main/en/cheatsheet-transformers-large-language-models.pdf)

## LLM datasets

- [amazon-reviews-2023](https://amazon-reviews-2023.github.io/)


---

# 1M context 

2020: GPT-3 ‚Üí 2K –∫–æ–Ω—Ç–µ–∫—Å—Ç
2023: GPT-4 ‚Üí 32K –∫–æ–Ω—Ç–µ–∫—Å—Ç (128K extended)
2024: Claude 3 ‚Üí 200K –∫–æ–Ω—Ç–µ–∫—Å—Ç
2024: Gemini 1.5 ‚Üí 1M –∫–æ–Ω—Ç–µ–∫—Å—Ç
2025: Gemini 2.0 ‚Üí 2M –∫–æ–Ω—Ç–µ–∫—Å—Ç (announced)

–î–ª—è N = 1M —Ç–æ–∫–µ–Ω–æ–≤:

–ü–∞–º—è—Ç—å –¥–ª—è attention matrix:
1M √ó 1M √ó 4 bytes (float32) = 4 TB (—Ç–æ–ª—å–∫–æ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Å–ª–æ—è!)

–í—ã—á–∏—Å–ª–µ–Ω–∏—è:
Query @ Key^T = 1M √ó 1M —É–º–Ω–æ–∂–µ–Ω–∏–π
–≠—Ç–æ –Ω—É–∂–Ω–æ –¥–µ–ª–∞—Ç—å –¥–ª—è –ö–ê–ñ–î–û–ì–û —Å–ª–æ—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, 80+ —Å–ª–æ—ë–≤)

–ù–∞ –æ–±—ã—á–Ω–æ–º GPU: –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ!

Gemini –ù–ï –∏—Å–ø–æ–ª—å–∑—É–µ—Ç "—á–µ—Å—Ç–Ω—ã–π" full self-attention O(N¬≤)
–í–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ: —É–º–Ω–∞—è –∫–æ–º–±–∏–Ω–∞—Ü–∏—è sparse patterns, compression, distribution
–†–µ–∑—É–ª—å—Ç–∞—Ç: effective 1M –∫–æ–Ω—Ç–µ–∫—Å—Ç, –Ω–µ literal 1M√ó1M attention

1. ‚úÖ Sparse Attention - –Ω–µ –≤–µ—Å—å –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å–æ –≤—Å–µ–º
2. ‚úÖ Ring Attention - —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞–º
3. ‚úÖ Flash Attention - —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è
4. ‚úÖ MQA/GQA - —ç–∫–æ–Ω–æ–º–∏—è KV cache
5. ‚úÖ Hierarchical processing - —É—Ä–æ–≤–Ω–∏ –∞–±—Å—Ç—Ä–∞–∫—Ü–∏–∏
6. ‚úÖ KV compression - –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è, pooling
7. ‚úÖ MoE - selective activation
8. ‚úÖ –û–≥—Ä–æ–º–Ω—ã–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã (TPU v5)

–î–∞–∂–µ —Å 1M –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, –º–æ–¥–µ–ª—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç ~10-20K —Ç–æ–∫–µ–Ω–æ–≤

–û—Å—Ç–∞–ª—å–Ω–æ–µ:
- Background context
- Reference material
- "–ù–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π"

–ù–æ –Ω–µ –∞–∫—Ç–∏–≤–Ω–æ —É—á–∞—Å—Ç–≤—É–µ—Ç –≤ –∫–∞–∂–¥–æ–º —à–∞–≥–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏

## –ü—Ä–æ–±–ª–µ–º–∞ "lost in the middle":

–ú–æ–¥–µ–ª—å —Ö—É–∂–µ "–ø–æ–º–Ω–∏—Ç" —Å–µ—Ä–µ–¥–∏–Ω—É –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
–ù–∞—á–∞–ª–æ –∏ –∫–æ–Ω–µ—Ü - —Ö–æ—Ä–æ—à–æ
–°–µ—Ä–µ–¥–∏–Ω–∞ - —Ö—É–∂–µ

–†–µ—à–µ–Ω–∏—è:
- Recency bias
- Position embeddings
- Attention boosting –¥–ª—è –≤–∞–∂–Ω—ã—Ö —á–∞—Å—Ç–µ–π

–í–µ—Ä–æ—è—Ç–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞

1. Input: 1M —Ç–æ–∫–µ–Ω–æ–≤

2. Chunking:
   –†–∞–∑–±–∏—Ç—å –Ω–∞ —á–∞–Ω–∫–∏ –ø–æ 10K —Ç–æ–∫–µ–Ω–æ–≤ (100 —á–∞–Ω–∫–æ–≤)

3. Local processing (Flash Attention):
   –ö–∞–∂–¥—ã–π chunk –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è –ª–æ–∫–∞–ª—å–Ω–æ
   Efficient O(chunk_size¬≤) —Å Flash Attention

4. Hierarchical aggregation:
   Level 1: Local (10K —Ç–æ–∫–µ–Ω–æ–≤)
   Level 2: Regional (100K —Ç–æ–∫–µ–Ω–æ–≤)  
   Level 3: Global (1M —Ç–æ–∫–µ–Ω–æ–≤)

5. Sparse global attention:
   –¢–æ–ª—å–∫–æ –º–µ–∂–¥—É important tokens
   Longformer-style patterns

6. Ring Attention:
   Distributed across many accelerators (TPU v5)
   KV cache rotation –º–µ–∂–¥—É —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞–º–∏

7. MQA/GQA:
   Shared K, V –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏

8. KV cache compression:
   INT4 quantization –¥–ª—è —Å—Ç–∞—Ä—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
   Pooling –¥–ª—è distant context

9. MoE routing:
   –†–∞–∑–Ω—ã–µ experts –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —á–∞—Å—Ç–µ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞


–¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏, –∫–æ—Ç–æ—Ä—ã–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç Gemini
1. Sparse Attention (–†–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ)
–ò–¥–µ—è
–ù–µ –≤—Å–µ —Ç–æ–∫–µ–Ω—ã –¥–æ–ª–∂–Ω—ã —Å–º–æ—Ç—Ä–µ—Ç—å –Ω–∞ –≤—Å–µ —Ç–æ–∫–µ–Ω—ã. –ë–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ connections –Ω–µ –Ω—É–∂–Ω—ã.
–í–∏–¥—ã sparse attention
A. Local Attention (Sliding Window)
–ö–∞–∂–¥—ã–π —Ç–æ–∫–µ–Ω —Å–º–æ—Ç—Ä–∏—Ç —Ç–æ–ª—å–∫–æ –Ω–∞ –±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–µ–π

–ü—Ä–∏–º–µ—Ä: –æ–∫–Ω–æ —Ä–∞–∑–º–µ—Ä–∞ W = 512

Token 1000 –≤–∏–¥–∏—Ç: [500-1500]  (–Ω–µ –≤–µ—Å—å –∫–æ–Ω—Ç–µ–∫—Å—Ç!)

–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è:
       Token 0   Token 500  Token 1000  Token 1500
Token 0    ‚óè        -           -           -
Token 500  -        ‚óè           ‚óè           -
Token 1000 -        ‚óè           ‚óè           ‚óè
Token 1500 -        -           ‚óè           ‚óè

–°–ª–æ–∂–Ω–æ—Å—Ç—å: O(N √ó W) –≤–º–µ—Å—Ç–æ O(N¬≤)
B. Strided Attention (—Å –ø—Ä–æ–ø—É—Å–∫–∞–º–∏)
–ö–∞–∂–¥—ã–π k-–π —Ç–æ–∫–µ–Ω

Token 0:    —Å–º–æ—Ç—Ä–∏—Ç –Ω–∞ [0, 100, 200, 300, ...]
Token 1:    —Å–º–æ—Ç—Ä–∏—Ç –Ω–∞ [1, 101, 201, 301, ...]

Sparse pattern:
‚óè  -  -  -  ‚óè  -  -  -  ‚óè  -  -  -
-  ‚óè  -  -  -  ‚óè  -  -  -  ‚óè  -  -
-  -  ‚óè  -  -  -  ‚óè  -  -  -  ‚óè  -
C. Block-Sparse Attention
–†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –±–ª–æ–∫–∏, attention –º–µ–∂–¥—É –±–ª–æ–∫–∞–º–∏

[Block 1] ‚Üê ‚Üí [Block 2] ‚Üê ‚Üí [Block 3]
   ‚Üï              ‚Üï              ‚Üï
[Block 4]     [Block 5]     [Block 6]

Attention —Ç–æ–ª—å–∫–æ –º–µ–∂–¥—É —Å–æ—Å–µ–¥–Ω–∏–º–∏ –±–ª–æ–∫–∞–º–∏
D. Longformer-style attention
–ö–æ–º–±–∏–Ω–∞—Ü–∏—è:
1. Local attention (sliding window)
2. Global attention –¥–ª—è special tokens ([CLS], important phrases)
3. Dilated attention (—Å —É–≤–µ–ª–∏—á–∏–≤–∞—é—â–∏–º–∏—Å—è –ø—Ä–æ–ø—É—Å–∫–∞–º–∏)

Attention pattern:
    0   1   2   3   4   5   6   7   8
0   ‚óè   ‚óè   ‚óè   -   -   -   -   -   -   ‚Üê Local (window=3)
1   ‚óè   ‚óè   ‚óè   ‚óè   -   -   -   -   -
2   ‚óè   ‚óè   ‚óè   ‚óè   ‚óè   -   -   -   -
3   -   ‚óè   ‚óè   ‚óè   ‚óè   ‚óè   -   -   -
4   ‚óè   -   ‚óè   ‚óè   ‚óè   ‚óè   ‚óè   -   -   ‚Üê Global token
5   -   -   -   ‚óè   ‚óè   ‚óè   ‚óè   ‚óè   -
...
–≠—Ñ—Ñ–µ–∫—Ç –¥–ª—è Gemini
–í–º–µ—Å—Ç–æ: 1M √ó 1M attention
–†–µ–∞–ª—å–Ω–æ: 1M √ó W –≥–¥–µ W << 1M

–ï—Å–ª–∏ W = 4096:
–ü–∞–º—è—Ç—å: 1M √ó 4K = 4B —ç–ª–µ–º–µ–Ω—Ç–æ–≤ (–≤–º–µ—Å—Ç–æ 1T!)
–°–æ–∫—Ä–∞—â–µ–Ω–∏–µ –≤ ~250,000 —Ä–∞–∑! üöÄ

2. Ring Attention (–ö–æ–ª—å—Ü–µ–≤–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ)
–ò–¥–µ—è –æ—Ç Google (2023)
–†–∞–∑–±–∏—Ç—å –¥–ª–∏–Ω–Ω—É—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ —á–∞–Ω–∫–∏ –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –∏—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ —á–µ—Ä–µ–∑ –∫–æ–ª—å—Ü–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤.
–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
–ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å 1M —Ç–æ–∫–µ–Ω–æ–≤ —Ä–∞–∑–±–∏—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏ –ø–æ 10K

GPU 1: Chunk 1  [0-10K]
GPU 2: Chunk 2  [10K-20K]
GPU 3: Chunk 3  [20K-30K]
...
GPU 100: Chunk 100 [990K-1M]

–ü—Ä–æ—Ü–µ—Å—Å (–ø–æ –∫–æ–ª—å—Ü—É):
Round 1: GPU_i –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å–≤–æ–π chunk —Å –ª–æ–∫–∞–ª—å–Ω—ã–º attention
Round 2: –ü–µ—Ä–µ–¥–∞—ë–º KV-cache —Å–æ—Å–µ–¥–Ω–µ–º—É GPU
Round 3: GPU_i –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —á—É–∂–æ–π chunk
...
–ü–æ—Å–ª–µ –ø–æ–ª–Ω–æ–≥–æ –∫—Ä—É–≥–∞: –∫–∞–∂–¥—ã–π chunk "—É–≤–∏–¥–µ–ª" –≤—Å–µ –æ—Å—Ç–∞–ª—å–Ω—ã–µ
–ü—Å–µ–≤–¥–æ–∫–æ–¥
for round in 1..num_chunks:
    // Local computation
    Q_local = compute_query(chunk_i)
    K_local = compute_key(chunk_i)
    V_local = compute_value(chunk_i)
    
    // Attention —Å —Ç–µ–∫—É—â–∏–º–∏ K, V
    attn_output = attention(Q_local, K_current, V_current)
    
    // Rotate K, V to next GPU (ring communication)
    send(K_local, V_local, to=next_gpu)
    K_current, V_current = receive(from=prev_gpu)
–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞
‚úÖ –ö–∞–∂–¥—ã–π GPU —Ö—Ä–∞–Ω–∏—Ç —Ç–æ–ª—å–∫–æ —Å–≤–æ–π chunk
‚úÖ –ü–æ–ª–Ω—ã–π attention –º–µ–∂–¥—É –≤—Å–µ–º–∏ —Ç–æ–∫–µ–Ω–∞–º–∏ (–Ω–µ sparse!)
‚úÖ Memory distributed: O(N/num_devices)
‚úÖ –ö–æ–º–º—É–Ω–∏–∫–∞—Ü–∏—è efficient (–≤—ã—Å–æ–∫–∞—è –ø—Ä–æ–ø—É—Å–∫–Ω–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–µ–∂–¥—É GPU)
–°–ª–æ–∂–Ω–æ—Å—Ç—å
Compute: O(N¬≤) –≤—Å—ë —Ä–∞–≤–Ω–æ, –Ω–æ distributed
Memory per device: O(N/D) –≥–¥–µ D = –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤
Communication: O(N¬≤ / D) - –Ω—É–∂–Ω–∞ –≤—ã—Å–æ–∫–∞—è bandwidth

–î–ª—è 1M —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ 100 GPU:
–ö–∞–∂–¥—ã–π GPU: 10K —Ç–æ–∫–µ–Ω–æ–≤
Memory per GPU: —É–ø—Ä–∞–≤–ª—è–µ–º–æ!

3. Multi-Query Attention (MQA) –∏ Grouped-Query Attention (GQA)
–ü—Ä–æ–±–ª–µ–º–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ MHA
Multi-Head Attention (MHA):

–ö–∞–∂–¥–∞—è head –∏–º–µ–µ—Ç —Å–≤–æ–∏ Q, K, V

Heads = 32
d_model = 512
d_head = 512 / 32 = 16

Memory –¥–ª—è KV cache:
N √ó heads √ó d_head √ó 2 (K –∏ V) = N √ó 32 √ó 16 √ó 2 = N √ó 1024

–î–ª—è N = 1M: 1B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Ç–æ–ª—å–∫–æ –¥–ª—è KV cache!
Multi-Query Attention (MQA)
–ò–¥–µ—è: –†–∞–∑–Ω—ã–µ Q –¥–ª—è –∫–∞–∂–¥–æ–π head, –Ω–æ –û–ë–©–ò–ï K –∏ V

Heads = 32
Q: 32 —Ä–∞–∑–Ω—ã—Ö –º–∞—Ç—Ä–∏—Ü
K, V: –û–î–ù–ê –æ–±—â–∞—è –º–∞—Ç—Ä–∏—Ü–∞

Memory –¥–ª—è KV cache:
N √ó d_head √ó 2 = N √ó 16 √ó 2 = N √ó 32

–°–æ–∫—Ä–∞—â–µ–Ω–∏–µ –≤ 32 —Ä–∞–∑–∞! üöÄ
Grouped-Query Attention (GQA) - –∫–æ–º–ø—Ä–æ–º–∏—Å—Å
–ì—Ä—É–ø–ø–∏—Ä—É–µ–º heads

Total heads = 32
Groups = 8
Heads per group = 4

–ö–∞–∂–¥–∞—è –≥—Ä—É–ø–ø–∞ –∏–º–µ–µ—Ç –æ–±—â–∏–µ K, V

Memory –¥–ª—è KV cache:
N √ó groups √ó d_head √ó 2 = N √ó 8 √ó 16 √ó 2 = N √ó 256

–°–æ–∫—Ä–∞—â–µ–Ω–∏–µ –≤ 4 —Ä–∞–∑–∞
–ù–æ –ª—É—á—à–µ –∫–∞—á–µ—Å—Ç–≤–æ, —á–µ–º MQA
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ Gemini
Gemini, –≤–µ—Ä–æ—è—Ç–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç GQA:
- –ú–µ–Ω—å—à–µ –ø–∞–º—è—Ç–∏ –¥–ª—è KV cache
- –ü–æ–∑–≤–æ–ª—è–µ—Ç caching –¥–ª—è –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
- –ö–∞—á–µ—Å—Ç–≤–æ –ø–æ—á—Ç–∏ –∫–∞–∫ MHA

4. Flash Attention (–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π)
–ü—Ä–æ–±–ª–µ–º–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ attention
Standard attention (materialized):

1. Compute Q @ K^T ‚Üí store N√óN matrix
2. Softmax ‚Üí requires full matrix
3. Multiply by V

Peak memory: O(N¬≤)
Flash Attention —Ä–µ—à–µ–Ω–∏–µ
–ò–¥–µ—è: –ù–µ —Ö—Ä–∞–Ω–∏–º –ø–æ–ª–Ω—É—é attention matrix!

–í—ã—á–∏—Å–ª—è–µ–º –ø–æ –±–ª–æ–∫–∞–º (tiling):
1. –†–∞–∑–±–∏–≤–∞–µ–º Q, K, V –Ω–∞ –±–ª–æ–∫–∏
2. –î–ª—è –∫–∞–∂–¥–æ–≥–æ –±–ª–æ–∫–∞ Q:
   - –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω—É–∂–Ω—ã–µ –±–ª–æ–∫–∏ K, V
   - –í—ã—á–∏—Å–ª—è–µ–º attention
   - –°—Ä–∞–∑—É aggreg–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
   - –£–¥–∞–ª—è–µ–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
3. –ù–∏–∫–æ–≥–¥–∞ –Ω–µ —Ö—Ä–∞–Ω–∏–º –ø–æ–ª–Ω—É—é N√óN –º–∞—Ç—Ä–∏—Ü—É!

Memory: O(N) –≤–º–µ—Å—Ç–æ O(N¬≤) üöÄ
Speed: 2-4x faster (–º–µ–Ω—å—à–µ memory I/O)
–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
Standard:
Q (N√ód) @ K^T (d√óN) = Attention Matrix (N√óN) ‚Üê –û–ì–†–û–ú–ù–û!
    ‚Üì
Softmax
    ‚Üì
@ V (N√ód)

Flash Attention (tiled):
For block_i in Q_blocks:
    For block_j in K_blocks:
        attention_block = Q_i @ K_j^T  ‚Üê –¢–æ–ª—å–∫–æ –±–ª–æ–∫!
        softmax_block = softmax(attention_block)
        output_i += softmax_block @ V_j
        # –£–¥–∞–ª—è–µ–º attention_block –∏–∑ –ø–∞–º—è—Ç–∏
–î–ª—è Gemini 1M –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
–ë–µ–∑ Flash Attention:
Memory = 1M √ó 1M √ó 4 bytes = 4TB ‚ùå

–° Flash Attention:
Memory = 1M √ó block_size √ó 4 bytes
–ï—Å–ª–∏ block_size = 256: 1M √ó 256 √ó 4 = 1GB ‚úÖ

–°–æ–∫—Ä–∞—â–µ–Ω–∏–µ –≤ ~4000 —Ä–∞–∑!

5. Hierarchical / Nested Attention
–ò–¥–µ—è
–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —É—Ä–æ–≤–Ω—è—Ö –∞–±—Å—Ç—Ä–∞–∫—Ü–∏–∏.
–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
Level 1 (Low): –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –º–µ–ª–∫–∏–µ –±–ª–æ–∫–∏ (512 —Ç–æ–∫–µ–Ω–æ–≤)
    ‚Üì Compress
Level 2 (Mid): –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å–∂–∞—Ç—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è (64K —Ç–æ–∫–µ–Ω–æ–≤)
    ‚Üì Compress
Level 3 (High): –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Å—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å (1M —Ç–æ–∫–µ–Ω–æ–≤)

–ê–Ω–∞–ª–æ–≥–∏—è: –ü–∏—Ä–∞–º–∏–¥–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
- –ù–∏–∑–∫–∏–π —É—Ä–æ–≤–µ–Ω—å: –¥–µ—Ç–∞–ª–∏ (–ø–∏–∫—Å–µ–ª–∏)
- –°—Ä–µ–¥–Ω–∏–π —É—Ä–æ–≤–µ–Ω—å: —Ä–µ–≥–∏–æ–Ω—ã (–ø–∞—Ç—Ç–µ—Ä–Ω—ã)
- –í—ã—Å–æ–∫–∏–π —É—Ä–æ–≤–µ–Ω—å: –ø–æ–ª–Ω–∞—è –∫–∞—Ä—Ç–∏–Ω–∞ (–∫–æ–Ω—Ç–µ–∫—Å—Ç)
–ü—Ä–∏–º–µ—Ä –¥–ª—è —Ç–µ–∫—Å—Ç–∞
–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: 1M —Ç–æ–∫–µ–Ω–æ–≤

Level 1: Local attention –≤ –±–ª–æ–∫–∞—Ö –ø–æ 512 —Ç–æ–∫–µ–Ω–æ–≤
    "The cat sat on the mat" ‚Üí embedding_1
    "It was a sunny day"     ‚Üí embedding_2
    ...

Level 2: Attention –º–µ–∂–¥—É embeddings –±–ª–æ–∫–æ–≤ (2000 –±–ª–æ–∫–æ–≤)
    [embedding_1, embedding_2, ..., embedding_2000]
    Attention –º–µ–∂–¥—É –Ω–∏–º–∏

Level 3: Global representation
    –ò—Ç–æ–≥–æ–≤–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –≤—Å–µ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞

–ü—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏:
- Level 3 –¥–∞—ë—Ç –æ–±—â–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
- Level 2 –¥–∞—ë—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Å–µ–∫—Ü–∏–∏
- Level 1 –¥–∞—ë—Ç —Ç–æ—á–Ω—ã–µ –¥–µ—Ç–∞–ª–∏

6. KV Cache Compression
–ü—Ä–æ–±–ª–µ–º–∞
KV cache —Ä–∞—Å—Ç—ë—Ç –ª–∏–Ω–µ–π–Ω–æ —Å –¥–ª–∏–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

Standard KV cache –¥–ª—è 1M —Ç–æ–∫–µ–Ω–æ–≤:
K: 1M √ó d_model
V: 1M √ó d_model

–û–≥—Ä–æ–º–Ω–∞—è –ø–∞–º—è—Ç—å!
–†–µ—à–µ–Ω–∏–µ: –°–∂–∞—Ç–∏–µ KV
A. Quantization (–∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è)
FP16 ‚Üí INT8 ‚Üí INT4

FP16: 2 bytes per value
INT4: 0.5 bytes per value

–°–∂–∞—Ç–∏–µ –≤ 4 —Ä–∞–∑–∞! üöÄ
B. Pooling —Å—Ç–∞—Ä—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
–ù–µ–¥–∞–≤–Ω–∏–µ —Ç–æ–∫–µ–Ω—ã: –ø–æ–ª–Ω–æ–µ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ
–°—Ç–∞—Ä—ã–µ —Ç–æ–∫–µ–Ω—ã: —Å–∂–∞—Ç—ã–µ

Tokens 0-1000:    —Å–∂–∞—Ç—å –≤ 100 embeddings (pooling)
Tokens 1000-2000: —Å–∂–∞—Ç—å –≤ 100 embeddings
...
Tokens 999000-1M: –ø–æ–ª–Ω–æ–µ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ (1000 tokens)

Total KV: 100 √ó 999 + 1000 = ~100K –≤–º–µ—Å—Ç–æ 1M
–°–∂–∞—Ç–∏–µ –≤ 10 —Ä–∞–∑!
C. Adaptive selection
–•—Ä–∞–Ω–∏–º —Ç–æ–ª—å–∫–æ "–≤–∞–∂–Ω—ã–µ" —Ç–æ–∫–µ–Ω—ã –≤ –ø–æ–ª–Ω–æ–º —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–∏

Importance scoring:
- Attention weights
- Gradient norms
- Manual markers (headings, keywords)

Top 10% tokens: –ø–æ–ª–Ω–æ–µ KV
Bottom 90% tokens: —Å–∂–∞—Ç–æ–µ –∏–ª–∏ —É–¥–∞–ª—ë–Ω–Ω–æ–µ

7. Mixture of Experts (MoE)
–°–≤—è–∑—å —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
–ù–µ –≤—Å–µ —á–∞—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –Ω—É–∂–Ω—ã –¥–ª—è –≤—Å–µ—Ö —Ç–æ–∫–µ–Ω–æ–≤

MoE –ø–æ–∑–≤–æ–ª—è–µ—Ç:
- –ê–∫—Ç–∏–≤–∏—Ä–æ–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ experts –¥–ª—è –¥–∞–Ω–Ω–æ–π —á–∞—Å—Ç–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
- –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–∞–º—è—Ç—å
- –ü–∞—Ä–∞–ª–ª–µ–ª–∏–∑–æ–≤–∞—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É —Ä–∞–∑–Ω—ã—Ö —á–∞—Å—Ç–µ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞

–ü—Ä–∏–º–µ—Ä:
Tokens 0-100K    (English technical): Expert 1, 3, 5
Tokens 100K-200K (Code Python):       Expert 2, 4, 7
Tokens 200K-1M   (English narrative): Expert 1, 6, 8

–ö–∞–∂–¥—ã–π expert —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ