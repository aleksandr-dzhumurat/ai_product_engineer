# Basics

# Exploding/Vanishing Gradients

## –í–æ–ø—Ä–æ—Å
–û–±—ä—è—Å–Ω–∏ exploding/vanishing gradients? –ö–∞–∫ —Å —ç—Ç–∏–º –±–æ—Ä–æ—Ç—å—Å—è?

## –û—Ç–≤–µ—Ç

–î–∞–≤–∞–π —Ä–∞–∑–ª–æ–∂–∏–º –ø—Ä–æ–±–ª–µ–º—É exploding –∏ vanishing gradients —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ, –∫–∞–∫ –Ω–∞ —Å–æ–±–µ—Å–µ –ø–æ deep learning.

### 1Ô∏è‚É£ –ß—Ç–æ —Ç–∞–∫–æ–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã

–í –Ω–µ–π—Ä–æ—Å–µ—Ç—è—Ö –æ–±—É—á–µ–Ω–∏–µ –∏–¥—ë—Ç —á–µ—Ä–µ–∑ –æ–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –æ—à–∏–±–∫–∏ (backprop):

$$w \gets w - \eta \frac{\partial L}{\partial w}$$

–≥–¥–µ $\frac{\partial L}{\partial w}$ ‚Äî –≥—Ä–∞–¥–∏–µ–Ω—Ç

–ï—Å–ª–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∏–µ –∏–ª–∏ –º–∞–ª–µ–Ω—å–∫–∏–µ ‚Üí –æ–±—É—á–µ–Ω–∏–µ –Ω–∞—Ä—É—à–∞–µ—Ç—Å—è

### 2Ô∏è‚É£ Vanishing gradients (–∑–∞—Ç—É—Ö–∞—é—â–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã)

**–ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç**

–í –≥–ª—É–±–æ–∫–æ–π —Å–µ—Ç–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã —É–º–µ–Ω—å—à–∞—é—Ç—Å—è —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ –ø–æ –º–µ—Ä–µ –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏—è —á–µ—Ä–µ–∑ —Å–ª–æ–∏.

**–†–µ–∑—É–ª—å—Ç–∞—Ç:** —Ä–∞–Ω–Ω–∏–µ —Å–ª–æ–∏ –ø–æ—á—Ç–∏ –Ω–µ –æ–±—É—á–∞—é—Ç—Å—è.

**–ü—Ä–∏—á–∏–Ω—ã**

–ú–∞–ª—ã–µ –≤–µ—Å–∞ + —Å–∏–≥–º–æ–∏–¥–Ω–∞—è/tanh –∞–∫—Ç–∏–≤–∞—Ü–∏—è:

$$\sigma'(x) = \sigma(x)(1-\sigma(x)) \le 0.25$$

–ú–Ω–æ–∂–∏—Ç–µ–ª—å < 1 –Ω–∞ –∫–∞–∂–¥–æ–º —Å–ª–æ–µ ‚Üí –≥—Ä–∞–¥–∏–µ–Ω—Ç –±—ã—Å—Ç—Ä–æ —Å—Ç—Ä–µ–º–∏—Ç—Å—è –∫ 0

**–°–∏–º–ø—Ç–æ–º—ã**
- –ü–æ—Ç–µ—Ä—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —Å–µ—Ç–∏ –æ–±—É—á–∞—Ç—å –Ω–∏–∂–Ω–∏–µ —Å–ª–æ–∏
- –û—á–µ–Ω—å –º–µ–¥–ª–µ–Ω–Ω–∞—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å
- –í—Å–µ –≤–µ—Å–∞ –≤ –Ω–∞—á–∞–ª–µ —Å–µ—Ç–∏ –ø–æ—á—Ç–∏ –Ω–µ –º–µ–Ω—è—é—Ç—Å—è

### 3Ô∏è‚É£ Exploding gradients (–≤–∑—Ä—ã–≤–∞—é—â–∏–µ—Å—è –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã)

**–ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç**

–ì—Ä–∞–¥–∏–µ–Ω—Ç—ã –æ—á–µ–Ω—å –±–æ–ª—å—à–∏–µ, –≤–µ—Å–∞ –æ–±–Ω–æ–≤–ª—è—é—Ç—Å—è —Å–ª–∏—à–∫–æ–º —Å–∏–ª—å–Ω–æ ‚Üí –æ–±—É—á–µ–Ω–∏–µ –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ

**–ü—Ä–∏—á–∏–Ω—ã**
- –ë–æ–ª—å—à–∏–µ –Ω–∞—á–∞–ª—å–Ω—ã–µ –≤–µ—Å–∞
- –î–ª–∏–Ω–Ω–∞—è —Å–µ—Ç—å ‚Üí –º–Ω–æ–≥–æ–∫—Ä–∞—Ç–Ω–æ–µ —É–º–Ω–æ–∂–µ–Ω–∏–µ –±–æ–ª—å—à–∏—Ö –º–∞—Ç—Ä–∏—Ü
- –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è –∞–∫—Ç–∏–≤–∞—Ü–∏—è (ReLU —Å –±–æ–ª—å—à–∏–º –≤—Ö–æ–¥–æ–º)

**–°–∏–º–ø—Ç–æ–º—ã**
- Loss —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è NaN
- –í–µ—Å–∞ —Ä–µ–∑–∫–æ –ø—Ä—ã–≥–∞—é—Ç

### 4Ô∏è‚É£ –ö–∞–∫ –±–æ—Ä–æ—Ç—å—Å—è

#### üîπ –î–ª—è vanishing gradients

**–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å "—Ö–æ—Ä–æ—à–∏–µ" –∞–∫—Ç–∏–≤–∞—Ü–∏–∏**
- ReLU, LeakyReLU, GELU ‚Üí –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –Ω–µ –∑–∞—Ç—É—Ö–∞—é—Ç

**–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤**
- Xavier/Glorot, He

**–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è**
- BatchNorm ‚Üí —Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä—É–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ

**Shortcut/Residual connections**
- ResNet ‚Üí –ø–æ–∑–≤–æ–ª—è–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç—É "–ø—Ä–æ—Ç–µ–∫–∞—Ç—å" —á–µ—Ä–µ–∑ —Å–µ—Ç—å

#### üîπ –î–ª—è exploding gradients

**Gradient clipping**
```python
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
```

**–ú–µ–Ω—å—à–∏–π learning rate**

**–•–æ—Ä–æ—à–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è**

**–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é**
- BatchNorm, LayerNorm

### üîπ –í—ã–≤–æ–¥

| –ü—Ä–æ–±–ª–µ–º–∞ | –°–∏–º–ø—Ç–æ–º | –†–µ—à–µ–Ω–∏–µ |
|----------|---------|---------|
| Vanishing | –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã –±–ª–∏–∑–∫–∏ –∫ 0, —Ä–∞–Ω–Ω–∏–µ —Å–ª–æ–∏ –Ω–µ —É—á–∞—Ç—Å—è | ReLU, BatchNorm, Residual, Xavier/He |
| Exploding | –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã –æ–≥—Ä–æ–º–Ω—ã–µ, loss ‚Üí NaN | Gradient clipping, —É–º–µ–Ω—å—à–∏—Ç—å LR, –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è |

---


# Variational Autoencoders (VAEs)

Variational Autoencoders are a type of generative model that learns to encode data into a compressed representation and then decode it back, while also being able to generate new, similar data.

## The Basic Idea

A VAE consists of two main parts:

**Encoder**: Compresses input data (like images) into a lower-dimensional latent space representation. Unlike standard autoencoders, the encoder outputs parameters of a probability distribution (typically mean and variance) rather than a single fixed point.

**Decoder**: Takes samples from this latent space and reconstructs them back into the original data format.

## What Makes Them "Variational"?

The key innovation is that VAEs don't just map inputs to fixed points in latent space. Instead, they:

1. Map inputs to **probability distributions** in latent space
2. Sample from these distributions during training
3. Ensure the latent space is continuous and well-structured

This is done by forcing the learned distributions to be similar to a standard normal distribution (mean 0, variance 1) using something called the KL divergence term in the loss function.

## The Loss Function

VAEs optimize two objectives simultaneously:

- **Reconstruction loss**: How well can the decoder recreate the original input?
- **KL divergence**: How close is the latent distribution to a standard normal distribution?

## Why This Matters

By learning smooth, continuous latent representations, VAEs can:
- Generate new data by sampling random points from the latent space
- Interpolate smoothly between different data points
- Learn meaningful features in an unsupervised way

## Common Applications

- Image generation
- Data compression
- Anomaly detection
- Drug molecule design
- Music generation

Would you like me to dive deeper into any particular aspect, like the mathematical formulation, implementation details, or specific use cases?