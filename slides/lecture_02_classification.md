# Logistic regression

* [Jupyter Notebook](../jupyter_notebooks/vol_00_pre_requirements_02_machine_learning_classification.ipynb)
* [Naive bayes classifier](../jupyter_notebooks/vol_04_deep_dive_00_probability_hw_2_naive_bayes_solved.ipynb)


## Logistic regression: simple case

- –í—ã–±–æ—Ä–∫–∞: 20 –æ–±—ä–µ–∫—Ç–æ–≤
    - –ö–ª–∞—Å—Å 0 (dogs): –≤–µ—Å < 4 –∫–≥
    - –ö–ª–∞—Å—Å 1 (cats): –≤–µ—Å > 4 –∫–≥
- –ü—Ä–∏–∑–Ω–∞–∫: **–≤–µ—Å** (x)
- –ú–æ–¥–µ–ª—å: **–ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è** —Å 1 –ø—Ä–∏–∑–Ω–∞–∫–æ–º

–•–æ—Ç–∏–º –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫—É—é —Ä–µ–≥—Ä–µ—Å—Å–∏—é, –∫–æ—Ç–æ—Ä–∞—è –±—É–¥–µ—Ç –∏–¥–µ–∞–ª—å–Ω–æ —Ä–∞–∑–¥–µ–ª—è—Ç—å —ç—Ç–∏ –∫–ª–∞—Å—Å—ã. –°–∫–æ–ª—å–∫–æ –±—É–¥–µ—Ç –æ–±—É—á–∞—é—â–∏—Ö—Å—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –∏ –∫–∞–∫–∏–º–∏ –æ–Ω–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å, —á—Ç–æ–±—ã —Ä–∞–∑–¥–µ–ª–∏—Ç—å —ç—Ç–∏ –¥–≤–∞ –∫–ª–∞—Å—Å–∞?

–õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è:

$$
P(y=1|x) = \sigma(w x + b) = \frac{1}{1 + e^{-(w x + b)}}
$$

–≥–¥–µ:

- $w$ ‚Äî –≤–µ—Å –ø—Ä–∏–∑–Ω–∞–∫–∞
- $b$ ‚Äî —Å–º–µ—â–µ–Ω–∏–µ (bias)
- $\sigma$ ‚Äî —Å–∏–≥–º–æ–∏–¥

---

–®–∞–≥ 1: –°–∫–æ–ª—å–∫–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤? 2 –æ–±—É—á–∞—é—â–∏—Ö—Å—è –ø–∞—Ä–∞–º–µ—Ç—Ä–∞

- –û–¥–∏–Ω –ø—Ä–∏–∑–Ω–∞–∫ ‚Üí 1 –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç $w$
- —Å–º–µ—â–µ–Ω–∏–µ $b$

---

–®–∞–≥ 2: –ö–∞–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏–º –Ω—É–∂–Ω—ã?

* –ö–ª–∞—Å—Å 0: $x < 4 \Rightarrow \sigma(w x + b) \approx 0$
* –ö–ª–∞—Å—Å 1: $x > 4 \Rightarrow \sigma(w x + b) \approx 1$

–°–∏–≥–º–æ–∏–¥ = 0.5 –ø—Ä–∏ 

$$w x + b = 0 \Rightarrow x = -b/w$$

–ß—Ç–æ–±—ã –∏–¥–µ–∞–ª—å–Ω–æ —Ä–∞–∑–¥–µ–ª–∏—Ç—å –∫–ª–∞—Å—Å—ã:

$$ b/w = 4 \quad \Rightarrow \quad b = -4 w $$

---

–®–∞–≥ 3: –£—Å–ª–æ–≤–∏–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è

- –ö–ª–∞—Å—Å 0 (<4 –∫–≥) ‚Üí 0
- –ö–ª–∞—Å—Å 1 (>4 –∫–≥) ‚Üí 1

–¢–æ–≥–¥–∞ –ø—Ä–∏ $x < 4$ –∞—Ä–≥—É–º–µ–Ω—Ç —Å–∏–≥–º–æ–∏–¥—ã < 0, –∞ –ø—Ä–∏ $x >4$  –∞—Ä–≥—É–º–µ–Ω—Ç —Å–∏–≥–º–æ–∏–¥—ã > 0 ‚Üí $w > 0$

---

‚úÖ –í—ã–≤–æ–¥

- 2 –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ $w$ –∏ $b$
- –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ –¥–ª—è –∏–¥–µ–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è:

$$b = -4 w, \quad w > 0 $$
    
- –õ—é–±–æ–µ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ $w$ —Å —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ–º $b=-4w$ —Ä–∞–∑–¥–µ–ª–∏—Ç –∫–ª–∞—Å—Å—ã –∏–¥–µ–∞–ª—å–Ω–æ.


# Classification metrics

**Binary classification problem**, confusion matrix:

|  | **Predicted Positive** | **Predicted Negative** |
| --- | --- | --- |
| **Actual Positive (1s)** | **True Positives (TP)** | **False Negatives (FN)** |
| **Actual Negative (0s)** | **False Positives (FP)** | **True Negatives (TN)** |
- **True Positives (TP)** ‚Üí Model correctly predicted **positive**.
- **True Negatives (TN)** ‚Üí Model correctly predicted **negative**.
- **False Positives (FP)** ‚Üí Model incorrectly predicted **positive** (Type I error).
- **False Negatives (FN)** ‚Üí Model incorrectly predicted **negative** (Type II error).

The **F1 score** is the harmonic mean of **precision** and **recall**:

$$F1 = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}$$

Compute Precision

$$\text{Precision} = \frac{TP}{TP + FP}$$

- Measures how many of the **predicted positives** were actually correct.


Compute Recall

$$\text{Recall} = \frac{TP}{TP + FN}$$

- Measures how many of the **actual positives** were correctly identified.

‚úÖ High recall ‚Üí The model catches most of the positives (few false negatives).

**When to Prioritize Recall?**

* **Medical Diagnosis (Cancer, COVID-19, etc.)** ‚Üí Missing a sick patient is dangerous.
* **Fraud Detection** ‚Üí Better to flag potential fraud than let it go unnoticed.
* **Security Systems (Intrusion Detection)** ‚Üí Better to have false alarms than miss real threats.

üîπ **Trade-off:** High recall can increase **false positives** (low precision).

Recall, also known as **Sensitivity** or **True Positive Rate (TPR)**, is a key metric in classification models, especially when missing positive cases is costly (e.g., medical diagnosis, fraud detection).

Sensitivity and specificity measure opposite aspects of model performance:

| Metric | Formula | Meaning |
|--------|---------|---------|
| **Sensitivity (Recall, TPR)** | $\frac{TP}{TP + FN}$ | **Ability to detect positives** |
| **Specificity (TNR)** | $\frac{TN}{TN + FP}$ | **Ability to detect negatives** |

* **High Sensitivity (Recall)** ‚Üí Good at finding **positives** (e.g., cancer screening).
* **High Specificity** ‚Üí Good at ruling out **negatives** (e.g., spam detection).

üí° **Trade-off**: Increasing recall often lowers specificity, and vice versa.

## Step 3: Compute F1 Score

$$F1 = 2 \times \frac{(TP/(TP + FP)) \times (TP/(TP + FN))}{(TP/(TP + FP)) + (TP/(TP + FN))}$$

---

**Where:**
- $TP$ = True Positives
- $FP$ = False Positives  
- $FN$ = False Negatives

**Use F1 when classes are imbalanced** (e.g., fraud detection, medical diagnosis).

- If **false positives & false negatives have different costs**, choose **precision or recall** instead:
    - **High Precision?** ‚Üí Model avoids **false positives**.
    - **High Recall?** ‚Üí Model avoids **false negatives**.

# ROC curve vs P-R curve

Both **Precision-Recall (P-R) Curves** and **ROC Curves** help evaluate classification models, but they are suited for **different scenarios**.

**ROC Curve** plots **True Positive Rate (Recall) vs. False Positive Rate (FPR)**:

- **Best for balanced datasets** where **positives & negatives are roughly equal**.
- **False positives matter less**, as FPR normalizes by total negatives.

**Use ROC Curve when**:

- Both **false positives (FP) and false negatives (FN) are equally important**.
- The dataset has **balanced classes** (e.g., disease detection where 50% have it).

ROC-AUC –º–æ–∂–µ—Ç –≤—ã–≥–ª—è–¥–µ—Ç—å –≤—ã—Å–æ–∫–æ–π –Ω–∞ –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –ø–æ—Ç–æ–º—É —á—Ç–æ –æ–Ω–∞ —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–∏ –∏ false positive rate, –∫–æ—Ç–æ—Ä—ã–µ –æ–ø—Ä–µ–¥–µ–ª—è—é—Ç—Å—è –º–∞–∂–æ—Ä–∏—Ç–∞—Ä–Ω—ã–º –∫–ª–∞—Å—Å–æ–º, —á–∞—Å—Ç–æ —Å–∫—Ä—ã–≤–∞—è –ø–ª–æ—Ö—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ –º–∏–Ω–æ—Ä–∏—Ç–∞—Ä–Ω–æ–º –∫–ª–∞—Å—Å–µ.

**1. –ù–µ—á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∫ –¥–∏—Å–±–∞–ª–∞–Ω—Å—É –∫–ª–∞—Å—Å–æ–≤**
ROC-AUC –æ—Å–Ω–æ–≤–∞–Ω–∞ –Ω–∞ true positive rate (TPR) –∏ false positive rate (FPR). –ö–æ–≥–¥–∞ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π –∫–ª–∞—Å—Å –¥–æ–º–∏–Ω–∏—Ä—É–µ—Ç, –¥–∞–∂–µ –±–æ–ª—å—à–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª–æ–∂–Ω–æ–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –º–∞–ª–æ–º—É FPR, –∏–∑-–∑–∞ —á–µ–≥–æ –º–æ–¥–µ–ª—å –≤—ã–≥–ª—è–¥–∏—Ç –ª—É—á—à–µ, —á–µ–º –µ—Å—Ç—å –Ω–∞ —Å–∞–º–æ–º –¥–µ–ª–µ.

**2. –•–æ—Ä–æ—à–∏–π —Ä–∞–Ω–∂–∏—Ä ‚â† —Ö–æ—Ä–æ—à–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è**
ROC-AUC –∏–∑–º–µ—Ä—è–µ—Ç, –Ω–∞—Å–∫–æ–ª—å–∫–æ —Ö–æ—Ä–æ—à–æ –º–æ–¥–µ–ª—å —Ä–∞–Ω–∂–∏—Ä—É–µ—Ç –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã –≤—ã—à–µ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö, –Ω–æ –Ω–µ —Ç–æ, –Ω–∞—Å–∫–æ–ª—å–∫–æ —Ç–æ—á–Ω–æ –æ–Ω–∞ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π –∫–ª–∞—Å—Å –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º–æ–º –ø–æ—Ä–æ–≥–µ. –ú–æ–¥–µ–ª—å –º–æ–∂–µ—Ç –∏–º–µ—Ç—å –≤—ã—Å–æ–∫–∏–π ROC-AUC, –Ω–æ –ø—Ä–∏ —ç—Ç–æ–º –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å –æ—á–µ–Ω—å –ø–ª–æ—Ö–∏–µ precision –∏–ª–∏ recall –¥–ª—è –º–∏–Ω–æ—Ä–∏—Ç–∞—Ä–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞.

**3. –ò–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç —Ä–µ–∞–ª—å–Ω—É—é —Ä–∞–±–æ—á—É—é —Ç–æ—á–∫—É**
–ù–∞ –ø—Ä–∞–∫—Ç–∏–∫–µ –≤–∞–∂–Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø—Ä–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º –ø–æ—Ä–æ–≥–µ –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, —á–∞—Å—Ç–æ—Ç–∞ —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏—è –∞–ª–µ—Ä—Ç–æ–≤, –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –ø–æ –∑–∞—Ç—Ä–∞—Ç–∞–º). ROC-AUC —É—Å—Ä–µ–¥–Ω—è–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–æ –≤—Å–µ–º –ø–æ—Ä–æ–≥–∞–º, –º–Ω–æ–≥–∏–µ –∏–∑ –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ–∞–∫—Ç—É–∞–ª—å–Ω—ã –≤ —É—Å–ª–æ–≤–∏—è—Ö —Å–∏–ª—å–Ω–æ–≥–æ –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞.

**4. –ú–æ–∂–µ—Ç —Å–∫—Ä—ã–≤–∞—Ç—å –ø–ª–æ—Ö—É—é —Ä–∞–±–æ—Ç—É –Ω–∞ –º–∏–Ω–æ—Ä–∏—Ç–∞—Ä–Ω–æ–º –∫–ª–∞—Å—Å–µ**
–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä, –∫–æ—Ç–æ—Ä—ã–π –ø–æ—á—Ç–∏ –≤—Å–µ–≥–¥–∞ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –º–∞–∂–æ—Ä–∏—Ç–∞—Ä–Ω—ã–π –∫–ª–∞—Å—Å, –≤—Å—ë —Ä–∞–≤–Ω–æ –º–æ–∂–µ—Ç –¥–æ—Å—Ç–∏—á—å –æ–±–º–∞–Ω—á–∏–≤–æ –≤—ã—Å–æ–∫–æ–≥–æ ROC-AUC, –µ—Å–ª–∏ –æ–Ω —Ö–æ—Ç—å –Ω–µ–º–Ω–æ–≥–æ —Ä–∞–∑–¥–µ–ª—è–µ—Ç –∫–ª–∞—Å—Å—ã.

The **P-R Curve** plots **Precision vs. Recall**, focusing only on **positive class performance**:

- **Best for imbalanced datasets**, where **positives are rare** (e.g., fraud detection).
- **More informative when false positives are costly**.

**Use P-R Curve when**:

- **Positive class is rare** (e.g., cancer detection, fraud, spam filtering).
- You care more about **precision and recall trade-offs**.

| **Scenario** | **ROC Curve** | **P-R Curve** |
| --- | --- | --- |
| Balanced dataset (50-50) | ‚úÖ Yes | ‚ùå No |
| Imbalanced dataset (e.g., 1% positives) | ‚ùå No | ‚úÖ Yes |
| Medical diagnosis (minimizing false negatives) | ‚úÖ Yes | ‚úÖ Yes |
| Fraud detection (rare class) | ‚ùå No | ‚úÖ Yes |
| When FP rate is misleading | ‚ùå No | ‚úÖ Yes |

# Threshold tuning

**Default Threshold Limitations**

- The default threshold (0.5) often favors the majority class in imbalanced settings, leading to poor recall/precision for the minority class[1](https://machinelearningmastery.com/threshold-moving-for-imbalanced-classification/)[4](https://machinelearningmastery.com/tour-of-evaluation-metrics-for-imbalanced-classification/).
- Example: A model might achieve 99% accuracy by always predicting the majority class but fail to detect critical minority cases (e.g., fraud or disease)[4](https://machinelearningmastery.com/tour-of-evaluation-metrics-for-imbalanced-classification/).

## **ROC Curve Analysis**

- Identify the threshold that maximizes the¬†**Youden‚Äôs J statistic**¬†(J = TPR + TNR - 1) or balances True Positive Rate (TPR) and False Positive Rate (FPR)[1](https://machinelearningmastery.com/threshold-moving-for-imbalanced-classification/).
- Example: Use the¬†**`roc_curve`**¬†function in¬†**`scikit-learn`**¬†to extract thresholds and select the point closest to the top-left corner of the ROC curve.

## **Precision-Recall Curve Analysis**

- Focus on thresholds that balance precision and recall, especially useful when the minority class is critical[1](https://machinelearningmastery.com/threshold-moving-for-imbalanced-classification/)[4](https://machinelearningmastery.com/tour-of-evaluation-metrics-for-imbalanced-classification/).
- Example: Optimize for the¬†**F1-score**¬†(harmonic mean of precision and recall) or target a specific recall value.

Grid Search for Threshold Tuning

```python
import numpy as np
from sklearn.metrics import confusion_matrix
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split

X, y = make_classification(n_samples=1000, n_features=20, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

clf = LogisticRegression()
clf.fit(X_train, y_train)

y_probs = clf.predict_proba(X_test)[:, 1]

# Define costs
cost_fp = 1
cost_fn = 5

thresholds = np.linspace(0, 1, 100)
min_cost = float('inf')
best_threshold = 0.5

for threshold in thresholds:
    y_pred = (y_probs >= threshold).astype(int)
    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()
    
    cost = cost_fp * fp + cost_fn * fn
    if cost < min_cost:
        min_cost = cost
        best_threshold = threshold

print(f"Best threshold: {best_threshold:.2f} with minimum cost: {min_cost}")
```

# Negative sampling

```python
def prepare_evaluation_df(input_df, negatives_per_one_positive=2):
    input_df = pl.from_pandas(input_df) if isinstance(input_df, pd.DataFrame) else input_df
    key_fields = ['dt', 'state_name', 'StoreID', 'IsOnline']
    # Select distinct combinations of 'dt', 'StoreID', 'CustomerID', and 'IsOnline'
    prepared_input_df = (
        input_df
        .select(key_fields + ['CustomerID', 'ProductID']).unique()
        .with_columns([pl.lit(1).alias('target').cast(pl.Int64)])
    )
    print('Transformation started...')
    cadidates_full_df = (
        input_df.select(key_fields + ['CustomerID']).unique()
        .join(
            input_df.select(key_fields + ['ProductID']).unique(),
            on=key_fields,
            how='inner'
        )
        .join(
            prepared_input_df,
            on=key_fields+['ProductID', 'CustomerID'],
            how='left'
        )
    )
    print(f"Negative candidates: {cadidates_full_df.filter(pl.col('target').is_null()).height}, Positive samples: {input_df.height}")
    negative_candidates_df = (
        cadidates_full_df.filter(pl.col('target').is_null())
        .sample(n=int(input_df.height * negatives_per_one_positive), seed=42)
        .with_columns([pl.lit(0).alias('target').cast(pl.Int64)])
    )
    user_item_df = (
        pl.concat([
            prepared_input_df.select(key_fields+['CustomerID', 'ProductID', 'target']),
            negative_candidates_df.select(key_fields+['CustomerID', 'ProductID', 'target'])
        ])
        .sort(by=['CustomerID', 'dt'])
    )
    user_item_df = user_item_df.with_columns([pl.col('dt').cast(pl.Date)])
    print(f"Num negatives {user_item_df.to_pandas()['target'].value_counts(normalize=True).to_dict().get(0)}")
    return user_item_df
```