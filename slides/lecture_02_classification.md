# Logistic regression

* [Jupyter Notebook](../jupyter_notebooks/vol_00_pre_requirements_02_machine_learning_classification.ipynb)
* [Naive bayes classifier](../jupyter_notebooks/vol_04_deep_dive_00_probability_hw_2_naive_bayes_solved.ipynb)

## LogLoss

Log Loss —ç—Ç–æ —Å–∏–Ω–æ–Ω–∏–º Binary Cross-Entropy (BCE).

–®—Ç—Ä–∞—Ñ—É–µ—Ç –ª–∏ logloss –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ –∫–ª–∞—Å—Å–æ–≤? –ò –µ—Å–ª–∏ –¥–∞, —Ç–æ –∑–∞—á–µ–º?

$$L(y, F(x)) = -y \log(p) - (1-y)\log(1-p)$$

–≥–¥–µ $p = \frac{1}{1 + e^{-F(x)}}$

- –ê–Ω—Ç–∏–≥—Ä–∞–¥–∏–µ–Ω—Ç: $r_{im} = y_i - p_i$

–î–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤ –Ω—É–∂–µ–Ω –ø–æ—Ä–æ–≥, –æ—à–∏–±–∫–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ (precision, recall) - –Ω–µ –≥–ª–∞–¥–∫–∏–µ.

–ú–æ–¥–µ–ª—å –≤—ã–¥–∞–µ—Ç –Ω–µ –º–µ—Ç–∫—É, –∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å - –ø—Ä–∏ –ø–æ—Ä–æ–≥–µ 0.5 –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø—Ä–æ–≥–Ω–æ–∑ –≤ 0.51 –º–æ–∂–µ—Ç –Ω–∞–∫–∞–∑—ã–≤–∞—Ç—å—Å—è –∑–∞ —Å–ª–∞–±—É—é —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å. 


# Classification metrics

**Binary classification problem**, confusion matrix:

|  | **Predicted Positive** | **Predicted Negative** |
| --- | --- | --- |
| **Actual Positive (1s)** | **True Positives (TP)** | **False Negatives (FN)** |
| **Actual Negative (0s)** | **False Positives (FP)** | **True Negatives (TN)** |
- **True Positives (TP)** ‚Üí Model correctly predicted **positive**.
- **True Negatives (TN)** ‚Üí Model correctly predicted **negative**.
- **False Positives (FP)** ‚Üí Model incorrectly predicted **positive** (Type I error).
- **False Negatives (FN)** ‚Üí Model incorrectly predicted **negative** (Type II error).

The **F1 score** is the harmonic mean of **precision** and **recall**:

$$F1 = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}$$

Compute Precision

$$\text{Precision} = \frac{TP}{TP + FP}$$

- Measures how many of the **predicted positives** were actually correct.


Compute Recall

$$\text{Recall} = \frac{TP}{TP + FN}$$

- Measures how many of the **actual positives** were correctly identified.

‚úÖ High recall ‚Üí The model catches most of the positives (few false negatives).

**When to Prioritize Recall?**

* **Medical Diagnosis (Cancer, COVID-19, etc.)** ‚Üí Missing a sick patient is dangerous.
* **Fraud Detection** ‚Üí Better to flag potential fraud than let it go unnoticed.
* **Security Systems (Intrusion Detection)** ‚Üí Better to have false alarms than miss real threats.

üîπ **Trade-off:** High recall can increase **false positives** (low precision).

Recall, also known as **Sensitivity** or **True Positive Rate (TPR)**, is a key metric in classification models, especially when missing positive cases is costly (e.g., medical diagnosis, fraud detection).

Sensitivity and specificity measure opposite aspects of model performance:

| Metric | Formula | Meaning |
|--------|---------|---------|
| **Sensitivity (Recall, TPR)** | $\frac{TP}{TP + FN}$ | **Ability to detect positives** |
| **Specificity (TNR)** | $\frac{TN}{TN + FP}$ | **Ability to detect negatives** |

* **High Sensitivity (Recall)** ‚Üí Good at finding **positives** (e.g., cancer screening).
* **High Specificity** ‚Üí Good at ruling out **negatives** (e.g., spam detection).

üí° **Trade-off**: Increasing recall often lowers specificity, and vice versa.

## Step 3: Compute F1 Score

$$F1 = 2 \times \frac{(TP/(TP + FP)) \times (TP/(TP + FN))}{(TP/(TP + FP)) + (TP/(TP + FN))}$$

---

**Where:**
- $TP$ = True Positives
- $FP$ = False Positives  
- $FN$ = False Negatives

**Use F1 when classes are imbalanced** (e.g., fraud detection, medical diagnosis).

- If **false positives & false negatives have different costs**, choose **precision or recall** instead:
    - **High Precision?** ‚Üí Model avoids **false positives**.
    - **High Recall?** ‚Üí Model avoids **false negatives**.

# ROC curve vs P-R curve

Both **Precision-Recall (P-R) Curves** and **ROC Curves** help evaluate classification models, but they are suited for **different scenarios**.

**ROC Curve** plots **True Positive Rate (Recall) vs. False Positive Rate (FPR)**:

- **Best for balanced datasets** where **positives & negatives are roughly equal**.
- **False positives matter less**, as FPR normalizes by total negatives.

**Use ROC Curve when**:

- Both **false positives (FP) and false negatives (FN) are equally important**.
- The dataset has **balanced classes** (e.g., disease detection where 50% have it).

ROC-AUC –º–æ–∂–µ—Ç –≤—ã–≥–ª—è–¥–µ—Ç—å –≤—ã—Å–æ–∫–æ–π –Ω–∞ –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –ø–æ—Ç–æ–º—É —á—Ç–æ –æ–Ω–∞ —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–∏ –∏ false positive rate, –∫–æ—Ç–æ—Ä—ã–µ –æ–ø—Ä–µ–¥–µ–ª—è—é—Ç—Å—è –º–∞–∂–æ—Ä–∏—Ç–∞—Ä–Ω—ã–º –∫–ª–∞—Å—Å–æ–º, —á–∞—Å—Ç–æ —Å–∫—Ä—ã–≤–∞—è –ø–ª–æ—Ö—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ –º–∏–Ω–æ—Ä–∏—Ç–∞—Ä–Ω–æ–º –∫–ª–∞—Å—Å–µ.

**1. –ù–µ—á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∫ –¥–∏—Å–±–∞–ª–∞–Ω—Å—É –∫–ª–∞—Å—Å–æ–≤**
ROC-AUC –æ—Å–Ω–æ–≤–∞–Ω–∞ –Ω–∞ true positive rate (TPR) –∏ false positive rate (FPR). –ö–æ–≥–¥–∞ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π –∫–ª–∞—Å—Å –¥–æ–º–∏–Ω–∏—Ä—É–µ—Ç, –¥–∞–∂–µ –±–æ–ª—å—à–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª–æ–∂–Ω–æ–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –º–∞–ª–æ–º—É FPR, –∏–∑-–∑–∞ —á–µ–≥–æ –º–æ–¥–µ–ª—å –≤—ã–≥–ª—è–¥–∏—Ç –ª—É—á—à–µ, —á–µ–º –µ—Å—Ç—å –Ω–∞ —Å–∞–º–æ–º –¥–µ–ª–µ.

**2. –•–æ—Ä–æ—à–∏–π —Ä–∞–Ω–∂–∏—Ä ‚â† —Ö–æ—Ä–æ—à–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è**
ROC-AUC –∏–∑–º–µ—Ä—è–µ—Ç, –Ω–∞—Å–∫–æ–ª—å–∫–æ —Ö–æ—Ä–æ—à–æ –º–æ–¥–µ–ª—å —Ä–∞–Ω–∂–∏—Ä—É–µ—Ç –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã –≤—ã—à–µ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö, –Ω–æ –Ω–µ —Ç–æ, –Ω–∞—Å–∫–æ–ª—å–∫–æ —Ç–æ—á–Ω–æ –æ–Ω–∞ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π –∫–ª–∞—Å—Å –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º–æ–º –ø–æ—Ä–æ–≥–µ. –ú–æ–¥–µ–ª—å –º–æ–∂–µ—Ç –∏–º–µ—Ç—å –≤—ã—Å–æ–∫–∏–π ROC-AUC, –Ω–æ –ø—Ä–∏ —ç—Ç–æ–º –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å –æ—á–µ–Ω—å –ø–ª–æ—Ö–∏–µ precision –∏–ª–∏ recall –¥–ª—è –º–∏–Ω–æ—Ä–∏—Ç–∞—Ä–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞.

–í–æ–ø—Ä–æ—Å: –ï—Å–ª–∏ –≤–æ–∑–≤–µ—Å—Ç–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∞–ª–≥–æ—Ä–∏—Ç–º–∞ –≤ –∫–≤–∞–¥—Ä–∞—Ç, —á—Ç–æ —Å—Ç–∞–Ω–µ—Ç —Å –º–µ—Ç—Ä–∏–∫–æ–π ROC AUC?

–û—Ç–≤–µ—Ç: –î–ª—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ [0,1] –≤–æ–∑–≤–µ–¥–µ–Ω–∏–µ –≤ –∫–≤–∞–¥—Ä–∞—Ç –ø—Ä–æ—Å—Ç–æ —Å–∂–∏–º–∞–µ—Ç –∏—Ö –∫ 0, –Ω–æ –º–æ–Ω–æ—Ç–æ–Ω–Ω–æ—Å—Ç—å —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç, –ø–æ—ç—Ç–æ–º—É –ø–æ—Ä—è–¥–æ–∫ –Ω–µ –º–µ–Ω—è–µ—Ç—Å—è -> ROC AUC –æ—Å—Ç–∞–Ω–µ—Ç—Å—è –Ω–µ–∏–∑–º–µ–Ω–Ω—ã–º, —Ç–∞–∫ –∫–∞–∫ –º–µ—Ç—Ä–∏–∫–∞ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è –∞–ª–≥–æ—Ä–∏—Ç–º–∞ - –∫–æ—Ç–æ—Ä–æ–µ –Ω–µ –∏–∑–º–µ–Ω–∏—Ç—Å—è –ø—Ä–∏ –≤–æ–∑–≤–µ–¥–µ–Ω–∏–∏ –≤ –∫–≤–∞–¥—Ä–∞—Ç. 

**3. –ò–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç —Ä–µ–∞–ª—å–Ω—É—é —Ä–∞–±–æ—á—É—é —Ç–æ—á–∫—É**
–ù–∞ –ø—Ä–∞–∫—Ç–∏–∫–µ –≤–∞–∂–Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø—Ä–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º –ø–æ—Ä–æ–≥–µ –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, —á–∞—Å—Ç–æ—Ç–∞ —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏—è –∞–ª–µ—Ä—Ç–æ–≤, –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –ø–æ –∑–∞—Ç—Ä–∞—Ç–∞–º). ROC-AUC —É—Å—Ä–µ–¥–Ω—è–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–æ –≤—Å–µ–º –ø–æ—Ä–æ–≥–∞–º, –º–Ω–æ–≥–∏–µ –∏–∑ –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ–∞–∫—Ç—É–∞–ª—å–Ω—ã –≤ —É—Å–ª–æ–≤–∏—è—Ö —Å–∏–ª—å–Ω–æ–≥–æ –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞.

**4. –ú–æ–∂–µ—Ç —Å–∫—Ä—ã–≤–∞—Ç—å –ø–ª–æ—Ö—É—é —Ä–∞–±–æ—Ç—É –Ω–∞ –º–∏–Ω–æ—Ä–∏—Ç–∞—Ä–Ω–æ–º –∫–ª–∞—Å—Å–µ**
–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä, –∫–æ—Ç–æ—Ä—ã–π –ø–æ—á—Ç–∏ –≤—Å–µ–≥–¥–∞ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –º–∞–∂–æ—Ä–∏—Ç–∞—Ä–Ω—ã–π –∫–ª–∞—Å—Å, –≤—Å—ë —Ä–∞–≤–Ω–æ –º–æ–∂–µ—Ç –¥–æ—Å—Ç–∏—á—å –æ–±–º–∞–Ω—á–∏–≤–æ –≤—ã—Å–æ–∫–æ–≥–æ ROC-AUC, –µ—Å–ª–∏ –æ–Ω —Ö–æ—Ç—å –Ω–µ–º–Ω–æ–≥–æ —Ä–∞–∑–¥–µ–ª—è–µ—Ç –∫–ª–∞—Å—Å—ã.

The **P-R Curve** plots **Precision vs. Recall**, focusing only on **positive class performance**:

- **Best for imbalanced datasets**, where **positives are rare** (e.g., fraud detection).
- **More informative when false positives are costly**.

**Use P-R Curve when**:

- **Positive class is rare** (e.g., cancer detection, fraud, spam filtering).
- You care more about **precision and recall trade-offs**.

| **Scenario** | **ROC Curve** | **P-R Curve** |
| --- | --- | --- |
| Balanced dataset (50-50) | ‚úÖ Yes | ‚ùå No |
| Imbalanced dataset (e.g., 1% positives) | ‚ùå No | ‚úÖ Yes |
| Medical diagnosis (minimizing false negatives) | ‚úÖ Yes | ‚úÖ Yes |
| Fraud detection (rare class) | ‚ùå No | ‚úÖ Yes |
| When FP rate is misleading | ‚ùå No | ‚úÖ Yes |

# –ö–µ–π—Å - lazy model

# –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: —Ç–µ—Å—Ç –∏–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤

## –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏

$$\text{Precision} = \frac{TP}{TP + FP} \quad \text{Recall} = \frac{TP}{TP + FN}$$

$$\text{F1} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}$$

---

## –ü—Ä–æ–±–ª–µ–º–∞: –∫–∞–∫ –æ–±–Ω–∞—Ä—É–∂–∏—Ç—å "–ª–µ–Ω–∏–≤—É—é" –º–æ–¥–µ–ª—å?

**"–õ–µ–Ω–∏–≤–∞—è" –º–æ–¥–µ–ª—å** - –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç —Ç–æ–ª—å–∫–æ –º–∞–∂–æ—Ä–∏—Ç–∞—Ä–Ω—ã–π –∫–ª–∞—Å—Å (–≤—Å–µ–≥–¥–∞ 1 –∏–ª–∏ –≤—Å–µ–≥–¥–∞ 0).

### –ü—Ä–∏–º–µ—Ä –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞

- –ö–ª–∞—Å—Å 1: 30,000 –ø—Ä–∏–º–µ—Ä–æ–≤ (83.3%)
- –ö–ª–∞—Å—Å 0: 6,000 –ø—Ä–∏–º–µ—Ä–æ–≤ (16.7%)
- **–ò—Ç–æ–≥–æ:** 36,000 –ø—Ä–∏–º–µ—Ä–æ–≤

---

## –°—Ü–µ–Ω–∞—Ä–∏–π 1: "–õ–µ–Ω–∏–≤–∞—è" –º–æ–¥–µ–ª—å

### –ü–æ–≤–µ–¥–µ–Ω–∏–µ: –º–æ–¥–µ–ª—å –≤—Å–µ–≥–¥–∞ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –∫–ª–∞—Å—Å 1

| | Pred 0 | Pred 1 |
|---------|--------|--------|
| **True 0** | 0 | 6,000 |
| **True 1** | 0 | 30,000 |

**–ú–µ—Ç—Ä–∏–∫–∏:**
- TP = 30,000, FP = 6,000, FN = 0, TN = 0
- Recall = $\frac{30,000}{30,000 + 0} = 1.0$ ‚úì
- Precision = $\frac{30,000}{30,000 + 6,000} = 0.833$ ‚úì

**‚ö†Ô∏è –ü—Ä–æ–±–ª–µ–º–∞:** –ú–µ—Ç—Ä–∏–∫–∏ –≤—ã–≥–ª—è–¥—è—Ç —Ö–æ—Ä–æ—à–æ, –Ω–æ –º–æ–¥–µ–ª—å –±–µ—Å–ø–æ–ª–µ–∑–Ω–∞!

### –¢–µ—Å—Ç: –∏–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ (0 ‚Üî 1)

–ü–æ—Å–ª–µ –∏–Ω–≤–µ—Ä—Å–∏–∏ –º–æ–¥–µ–ª—å –≤—Å—ë –µ—â—ë –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç "—Å—Ç–∞—Ä—ã–π –∫–ª–∞—Å—Å 1" = **–Ω–æ–≤—ã–π –∫–ª–∞—Å—Å 0**

| | Pred 0 (–Ω–æ–≤—ã–π) | Pred 1 (–Ω–æ–≤—ã–π) |
|---------|--------|--------|
| **True 0** | 30,000 | 0 |
| **True 1** | 6,000 | 0 |

**–ù–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏:**
- TP = 0, FP = 0, FN = 6,000
- Recall = $\frac{0}{0 + 6,000} = 0$ ‚ùå
- Precision = undefined (0/0) ‚ùå

**–í—ã–≤–æ–¥:** –ú–µ—Ç—Ä–∏–∫–∏ —É–ø–∞–ª–∏ –≤ –Ω–æ–ª—å ‚Üí –º–æ–¥–µ–ª—å –ª–µ–Ω–∏–≤–∞—è!

---

## –°—Ü–µ–Ω–∞—Ä–∏–π 2: –•–æ—Ä–æ—à–æ –æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å

### –ò—Å—Ö–æ–¥–Ω–∞—è confusion matrix

| | Pred 0 | Pred 1 |
|---------|--------|--------|
| **True 0** | 5,500 | 500 |
| **True 1** | 600 | 29,400 |

**–ú–µ—Ç—Ä–∏–∫–∏:**
- Recall = $\frac{29,400}{29,400 + 600} = 0.98$
- Precision = $\frac{29,400}{29,400 + 500} = 0.983$

### –ü–æ—Å–ª–µ –∏–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤

| | Pred 0 | Pred 1 |
|---------|--------|--------|
| **True 0** | 29,400 | 600 |
| **True 1** | 500 | 5,500 |

**–ù–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏:**
- Recall = $\frac{5,500}{5,500 + 500} = 0.917$
- Precision = $\frac{5,500}{5,500 + 600} = 0.901$

**–í—ã–≤–æ–¥:** –ú–µ—Ç—Ä–∏–∫–∏ –æ—Å—Ç–∞—é—Ç—Å—è –≤—ã—Å–æ–∫–∏–º–∏ ‚Üí –º–æ–¥–µ–ª—å –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–∞!

---

## –ü—Ä–∞–≤–∏–ª–æ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏

| –ü—Ä–∏–∑–Ω–∞–∫ | –õ–µ–Ω–∏–≤–∞—è –º–æ–¥–µ–ª—å | –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å |
|---------|----------------|------------------|
| **Recall –Ω–∞ –º–∞–∂–æ—Ä–∏—Ç–∞—Ä–Ω–æ–º –∫–ª–∞—Å—Å–µ** | ‚âà 1.0 | < 1.0 (–µ—Å—Ç—å –æ—à–∏–±–∫–∏) |
| **Precision** | ‚âà –¥–æ–ª—è –º–∞–∂–æ—Ä–∏—Ç–∞—Ä–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞ | > –¥–æ–ª—è –∫–ª–∞—Å—Å–∞ |
| **–ü–æ—Å–ª–µ –∏–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–∏—è** | –ú–µ—Ç—Ä–∏–∫–∏ ‚Üí 0 | –ú–µ—Ç—Ä–∏–∫–∏ –æ—Å—Ç–∞—é—Ç—Å—è –≤—ã—Å–æ–∫–∏–º–∏ |
| **Confusion matrix** | –û–¥–Ω–∞ —Å—Ç—Ä–æ–∫–∞/—Å—Ç–æ–ª–±–µ—Ü = 0 | –í—Å–µ —è—á–µ–π–∫–∏ –∑–∞–ø–æ–ª–Ω–µ–Ω—ã |

---

## –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏

### 1. –í—Å–µ–≥–¥–∞ –ø—Ä–æ–≤–µ—Ä—è–π Confusion Matrix

```python
from sklearn.metrics import confusion_matrix
import seaborn as sns

cm = confusion_matrix(y_true, y_pred)
sns.heatmap(cm, annot=True, fmt='d')
```

**–ö—Ä–∞—Å–Ω—ã–µ —Ñ–ª–∞–≥–∏:**
- –°—Ç—Ä–æ–∫–∞ –∏–ª–∏ —Å—Ç–æ–ª–±–µ—Ü –ø–æ–ª–Ω–æ—Å—Ç—å—é –Ω—É–ª–µ–≤—ã–µ
- –î–∏–∞–≥–æ–Ω–∞–ª—å –∏–º–µ–µ—Ç –æ–¥–∏–Ω –Ω—É–ª–µ–≤–æ–π —ç–ª–µ–º–µ–Ω—Ç

### 2. –ò—Å–ø–æ–ª—å–∑—É–π –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏

```python
from sklearn.metrics import balanced_accuracy_score, f1_score

# –£—á–∏—Ç—ã–≤–∞–µ—Ç –¥–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤
balanced_acc = balanced_accuracy_score(y_true, y_pred)

# –ì–∞—Ä–º–æ–Ω–∏—á–µ—Å–∫–æ–µ —Å—Ä–µ–¥–Ω–µ–µ Precision –∏ Recall
f1 = f1_score(y_true, y_pred)
```

### 3. –¢–µ—Å—Ç –∏–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–∏—è

```python
# –ò–Ω–≤–µ—Ä—Ç–∏—Ä—É–π –∫–ª–∞—Å—Å—ã
y_true_inv = 1 - y_true
y_pred_inv = 1 - y_pred

# –ü–µ—Ä–µ—Å—á–∏—Ç–∞–π –º–µ—Ç—Ä–∏–∫–∏
recall_inv = recall_score(y_true_inv, y_pred_inv)

# –ï—Å–ª–∏ —É–ø–∞–ª–∏ –≤ –Ω–æ–ª—å ‚Üí –ª–µ–Ω–∏–≤–∞—è –º–æ–¥–µ–ª—å
if recall_inv < 0.1:
    print("‚ö†Ô∏è –ú–æ–¥–µ–ª—å –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω –∫–ª–∞—Å—Å!")
```

### 4. –ë–æ—Ä—å–±–∞ —Å –¥–∏—Å–±–∞–ª–∞–Ω—Å–æ–º

**–í–∑–≤–µ—à–∏–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤:**
```python
from sklearn.linear_model import LogisticRegression

model = LogisticRegression(class_weight='balanced')
```

**–†–µ—Å–µ–º–ø–ª–∏–Ω–≥:**
```python
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler

# Oversampling –º–∏–Ω–æ—Ä–∏—Ç–∞—Ä–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞
smote = SMOTE(sampling_strategy=0.5)
X_res, y_res = smote.fit_resample(X, y)

# Undersampling –º–∞–∂–æ—Ä–∏—Ç–∞—Ä–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞
rus = RandomUnderSampler(sampling_strategy=0.8)
X_res, y_res = rus.fit_resample(X, y)
```

---

## –ë—ã—Å—Ç—Ä–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∞—è —Ç–∞–±–ª–∏—Ü–∞

| –ú–µ—Ç—Ä–∏–∫–∏ | –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è | –î–µ–π—Å—Ç–≤–∏–µ |
|---------|---------------|----------|
| Recall=1.0, Precision=–¥–æ–ª—è_–∫–ª–∞—Å—Å–∞ | üö® –õ–µ–Ω–∏–≤–∞—è –º–æ–¥–µ–ª—å | –ü—Ä–æ–≤–µ—Ä–∏—Ç—å CM, –∏–∑–º–µ–Ω–∏—Ç—å –≤–µ—Å–∞ |
| Recall‚âà1.0, Precision>>–¥–æ–ª—è_–∫–ª–∞—Å—Å–∞ | ‚úÖ –•–æ—Ä–æ—à–∞—è –º–æ–¥–µ–ª—å –Ω–∞ –º–∞–∂–æ—Ä–∏—Ç–∞—Ä–Ω–æ–º | –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –º–∏–Ω–æ—Ä. –∫–ª–∞—Å—Å |
| Recall<0.9, Precision>0.9 | ‚ö†Ô∏è –ö–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å | –ü–æ—Ä–æ–≥ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏? |
| –û–±–µ –º–µ—Ç—Ä–∏–∫–∏ ~0.5 | ü§∑ –†–∞–Ω–¥–æ–º–Ω–∞—è –º–æ–¥–µ–ª—å | –ü–µ—Ä–µ–æ–±—É—á–∏—Ç—å —Å –Ω—É–ª—è |

---

## –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ

### –ü–æ—á–µ–º—É Precision = –¥–æ–ª—è –º–∞–∂–æ—Ä–∏—Ç–∞—Ä–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞?

–ï—Å–ª–∏ –º–æ–¥–µ–ª—å –≤—Å–µ–≥–¥–∞ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –∫–ª–∞—Å—Å 1:

$$\text{Precision} = \frac{TP}{TP + FP} = \frac{n_1}{n_1 + n_0} = \frac{n_1}{n_{total}}$$

–≥–¥–µ $n_1$ - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –∫–ª–∞—Å—Å–∞ 1

**–ü—Ä–∏–º–µ—Ä:**
$$\frac{30,000}{36,000} = 0.833 = 83.3\%$$

### –ü—Ä–∏ –∏–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ –∫–ª–∞—Å—Å–æ–≤

$$\begin{aligned}
TP_{new} &= TN_{old} \\
FP_{new} &= FN_{old} \\
FN_{new} &= FP_{old} \\
TN_{new} &= TP_{old}
\end{aligned}$$

–î–ª—è –ª–µ–Ω–∏–≤–æ–π –º–æ–¥–µ–ª–∏: $TN_{old} = 0$ ‚Üí $TP_{new} = 0$ ‚Üí –º–µ—Ç—Ä–∏–∫–∏ –ø–∞–¥–∞—é—Ç –≤ 0

---

## –ó–∞–∫–ª—é—á–µ–Ω–∏–µ

**–ö–ª—é—á–µ–≤–∞—è –∏–¥–µ—è:** –ù–∞ –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤—ã—Å–æ–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –º–æ–≥—É—Ç –±—ã—Ç—å –æ–±–º–∞–Ω—á–∏–≤—ã.

**–ü—Ä–æ–≤–µ—Ä–æ—á–Ω—ã–π —Ç–µ—Å—Ç:** –ò–Ω–≤–µ—Ä—Ç–∏—Ä—É–π –∫–ª–∞—Å—Å—ã –∏ –ø–µ—Ä–µ—Å—á–∏—Ç–∞–π –º–µ—Ç—Ä–∏–∫–∏
- –£–ø–∞–ª–∏ –≤ –Ω–æ–ª—å ‚Üí –ª–µ–Ω–∏–≤–∞—è –º–æ–¥–µ–ª—å
- –û—Å—Ç–∞–ª–∏—Å—å –≤—ã—Å–æ–∫–∏–º–∏ ‚Üí –º–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞

**–í—Å–µ–≥–¥–∞ —Å–º–æ—Ç—Ä–∏:**
1. Confusion Matrix
2. –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –û–ë–û–ò–• –∫–ª–∞—Å—Å–æ–≤
3. Balanced Accuracy / F1-score
4. –ú–µ—Ç—Ä–∏–∫–∏ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –ø–æ—Å–ª–µ class_weight –∏–ª–∏ resampling

# Threshold tuning

**Default Threshold Limitations**

- The default threshold (0.5) often favors the majority class in imbalanced settings, leading to poor recall/precision for the minority class[1](https://machinelearningmastery.com/threshold-moving-for-imbalanced-classification/)[4](https://machinelearningmastery.com/tour-of-evaluation-metrics-for-imbalanced-classification/).
- Example: A model might achieve 99% accuracy by always predicting the majority class but fail to detect critical minority cases (e.g., fraud or disease)[4](https://machinelearningmastery.com/tour-of-evaluation-metrics-for-imbalanced-classification/).

## **ROC Curve Analysis**

- Identify the threshold that maximizes the¬†**Youden‚Äôs J statistic**¬†(J = TPR + TNR - 1) or balances True Positive Rate (TPR) and False Positive Rate (FPR)[1](https://machinelearningmastery.com/threshold-moving-for-imbalanced-classification/).
- Example: Use the¬†**`roc_curve`**¬†function in¬†**`scikit-learn`**¬†to extract thresholds and select the point closest to the top-left corner of the ROC curve.

## **Precision-Recall Curve Analysis**

- Focus on thresholds that balance precision and recall, especially useful when the minority class is critical[1](https://machinelearningmastery.com/threshold-moving-for-imbalanced-classification/)[4](https://machinelearningmastery.com/tour-of-evaluation-metrics-for-imbalanced-classification/).
- Example: Optimize for the¬†**F1-score**¬†(harmonic mean of precision and recall) or target a specific recall value.

Grid Search for Threshold Tuning

```python
import numpy as np
from sklearn.metrics import confusion_matrix
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split

X, y = make_classification(n_samples=1000, n_features=20, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

clf = LogisticRegression()
clf.fit(X_train, y_train)

y_probs = clf.predict_proba(X_test)[:, 1]

# Define costs
cost_fp = 1
cost_fn = 5

thresholds = np.linspace(0, 1, 100)
min_cost = float('inf')
best_threshold = 0.5

for threshold in thresholds:
    y_pred = (y_probs >= threshold).astype(int)
    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()
    
    cost = cost_fp * fp + cost_fn * fn
    if cost < min_cost:
        min_cost = cost
        best_threshold = threshold

print(f"Best threshold: {best_threshold:.2f} with minimum cost: {min_cost}")
```

# Negative sampling

```python
def prepare_evaluation_df(input_df, negatives_per_one_positive=2):
    input_df = pl.from_pandas(input_df) if isinstance(input_df, pd.DataFrame) else input_df
    key_fields = ['dt', 'state_name', 'StoreID', 'IsOnline']
    # Select distinct combinations of 'dt', 'StoreID', 'CustomerID', and 'IsOnline'
    prepared_input_df = (
        input_df
        .select(key_fields + ['CustomerID', 'ProductID']).unique()
        .with_columns([pl.lit(1).alias('target').cast(pl.Int64)])
    )
    print('Transformation started...')
    cadidates_full_df = (
        input_df.select(key_fields + ['CustomerID']).unique()
        .join(
            input_df.select(key_fields + ['ProductID']).unique(),
            on=key_fields,
            how='inner'
        )
        .join(
            prepared_input_df,
            on=key_fields+['ProductID', 'CustomerID'],
            how='left'
        )
    )
    print(f"Negative candidates: {cadidates_full_df.filter(pl.col('target').is_null()).height}, Positive samples: {input_df.height}")
    negative_candidates_df = (
        cadidates_full_df.filter(pl.col('target').is_null())
        .sample(n=int(input_df.height * negatives_per_one_positive), seed=42)
        .with_columns([pl.lit(0).alias('target').cast(pl.Int64)])
    )
    user_item_df = (
        pl.concat([
            prepared_input_df.select(key_fields+['CustomerID', 'ProductID', 'target']),
            negative_candidates_df.select(key_fields+['CustomerID', 'ProductID', 'target'])
        ])
        .sort(by=['CustomerID', 'dt'])
    )
    user_item_df = user_item_df.with_columns([pl.col('dt').cast(pl.Date)])
    print(f"Num negatives {user_item_df.to_pandas()['target'].value_counts(normalize=True).to_dict().get(0)}")
    return user_item_df
```

ROC-AUC can be deceptive on imbalanced datasets because it does not reflect how the model performs on the minority (often the most important) class.

Key reasons:

* Insensitive to class imbalance. ROC-AUC is based on true positive rate (TPR) and false positive rate (FPR). When the negative class dominates, even a large number of false positives can result in a small FPR, making the model look better than it actually is.

* Good ranking ‚â† good predictions ROC-AUC measures how well the model ranks positives above negatives, not how well it predicts the positive class at a usable threshold. A model can have a high ROC-AUC while producing very poor precision or recall for the minority class.

* Ignores real operating point. In practice, you care about performance at a specific decision threshold (e.g., alert rate, cost constraint). ROC-AUC averages performance over all thresholds, many of which are irrelevant in highly imbalanced settings.

* Can hide poor minority-class performance. A classifier that almost always predicts the majority class can still achieve a deceptively high ROC-AUC if it slightly separates the classes.

Better alternatives for imbalanced problems

* Precision‚ÄìRecall AUC (PR-AUC)
* Recall, Precision, F1-score at a chosen threshold
* Cost-sensitive metrics aligned with business impact