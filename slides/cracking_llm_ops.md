# LLM ops

[Youtube|Where Does the Memory Go? (7B)](https://www.youtube.com/watch?v=zSNk0FC3rr8)

- **14 GB** â€” model weights  
- **14 GB** â€” gradients (peak)  
- **84 GB** â€” optimizer states (Adam, mixed precision)  
- **40 GB** â€” activations


# LLM optimization tools

| Category | Purpose | Examples |
| --- | --- | --- |
| **Memory/Speed Hacks** | Speed up training/inference | FlashAttention, xFormers, DeepSpeed |
| **Efficient Inference** | Serve models faster + at scale | vLLM, TensorRT-LLM, TGI |
| **Compression** | Reduce model size | Quantization (GPTQ, AWQ), Pruning |

## ðŸ” 1. **FlashAttention** â€” Faster Attention

- **What:** A memory-efficient implementation of attention using CUDA kernel fusion.
- **Why:** Standard attention scales poorly (`O(nÂ²)` in memory); FlashAttention reduces memory overhead and increases speed.
- **Result:** You can train or run **longer sequences** (e.g., 8K, 32K tokens) on the same hardware.
- **Used in:** GPT-NeoX, LLaMA 2+, Mistral, OpenChat

âœ… Speeds up training & inference

âœ… Reduces GPU memory consumption

---

## ðŸš€ 2. **vLLM** â€” Efficient LLM Inference Engine

VLLM explained

- **What:** A high-performance inference engine for LLMs focused on **serving at scale**.
- **Key feature:** **PagedAttention**, which allows dynamic batching of requests with varying lengths, avoiding GPU memory waste.
- **Supports:** HuggingFace models (OPT, LLaMA, Mistral, Falcon, etc.)
- **Use case:** Serve LLMs in production with high throughput.

âœ… Up to **10x higher throughput** than naÃ¯ve HuggingFace pipelines

âœ… Plug-and-play with many LLMs

---

## ðŸ§  3. **Quantization Methods** â€” Smaller & Faster Models

Quantization reduces the **precision** of weights (from FP32 to INT8, INT4, etc.) to make models run faster and use less memory â€” often **without a big accuracy loss**.

### Popular Quantization Tools:

| Tool | Description |
| --- | --- |
| **GPTQ** | Post-training quantization, used in LLaMA models |
| **AWQ** | Outlier-aware quantization, reduces degradation |
| **BitsAndBytes** | HuggingFace-compatible 8-bit/4-bit loading |
| **ExLlama / ExLlamaV2** | Fast 4-bit inference on GPUs |

âœ… Load 13B+ models on consumer GPUs (e.g., RTX 3090)

âœ… Useful for edge deployment, local inference

---

## ðŸ§° 4. **DeepSpeed** â€” End-to-End Training Optimizer

- **What:** A full training optimization library for LLMs from Microsoft.
- **Features:**
    - **ZeRO Offload**: shard model states across GPUs
    - **Activation checkpointing**: reduce memory
    - **FP16/bfloat16 training**: lower precision = faster training
- **Use case:** Train massive models like Bloom, GPT-J, etc.

âœ… Enables model training that would otherwise not fit into memory

---

## ðŸ§± 5. **TensorRT-LLM** (NVIDIA)

- Optimized LLM inference engine that compiles models to **GPU-native kernels**.
- Integrates quantization (INT8, FP8), FlashAttention, and graph optimizations.
- **Highly performant** on NVIDIA hardware (H100, A100, L4, etc.)

---

## ðŸ“¡ 6. **TGI** (Text Generation Inference)

- **What:** HuggingFaceâ€™s scalable serving system for LLMs.
- **Includes:** Token streaming, batching, quantization support.
- **Used for:** Open-source LLM deployments like Mistral or Falcon

---

## ðŸ’¡ When to Use What?

| Use Case | Tool(s) |
| --- | --- |
| Run LLMs on limited hardware | GPTQ, AWQ, BitsAndBytes |
| Fast inference in production | vLLM, TensorRT-LLM, TGI |
| Efficient training on multiple GPUs | DeepSpeed, FlashAttention, FSDP |
| Serve 4-bit quantized models | ExLlama, llama.cpp (GGUF) |

# Model inference

| Ð¤Ð¾Ñ€Ð¼Ð°Ñ‚ | Ð“Ð´Ðµ ÑƒÐ¼ÐµÑÑ‚ÐµÐ½ | ÐŸÑ€ÐµÐ¸Ð¼ÑƒÑ‰ÐµÑÑ‚Ð²Ð° |
| --- | --- | --- |
| GGUF | Ð›Ð¾ÐºÐ°Ð»ÑŒÐ½Ñ‹Ð¹ Ð·Ð°Ð¿ÑƒÑÐº LLM, Ð±ÐµÐ· GPU | Ð‘Ñ‹ÑÑ‚Ñ€Ð¾, Ð¼Ð°Ð»Ð¾ Ð¿Ð°Ð¼ÑÑ‚Ð¸, Ð»ÐµÐ³ÐºÐ¾ Ð·Ð°Ð¿ÑƒÑÐºÐ°Ñ‚ÑŒ |
| ONNX | ÐšÑ€Ð¾ÑÑ-Ñ„Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€Ðº, production-Ð¸Ð½Ñ„Ñ€Ð°ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð° | Ð£Ð½Ð¸Ð²ÐµÑ€ÑÐ°Ð»ÑŒÐ½Ð¾ÑÑ‚ÑŒ, Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶ÐºÐ° Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ð¹ |
| TorchScript | PyTorch-only Ð¿Ñ€Ð¾Ð´Ð°ÐºÑˆÐµÐ½ | Ð’Ñ‹ÑÐ¾ÐºÐ°Ñ ÑÐºÐ¾Ñ€Ð¾ÑÑ‚ÑŒ, Ð²Ð½Ðµ Python |
| SavedModel | TensorFlow-Ð¿Ñ€Ð¾ÐµÐºÑ‚Ñ‹ | ÐŸÐ¾Ð»Ð½Ð°Ñ Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶ÐºÐ° TF-Ñ„Ð¸Ñ‡ÐµÐ¹ |
| TFLite | ÐœÐ¾Ð±Ð¸Ð»ÑŒÐ½Ñ‹Ðµ Ð¸ IoT | ÐœÐ¸Ð½Ð¸Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ñ€Ð°Ð·Ð¼ÐµÑ€, Ð½Ð¸Ð·ÐºÐ¾Ðµ Ð¿Ð¾Ñ‚Ñ€ÐµÐ±Ð»ÐµÐ½Ð¸Ðµ |
| OpenVINO IR | Intel ÑƒÑÑ‚Ñ€Ð¾Ð¹ÑÑ‚Ð²Ð°, edge inference | ÐžÑ‚Ð»Ð¸Ñ‡Ð½Ð°Ñ Ð¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚ÑŒ Ð½Ð° Intel |

## ðŸ”§ **BentoML**, **LitAPI**, and **LitServe** Explained

These are modern tools in the **LLM deployment ecosystem**â€”each focused on **model serving**, **scaling**, and **production readiness**. Letâ€™s go through them one by one and then tie them together with your example of using **vLLM** with **LitServe**.

---

### ðŸ¥¡ **BentoML**

**What it is:**

A **framework for serving ML models** in production. Think of it as a full-service platform to:

- Package ML models (from PyTorch, TensorFlow, HuggingFace, etc.)
- Build **inference APIs**
- Deploy models to **local**, **cloud**, **Kubernetes**, or even **serverless** environments

**Key features:**

- Model packaging (`bentoml.Model`)
- API definition using FastAPI or Starlette-style syntax
- Supports custom runners (for things like Triton or vLLM)
- Scalable serving with containerization and Yatai (its model registry & deployment platform)

ðŸ“¦ *BentoML = model packaging + serving + deployment infrastructure*

---

### âš¡ **LitAPI** & **LitServe** (from **Lightning AI**)

These are **newer, lightweight serving components** built by the creators of PyTorch Lightning.

### ðŸ”¹ **LitAPI**

- A **minimal web server framework** to define your modelâ€™s HTTP interface.
- You define "endpoints" (`@app.get("/generate")`, etc.), very similar to FastAPI.
- Focused on being fast, lightweight, and production-ready.

### ðŸ”¹ **LitServe**

- A **serving layer** that wraps your LitAPI into a **scalable inference server**.
- Supports autoscaling, batching, and request queueing.
- Can integrate with **different inference engines**, like HuggingFace pipelines, Triton, or **vLLM**.

ðŸ“¡ *LitServe = production-grade wrapper for LitAPI apps with scaling & performance optimizations*

---

### ðŸ¤– **vLLM** â€” the Inference Engine

- Highly optimized **inference engine for LLMs** (like LLaMA, Mistral, etc.).
- Known for:
    - **PagedAttention** â€“ efficient memory usage
    - **Continuous batching** â€“ serving multiple users concurrently
    - Native HuggingFace support

ðŸ’¡ *Think of it like a high-performance engine under the hood, built for fast LLM inference.*

---

## ðŸ”„ Putting It All Together

Hereâ€™s what your sentence is saying, with explanation:

> "LitServe is not a replacement of vLLM, but you could use vLLM as the Inference Engine, and LitServe as the Inference Server, serving that engine."
> 
- vLLM does **fast inference** (engine).
- LitServe **wraps it in a scalable HTTP server** (inference API + request routing).
- This means you can:
    - Load your model using `from vllm import LLM`
    - Wrap it with a **LitAPI**
    - Deploy it using **LitServe** for **autoscaling, batching, and endpoint serving**

---

## âœ… Summary

| Component | Role in Pipeline | Example Tool |
| --- | --- | --- |
| Inference Engine | Runs the model fast | `vLLM`, `Transformers`, `Triton` |
| Inference API | Defines HTTP/REST interface | `LitAPI`, `FastAPI`, `Flask` |
| Inference Server | Scales and serves the API | `LitServe`, `BentoML`, `TGI` |
| Deployment | Sends it to cloud / prod | `Lightning`, `BentoML`, `Kubernetes` |

---

Let me know if you want a working code example of wrapping `vLLM` with `LitAPI` + `LitServe`, or a comparison with **BentoML** vs **LitServe**!

Regarding ONNX

| Role | Tool/Format |
| --- | --- |
| **Model Format** | âœ… ONNX, TorchScript, SavedModel |
| **Inference Engine** | âœ… ONNX Runtime, TensorRT, vLLM |
| **API Layer** | âœ… FastAPI, LitAPI, BentoML APIs |
| **Inference Server** | âœ… BentoML, LitServe, TGI, Triton |
| **Deployment Infra** | âœ… Kubernetes, Docker, Lightning AI |

---

# Refs

LoRA fine tuning: [Collab notebook](https://colab.research.google.com/#fileId=https://huggingface.co/agents-course/notebooks/blob/main/bonus-unit1/bonus-unit1.ipynb)

[Designing Data-Intensive applications](https://cloud.mail.ru/public/Xwje/GsKPVbagV)

[DevOps for data science](https://do4ds.com/chapters/intro.html)

[Prompt handbook](https://www.linkedin.com/feed/update/ugcPost:7208896181089808384)