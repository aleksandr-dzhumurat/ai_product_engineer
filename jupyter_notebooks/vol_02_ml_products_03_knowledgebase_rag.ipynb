{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23ad40ec-f4ed-4283-8eb3-d7541d70ad76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data dir content /Users/adzhumurat/PycharmProjects/ai_product_engineer/data: client_segmentation.csv, messages.db, labeled_data_corpus.csv, chroma, content_description.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "run_env = os.getenv('RUN_ENV', 'COLLAB')\n",
    "if run_env == 'COLLAB':\n",
    "  from google.colab import drive\n",
    "  ROOT_DIR = '/content/drive'\n",
    "  drive.mount(ROOT_DIR)\n",
    "  print('Google drive connected')\n",
    "  root_data_dir = os.path.join(ROOT_DIR, 'MyDrive', 'ml_course_data')\n",
    "  lib_path = os.path.join(ROOT_DIR, 'MyDrive', 'src')\n",
    "  if not os.path.exists(lib_path):\n",
    "    raise RuntimeError('Upload and `src` dir with code')\n",
    "  sys.path.append(lib_path)\n",
    "else:\n",
    "  root_data_dir = os.getenv('DATA_DIR', '/srv/data')\n",
    "\n",
    "if not os.path.exists(root_data_dir):\n",
    "  raise RuntimeError('Data dir not exists')\n",
    "else:\n",
    "  print('Data dir content %s: %s' % (root_data_dir, ', '.join(os.listdir(root_data_dir)[:5])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb16d7b4-fae6-4efb-9c83-51fd5db6ea82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to ChromaDB service at http://localhost:8000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<chromadb.api.client.Client at 0x1076d4dd0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rag.connections import get_chroma_client\n",
    "\n",
    "client = get_chroma_client()\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "701cbca7-a305-4e03-a19a-b781397526dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1536,)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "host = 'http://0.0.0.0:11434'\n",
    "query = 'что такое антиградиент'\n",
    "timeout = 5\n",
    "\n",
    "model = 'granite4:350m'\n",
    "model = 'qwen2.5:1.5b'\n",
    "\n",
    "url = host.rstrip(\"/\") + \"/api/embeddings\"\n",
    "response = requests.post(\n",
    "    url,\n",
    "    json={\"model\": model, \"prompt\": query},\n",
    "    timeout=timeout,\n",
    ")\n",
    "response.raise_for_status()\n",
    "data = response.json()\n",
    "\n",
    "query_embedding = data.get(\"embedding\") or data.get(\"embeddings\")\n",
    "print(np.array(query_embedding).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ee6cab-a833-4051-bc3e-d5bb391b7c0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "672be5e8-790c-4681-a757-9c8d28629fa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['44910ac67cd260627cc10b7a1e98ffc0_5',\n",
       "   '521cd333018c5db9ad1eaf97b0c7b5dd_57',\n",
       "   'e2a544eb5aafdaa54e2eece40381c0ef_27',\n",
       "   '521cd333018c5db9ad1eaf97b0c7b5dd_49',\n",
       "   'e102ed911c56b270ee42a7663c35e4b8_1',\n",
       "   '521cd333018c5db9ad1eaf97b0c7b5dd_39',\n",
       "   '11ca17fdd8bb5b27217f9691effa4e03_5',\n",
       "   '87e5ac00900a7395167b7338d840887e_6',\n",
       "   '44afae845aa8ae52cd29d3ec7e50f745_14',\n",
       "   'e2a544eb5aafdaa54e2eece40381c0ef_39']],\n",
       " 'distances': [[5993.615,\n",
       "   6101.614,\n",
       "   6239.2065,\n",
       "   6521.266,\n",
       "   6534.8027,\n",
       "   6565.432,\n",
       "   6649.8496,\n",
       "   6649.8496,\n",
       "   6664.7915,\n",
       "   6682.0903]],\n",
       " 'embeddings': None,\n",
       " 'metadatas': [[{'source_file_name': 'vol_04_deep_dive_09_trees_boosting.ipynb',\n",
       "    'chunk_index': '5',\n",
       "    'source': 'data/md_docs/vol_04_deep_dive_09_trees_boosting.md',\n",
       "    'source_dir': '/Users/adzhumurat/PycharmProjects/ai_product_engineer/jupyter_notebooks'},\n",
       "   {'chunk_index': '57',\n",
       "    'source_file_name': 'vol_04_deep_dive_00_probability.ipynb',\n",
       "    'source': 'data/md_docs/vol_04_deep_dive_00_probability.md',\n",
       "    'source_dir': '/Users/adzhumurat/PycharmProjects/ai_product_engineer/jupyter_notebooks'},\n",
       "   {'source': 'data/md_docs/vol_00_pre_requirements_02_machine_learning_classification.md',\n",
       "    'source_file_name': 'vol_00_pre_requirements_02_machine_learning_classification.ipynb',\n",
       "    'chunk_index': '27',\n",
       "    'source_dir': '/Users/adzhumurat/PycharmProjects/ai_product_engineer/jupyter_notebooks'},\n",
       "   {'source_file_name': 'vol_04_deep_dive_00_probability.ipynb',\n",
       "    'chunk_index': '49',\n",
       "    'source': 'data/md_docs/vol_04_deep_dive_00_probability.md',\n",
       "    'source_dir': '/Users/adzhumurat/PycharmProjects/ai_product_engineer/jupyter_notebooks'},\n",
       "   {'chunk_index': '1',\n",
       "    'source_dir': '/Users/adzhumurat/PycharmProjects/ai_product_engineer/jupyter_notebooks',\n",
       "    'source_file_name': 'vol_00_pre_requirements_01_machine_learning_intro.ipynb',\n",
       "    'source': 'data/md_docs/vol_00_pre_requirements_01_machine_learning_intro.md'},\n",
       "   {'source_file_name': 'vol_04_deep_dive_00_probability.ipynb',\n",
       "    'source_dir': '/Users/adzhumurat/PycharmProjects/ai_product_engineer/jupyter_notebooks',\n",
       "    'source': 'data/md_docs/vol_04_deep_dive_00_probability.md',\n",
       "    'chunk_index': '39'},\n",
       "   {'source': 'data/md_docs/vol_04_deep_dive_10_unsupervised_learning_implementation.md',\n",
       "    'source_dir': '/Users/adzhumurat/PycharmProjects/ai_product_engineer/jupyter_notebooks',\n",
       "    'chunk_index': '5',\n",
       "    'source_file_name': 'vol_04_deep_dive_10_unsupervised_learning_implementation.ipynb'},\n",
       "   {'source_file_name': 'vol_04_deep_dive_06_unsupervised_learning_implementation.ipynb',\n",
       "    'chunk_index': '6',\n",
       "    'source_dir': '/Users/adzhumurat/PycharmProjects/ai_product_engineer/jupyter_notebooks',\n",
       "    'source': 'data/md_docs/vol_04_deep_dive_06_unsupervised_learning_implementation.md'},\n",
       "   {'source': 'data/md_docs/vol_00_pre_requirements_05_unsupervised_intro.md',\n",
       "    'source_dir': '/Users/adzhumurat/PycharmProjects/ai_product_engineer/jupyter_notebooks',\n",
       "    'chunk_index': '14',\n",
       "    'source_file_name': 'vol_00_pre_requirements_05_unsupervised_intro.ipynb'},\n",
       "   {'source': 'data/md_docs/vol_00_pre_requirements_02_machine_learning_classification.md',\n",
       "    'chunk_index': '39',\n",
       "    'source_file_name': 'vol_00_pre_requirements_02_machine_learning_classification.ipynb',\n",
       "    'source_dir': '/Users/adzhumurat/PycharmProjects/ai_product_engineer/jupyter_notebooks'}]],\n",
       " 'documents': [[\"```python\\nfrom sklearn import tree\\n\\nclf = tree.DecisionTreeClassifier(criterion='entropy').fit(X, y)\\n```\\n\\nСчитаем Accuracy\\n\\n```python\\npredicted_classes = clf.predict(X)\\n\\naccuracy = (predicted_classes == y).sum() / len(y)\\n\\nprint(f'Точность предсказаний: {accuracy}')\\n```\\n\\n```text\\nТочность предсказаний: 1.0\\n```\\n\\nВизуализация\\n\\n```python\\n%matplotlib inline\\nfrom matplotlib import pyplot as plt\\n\\nfig = plt.figure(figsize=(15,10))\\n_ = (\\n    tree.plot_tree(\\n        clf,\\n        feature_names=feature_cols,\\n        class_names=y_labels,\\n        filled=True\\n    )\\n)\\n```\\n\\n```text\\n<Figure size 1500x1000 with 1 Axes>\\n```\\n\\nЧто можно видеть на визуализации\\n* `value` - сколько объектов каждого класса в узле. На первом этапе 4 объекта одного класса и 5 объектов другого - см. `.value_counts()` немного выше\\n* в каждом узле считается энтропия\\n* цветом закодировано количество объектов класса в сплите\\n\\n## Как производить разбирение\\n\\n### Энтропия\",\n",
       "   'Статистики ранговых критериев имеют табличное распределение, для них характерна нормальная аппроксимация статистики с ростом объёма выборки.\\n\\n\\n**Вариационный ряд** (set of order statistic) — последовательность значений заданной выборки $X^n=(X_1,\\\\ldots,X_n)$ , расположенных в порядке неубывания: $X(1)\\\\le \\\\ldots \\\\le X(2)$. \\nСвязка - это подмоножество объектов, у которых одинаковый средний ранг. Если элемент не в связке - его ранг совпадает с номером в вариационном ряду.\\n\\nПример рангового критерия: [критерий знаковых рангов Вилкоксона](http://www.machinelearning.ru/wiki/index.php?title=Критерий_знаковых_рангов_Уилкоксона), который проверяет гипотезу о равенстве медиан двух выборок $H_0: med(X^{n_1})=med(X^{n_2})$\\n\\nДругие примеры\\n\\n* Критерий [Манна-Уитни](https://ru.wikipedia.org/wiki/U-критерий_Манна_—_Уитни) о равенстве распределений\\n\\n# Самостоятельное изучение',\n",
       "   'Для начала вычислим априорные вероятности классов $P(c)$\\n\\n```python\\nfrom collections import Counter\\n\\nnum_samples = df_source.shape[0]  # число объектов\\nprior_class_counts = Counter(y)  # группируем по классу и считаем колияество объектов каждого класса\\nprior_class_probs = dict() # тут будем хранить \\xa0априорные вероятности классов\\nprint(prior_class_counts)\\nfor class_label in prior_class_counts:\\n  prior_class_probs[class_label] = prior_class_counts[class_label] / num_samples\\nprint(prior_class_probs)\\n```\\n\\n```text\\nCounter({np.uint16(1): 66, np.uint16(0): 34})\\n{np.uint16(0): 0.34, np.uint16(1): 0.66}\\n```\\n\\nКак посчитать $P(X\\\\mid c)$? Каждый объект $x \\\\in X$ состоит из фичей, это вектор $x = [x_1, \\\\ldots, n_n ]$, в нашем случае это из три элементов (у нас три фичи). Классификатор называется *наивным*, потому что мы делаем предположение о независимости фичей - в этом случае совместное распределение можно расписать через произведение маргинальных распределений',\n",
       "   'Статья на вики про [Критерий Стьюдента](https://ru.wikipedia.org/wiki/T-Критерий_Стьюдента ) или на [machinelearning.ru](http://www.machinelearning.ru/wiki/index.php?title=Критерий_Стьюдента )\\n\\n\\nВ качестве самостоятельной работы нужно написать функцию проверки критерия Стьюдента для двух выборок в случае неизвестной дисперсии и сравните с любой готовой реализацией.\\n\\n[Реализация двухвыборочного теста](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html) в scipy\\n\\n```python\\ndef student_statistics(X_1, X_2):\\n    stat_value = None\\n    \\n    return stat_value\\n\\nsample_1 = []\\nsample_2 = []\\n\\nstat_value = student_statistics(sample_1, sample_2)\\nquantile_level = 0.05\\n\\nquantile_value = None\\n\\ntest_result = stat_value < quantile_value\\n\\nprint(\"Результат проверки гипотезы : H0 = {}\".format(test_result))\\n```\\n\\n```text\\n0\\n```\\n\\n### Z-критерий для 2-ух долей',\n",
       "   'Зачем нужно разрабатывать ML проекты, когда в каждой из областей есть экспертные знания?\\n\\n* сложно придумать правила  в понятном человеку виде (например, распознавание изображений)\\n* эвристики устаревают и нужно придумывать новые (борьба со спамом - непрерывный процесс)\\n\\n## Неформальное определение\\n\\nДопустим, вы хотите решить какую-то задачу (например, выявление спам-смс) с помощью ML. Независимо от доменной области ML проект проходит следующие этапы:\\n\\n1. Выгрузка *обучающей выборки* (например, набор смс)\\n1. Разметка обучающей выборки (отметить те, которые являются спамом). Хотя некоторые алгоритмы могут работать и без разметки.\\n1. Выбираем метрику качества решения задачи (долю точно определённых спам-смс)\\n1. Каждый объект выборки описываем некоторыми признаками, которые называются фичами (признаками) (фичами смс могут стать входящие в него слова)\\n1. Настраиваем выбранную модель машинного обучения наилучшим образом (тут поможет метрика) решать поставленную задачу',\n",
       "   'В пакетах для проведения статистических тестов на Python вычисляется не критическая область значений статистики, а именно *$p$-value*, которое соответствует полученной реализации статистики.\\n\\nНа картинке пример статистики, которая распределена нормальным образом - в центре область, где значения статистики подтверждают нулевую гипотезу, а справа и слева т.н. *критические области*, где значения статистики *отвергают* гипотезу $H_0$\\n\\n![stat_test](img/stat_test.jpg)\\n\\nЕсли p-value мало, значит данные эксперимента свидетельствуют против нулевой гипотезы. Величину $p-value$ сравнивают с порогом, который определяют перед началом эксперимента: $p \\\\le \\\\alpha \\\\longrightarrow H_0$ отвергается в пользу альтернативы $H_1$ . $\\\\alpha$ называют уровнем значимости (обычно принимают  $\\\\alpha = 0.05$).\\n\\nПри оценивании статистических критеритериев различаю ошибки I и II рода',\n",
       "   '```text\\n<Figure size 640x480 with 1 Axes>\\n```\\n\\n```python\\nc_1\\n```\\n\\n```text\\narray([-0.97742455, -0.03263543])\\n```\\n\\n**Задача**\\n\\nПусть центр\\n\\n* Оставьте в датасете `X` только точки, которые соответствуют кластеру `Y==0` (c помощью `np.where`)\\n* посчитать сумму расстояний от каждой точки полученного датасета до центроида лейбла `c1 = [-0.97742455, -0.03263543]`\\n* округлите результат\\n\\nВоспользуемся `from sklearn.metrics.pairwise.euclidean_distances`\\n\\nОкруглите результат `res` с помощью\\n```\\nnp.round(res, 2)\\n```\\n\\nМы посчитаем т.н. сумму внутрикластерных расстояний - эта величина часто используется в метриках кластеризации\\n\\n```python\\nfrom sklearn.metrics.pairwise import euclidean_distances\\n\\n#------ ВАШ КОД ТУТ ----------\\n\\n\\n\\n\\n\\n#-------------------------------\\n```\\n\\n### Вычислительная сложность\\n\\nПрименяют вычисление центроидов по подвыборке, такая модификация получила название **Mini batch K-mean**\\n\\n### Инициализация центроидов',\n",
       "   '```text\\n<Figure size 640x480 with 1 Axes>\\n```\\n\\n```python\\nc_1\\n```\\n\\n```text\\narray([-0.97742455, -0.03263543])\\n```\\n\\n**Задача**\\n\\nПусть центр\\n\\n* Оставьте в датасете `X` только точки, которые соответствуют кластеру `Y==0` (c помощью `np.where`)\\n* посчитать сумму расстояний от каждой точки полученного датасета до центроида лейбла `c1 = [-0.97742455, -0.03263543]`\\n* округлите результат\\n\\nВоспользуемся `from sklearn.metrics.pairwise.euclidean_distances`\\n\\nОкруглите результат `res` с помощью\\n```\\nnp.round(res, 2)\\n```\\n\\nМы посчитаем т.н. сумму внутрикластерных расстояний - эта величина часто используется в метриках кластеризации\\n\\n```python\\nfrom sklearn.metrics.pairwise import euclidean_distances\\n\\n#------ ВАШ КОД ТУТ ----------\\n\\n\\n\\n\\n\\n#-------------------------------\\n```\\n\\n### Вычислительная сложность\\n\\nПрименяют вычисление центроидов по подвыборке, такая модификация получила название **Mini batch K-mean**\\n\\n### Инициализация центроидов',\n",
       "   'centroid = np.array([-0.9774245525274352, -0.032635425821084516])\\n\\n# -- ВАШ КОД ТУТ --\\n\\n\\n\\n\\n\\n# ------------------\\n```\\n\\nДля подбора вводится метрика качества кластеризации с центроидами $\\\\mu_k: \\\\forall k=1,\\\\ldots,K$, которая измеряет внутрикластерное расстояние - насколько хорошо центр, который мы выбрали, совпадает с \"идеальным\" цетром кластера\\n$$\\nJ(C) = \\\\sum_{k=1}^{K}\\\\sum_{j \\\\in C_k} \\\\mid x_j - \\\\mu_k \\\\mid \\\\rightarrow min\\n$$\\n\\nВ этой формуле\\n* $\\\\mu_k$ - координаты центроида кластера под номером $k$, количество кластеров $k$\\n* $x_j$ - объект под номером $j$, принадлежащий кластеру под номером $k$\\n* $\\\\mid x_j - \\\\mu_k \\\\mid$ - евклидово расстояние от примера $x_j$ до центроида $\\\\mu_k$',\n",
       "   'Теперь проведем небольшьшой трюк и логарифмируем наши шансы на успех\\n\\n$$\\n\\\\log odds = \\\\log \\\\frac{p}{1-p}\\n$$\\n\\nГрафик логарифма мы строили ранее в этом семинаре: для значений от $0$ до $1$ получим отрицательные значения, выше - положительные\\n\\nМы знаем, что линейная регрессия $\\\\hat{y} = w_0x_0 + \\\\ldots w_nx_n$ \"умеет\" предсказывать величины от $-\\\\infty$ до $\\\\infty$. Получается, мы по данным $X$ хотим предсказывать шансы на успех (как в ставках на спорт), например для конкретного объекта x у которого $n$ фичей:\\n\\n$$\\nw_0x_0 + \\\\ldots w_nx_n = \\\\overline{w}x = \\\\log \\\\frac{p}{1-p}\\n$$\\n\\nПревращаем логарифм в экспоненту\\n$$\\n\\\\exp(\\\\overline{w}x) = \\\\frac{p}{1-p}\\n$$\\n\\nИ окончательно выражаем вероятность успеха $p$ для конкретного объекта $x_i$:\\n$$\\np_i = \\\\frac{1}{1+e^{-\\\\overline{w}x_i}}\\n$$\\n\\nФункция $\\\\sigma (z) = \\\\frac{1}{1+e^{-z}}$ называеся *сигмоидой* - в курсе по нейросетям с сигмоидой вы встретитесь ещё не раз. Она обладает рядом интересных свойств, например $\\\\sigma (-z) = 1 - \\\\sigma (z)$']],\n",
       " 'uris': None,\n",
       " 'data': None,\n",
       " 'included': ['documents', 'metadatas', 'distances']}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "include = [\"documents\", \"metadatas\", \"distances\"]\n",
    "collection = client.get_collection('docs1')\n",
    "limit = 10\n",
    "\n",
    "collection.query(\n",
    "    query_embeddings=[query_embedding],\n",
    "    n_results=limit,\n",
    "    include=include,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f74b299a-6b57-42ca-bacd-2ce66eee07cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25743\n",
      "12328\n",
      "50755\n",
      "22446\n",
      "13410\n",
      "12348\n",
      "23547\n",
      "9653\n",
      "45966\n",
      "22105\n",
      "19253\n",
      "15822\n",
      "26619\n",
      "9263\n",
      "13511\n",
      "2108\n",
      "32743\n",
      "31300\n",
      "25222\n",
      "40373\n",
      "6008\n",
      "42048\n",
      "35539\n",
      "12521\n",
      "8400\n",
      "50919\n",
      "2246\n",
      "13204\n",
      "15410\n",
      "1065\n",
      "14506\n",
      "31357\n",
      "20802\n",
      "23344\n",
      "35824\n",
      "21435\n"
     ]
    }
   ],
   "source": [
    "docs_path = os.path.join(root_data_dir, 'md_docs')\n",
    "for current_file in os.listdir(docs_path):\n",
    "    file_path = os.path.join(docs_path, current_file)\n",
    "    with open(file_path, 'r') as f:\n",
    "        file_content = f.read()\n",
    "    print(len(file_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fbfc33-fb3a-4530-b4e9-22bc217b6d58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
