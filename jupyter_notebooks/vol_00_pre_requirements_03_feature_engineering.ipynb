{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ngEBDlPa5Q2Y"
   },
   "source": [
    "# Feature egineering\n",
    "\n",
    "Мы узнали про достаточное количество алгоритмов машинного обучения для решения  задач классификации и решрессии, и про то, как оценивать качество алгоритмов\n",
    "\n",
    "Следующий шаг - научиться настраивать алгоритмы для получения максимального качества\n",
    "\n",
    "В задачи unsupervised и supervised объединяет общий элемент - матрица *объекты* $\\times$ *признаки* размерность $m \\times n$, где $m$ - число объектов, а $n$ - число признаков\n",
    "\n",
    "$$\n",
    "X = \\left[\n",
    "\\begin{array}{cccc}\n",
    "x_{11} & x_{12} & \\ldots & x_{14} \\\\\n",
    "x_{21} & x_{22} & \\ldots & x_{14} \\\\\n",
    "\\ldots & \\ldots & \\ldots & \\ldots \\\\\n",
    "x_{m1} & x_{m2} & \\ldots & x_{mn} \\\\\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "Таким образом, каждый объект описан признаками(фичами) в количестве $n$ штук: $x^i = (x_1, x_2, \\ldots, , x_n)$. Мы уже знаем, что фичи бывают численными и категориальными.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NFkdRiFT5RLv"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import shutil \n",
    "\n",
    "import numpy as np\n",
    "from google.colab import drive\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)  # гарантируем воспроизводимость\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.info('Инициализировали логгер')\n",
    "\n",
    "ROOT_DIR = '/content/drive' \n",
    "drive.mount(ROOT_DIR)\n",
    "logger.info('Подключили диск')\n",
    "\n",
    "root_data_dir = os.path.join(ROOT_DIR, 'MyDrive', 'hse_nlp_2021')\n",
    "if not os.path.exists(root_data_dir):\n",
    "  raise RuntimeError('Отсутствует директория с данными')\n",
    "else:\n",
    "  logger.info('Содержимое директории %s: %s', root_data_dir, os.listdir(root_data_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Фичи можно назвать \"топливом\" алгоритмов машинного обучения. Хорошие фичи позволят повысить качество решения задачи, примерно как на картинке\n",
    "\n",
    "![ml_blackbox](img/ml_blackbox.png)\n",
    "\n",
    "\n",
    "В алгоритмах машинного обучения и анализа данных часто встречаются требования к фичам входных данных\n",
    "* распределение данных\n",
    "* масштаб\n",
    "\n",
    "Перед аналитиком часто стоит задача трансформации (преобразования) входных данных таким образом, чтобы удовлетворить условиям алгоритма. Игнорирование требований к входным данным приводит некорректным выводам, это основной принцип ML (и не только ML): **garbage in - garbage out**. Процесс \"придумывания\" фичей называется feature engineering\n",
    "\n",
    "В этом занятии поговорим о том, как трансформировать исходный csv файл в набор фичей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Трансформации непрерывных фичей\n",
    "\n",
    "В задачах линейной регрессии такие трансформации особенно важны - чтобы линейная регрессия хорошо работала должны выполняться следующие требования:\n",
    "\n",
    "* остатки регрессии должны иметь нормальное (гауссово) распределение\n",
    "* все фичи должны быть примерно в одном масшабе\n",
    "\n",
    "При подготовке данных для обучения линейной регрессии применяются следющие приемы: масштабирование и нормализация.\n",
    "\n",
    "\n",
    "Существуют чисто инженерные приёмы первичной обработки данных, например для борьбы с большими по модулю значениями обычно используют т.н. *Монотонные преобразования*:\n",
    "\n",
    "* логарифмирование np.log\n",
    "* извлечение квадратного корня np.sqrt\n",
    "\n",
    "Оба этих преобразования являются *монотонными*, т.е. они меняют абсолютные значения, но сохраняют порядок величин."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### z-score\n",
    "\n",
    "Более интересный метод - это Standart Scaling или Z-score normalization. Это преобразование позволяет \"сгладить\" данные, избавить их от выбросов. Для этого есть инструмента [есть реализация в sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "data = pd.read_csv('data/task.csv')\n",
    "print(data.head(3))\n",
    "\n",
    "raw_data = data[data.columns.values[-1]].values.astype(np.float32)\n",
    "print(\"Сырой датасет: %s\" % raw_data)\n",
    "print(\"stat = %s, p-value=%s\\n\" % shapiro(raw_data) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проводим трансформацию фичи (масштабирование)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_data = StandardScaler().fit_transform(raw_data.reshape(-1, 1)).reshape(-1)\n",
    "print(\"z-transform датасет: %s\" % transformed_data)\n",
    "print(\"stat = %s, p-value=%s\\n\" % shapiro(transformed_data) )\n",
    "\n",
    "print(\"Проверка на нормальность p_1 > p_2: %s\" % (shapiro(transformed_data)[1] > shapiro(raw_data)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тест Шапиро-Уилка показывает, что гипотеза о нормальном распределении данных стала чуть более вероятной, чем до \"Z-score\" нормализации.\n",
    "\n",
    "#### min-max normalization\n",
    "\n",
    "Другой распространённый метод называется MinMax Scaling. Этот метод переносит все точки на отрезок [0-1]\n",
    "$$\n",
    "X_{scaled} = \\frac{X - X_{\\min}}{X_{\\max} - X_{\\min}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "print(\"Сырой датасет: %s\" % raw_data)\n",
    "\n",
    "transformed_data = MinMaxScaler().fit_transform(raw_data.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "print(\"Min-Max scale датасет: %s\" % transformed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Трансформации категориальных фичей\n",
    "\n",
    "Категориальная переменная - это набор меток (классов). В приложенном датасете по задам столбец `Компонент` - категориальная фича, а `Затрачено в часах` - непрерывная\n",
    "\n",
    "Посчитаем количество различных меток в поле \"Компонент\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/task.csv')\n",
    "print(df['Компонент'].value_counts().to_dict())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Кодирование One-Hot\n",
    "\n",
    "Кодируем вектор, где все нули и одна единица"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "one_hot_encoded = ohe.fit_transform(df[['Компонент']])\n",
    "\n",
    "one_hot_encoded.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hashing trick\n",
    "\n",
    "В случае, когда признаков слишком много, применяют хеширование\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in df['Компонент'].unique():\n",
    "    #print(label, '->', hash(label) % 8 )\n",
    "    print(label, '->', hash(label) % 12 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Премер хеширования (с формулами!) в [Лекциях от ВШЭ](https://github.com/esokolov/ml-course-hse/blob/master/2016-fall/lecture-notes/lecture06-linclass.pdf)\n",
    "\n",
    "Прекрасный разбор есть на хабре в статье от [ODS про feature engineering](https://habr.com/ru/company/ods/blog/326418/#rabota-s-kategorialnymi-priznakami-label-encoding-one-hot-encoding-hashing-trick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XR46GF8Q64_M"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "input_dataset_df = pd.read_csv(os.path.join(root_data_dir, 'final_dataset.zip'), compression='zip')\n",
    "logger.info('num rows %d', input_dataset_df.shape[0])\n",
    "\n",
    "input_dataset_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E3__EeFM6TI4"
   },
   "source": [
    "# Визуализации данных\n",
    "\n",
    "Чаще всего используется визуализация `scatter plot`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oSM6UoZ26XWV"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.scatter(input_dataset_df['num_comments'], input_dataset_df['num_shares'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eh4p0e-F6_45"
   },
   "source": [
    "Строим гистограмму"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L9yBIb-b6-sQ"
   },
   "outputs": [],
   "source": [
    "plt.hist(input_dataset_df['num_comments'], bins=10, density=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "woehAAJP7isN"
   },
   "source": [
    "Выглядит странно, как будто есть небольшое количество данных, больших по модулю и большое количество данных около-нулевых - такие большие элементы называют выбросами (outliers)\n",
    "\n",
    "Выявляем выбросы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JSz2zY887m21"
   },
   "outputs": [],
   "source": [
    "input_dataset_df['num_comments'].describe(percentiles=[.5, .95, .99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XlUvafGqFyik"
   },
   "source": [
    "Как ещё проверить на выбросы? С помощью `.boxplot()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jaNnyvPIFzI2"
   },
   "outputs": [],
   "source": [
    "input_dataset_df[['num_comments']].boxplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_kSC8ZF96-8K"
   },
   "source": [
    "Переделываем гистограмму (без выбросов)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kzs4BrUF6XZO"
   },
   "outputs": [],
   "source": [
    "plt.hist(\n",
    "    input_dataset_df[input_dataset_df['num_comments'] < 12]['num_comments'],\n",
    "    bins=10, density=True\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4k0dUSE08dMm"
   },
   "source": [
    "# Трансформации данных\n",
    "\n",
    "Получили т.н. распределение с \"тяжёлым хвостом\"\n",
    "\n",
    "Данные можно центрировать и снизить дисперсию - такое преобразование называется z-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "weuBfPJo6Xb5"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "transformed_z_score = (\n",
    "    StandardScaler()\n",
    "    .fit_transform(\n",
    "        input_dataset_df[input_dataset_df['num_comments'] < 12]['num_comments']\n",
    "        .values.reshape(-1, 1)\n",
    "    )\n",
    ")\n",
    "\n",
    "plt.hist(transformed_z_score, bins=10, density=True)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s6E68-bf-DNN"
   },
   "source": [
    "Можно ли применять логарифмирование всегда?\n",
    "\n",
    "Давайте попробуем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "unI3zzJv6Xek"
   },
   "outputs": [],
   "source": [
    "plt.hist(np.log(transformed_z_score), bins=5, density=True)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O5owlyVQYnXm"
   },
   "source": [
    "Получили какой-то пропуск в данных, почему так вышло?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PKTUVCec6Xhi"
   },
   "outputs": [],
   "source": [
    "logger.info('range=[%.4f, %.4f]', transformed_z_score.min(), transformed_z_score.max())\n",
    "\n",
    "carrier = np.linspace(transformed_z_score.min(), transformed_z_score.max())\n",
    "\n",
    "plt.plot(\n",
    "    carrier, np.log(carrier)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2NUqGpjPEWy7"
   },
   "source": [
    "Получается, в numpy логарифм для отрицательных чисел не определён - тут нужно применить другое масштабирование, например min-max.\n",
    "\n",
    "Получается, что логарифмирование (и извлечение квадратного корня нельзя применять после вычисления z-score)\n",
    "\n",
    "Нужно выбрать один из вариантов \n",
    "* сначала логарифмировать, а потом применить z-score\n",
    "* только z-score\n",
    "*только логарифмирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k4U55F7cEyef"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "transformed_z_score = (\n",
    "    MinMaxScaler()\n",
    "    .fit_transform(\n",
    "        input_dataset_df[input_dataset_df['num_comments'] < 12]['num_comments']\n",
    "        .values.reshape(-1, 1)\n",
    "    )\n",
    ")\n",
    "\n",
    "plt.hist(transformed_z_score, bins=10, density=True)\n",
    "plt.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B1d-xlayExRn"
   },
   "source": [
    "# Пропущенные значения\n",
    "\n",
    "Проверка на пропущенные значения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v80m2JgWFjh9"
   },
   "outputs": [],
   "source": [
    "null_values_share = (\n",
    "    input_dataset_df['declined_reason']\n",
    "    .isna()\n",
    "    .value_counts(normalize=True)\n",
    "    .to_dict()\n",
    "    .get(True, 0.0)\n",
    ")\n",
    "logger.info('Доля пропущенных значений %.4f', null_values_share)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dOBdNPk3HThE"
   },
   "source": [
    "Заменяем `NULL` на пустую строку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QD259jiuHaXC"
   },
   "outputs": [],
   "source": [
    "input_dataset_df['declined_reason'].fillna(value='Value missed', inplace=True)\n",
    "\n",
    "null_values_share = (\n",
    "    input_dataset_df['declined_reason']\n",
    "    .isna()\n",
    "    .value_counts(normalize=True)\n",
    "    .to_dict()\n",
    "    .get(True, 0.0)\n",
    ")\n",
    "logger.info('Доля пропущенных значений %.4f', null_values_share)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "boyihK3jhSts"
   },
   "outputs": [],
   "source": [
    "input_dataset_df['declined_reason'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9N2yXORAHvs8"
   },
   "source": [
    "# Кодирование категориальных фичей\n",
    "\n",
    "Кодируем категориальные фичи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8PwiSyz3H4Pu"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "le_transformed = le.fit_transform(input_dataset_df['declined_reason'])\n",
    "\n",
    "pd.Series(le_transformed).value_counts().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "33AO7wjdInGJ"
   },
   "source": [
    "Другой (более подходящий) вид энкодинга"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "grtMs14GI3_O"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder()\n",
    "\n",
    "ohe_transformed = ohe.fit_transform(input_dataset_df[['declined_reason']])\n",
    "\n",
    "ohe_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lowFEQvzJ-ER"
   },
   "source": [
    "# Текстовые фичи\n",
    "\n",
    "## Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XR1gNndRKF-S"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    lowercase=True,\n",
    "    token_pattern=r'\\b[\\w\\d]{3,}\\b',\n",
    "    min_df=0.001\n",
    ")\n",
    "\n",
    "bow_matrix = vectorizer.fit_transform(ocr_dataset_df.text.values)\n",
    "bow_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WjvMeuFKMOvs"
   },
   "source": [
    "По sparse матрице можно найти попарное расстояние между текстами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "38a99UAMMfdp"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "text_distance = 1 - pairwise_distances(bow_matrix, metric=\"cosine\")\n",
    "\n",
    "text_distance.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ldr3ZZm0Mc_x"
   },
   "source": [
    "Мы получили квадратную матрицy, которая содержит столько строк и столбцов, сколько документов в нашем корпусе (наборе текстов).\n",
    "\n",
    "Зачем нужна такая матрица? Например, можно искать \"похожие\" тексты для задачи рекомендаций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0lZqIVAS5o75"
   },
   "outputs": [],
   "source": [
    "source_tweet_index = 14  # тут может быть любое число в диапазоне от 1 до 1000\n",
    "\n",
    "print(ocr_dataset_df.iloc[source_tweet_index].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bG_7aER36SBB"
   },
   "source": [
    "Находим ближайший похожий текст\n",
    "\n",
    "отсортируем твиты по “похожести” - чем похожее на `source_tweet_index`, тем ближе к началу списка sorted_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5yDFHfG46XeR"
   },
   "outputs": [],
   "source": [
    "sorted_similarity = np.argsort(-1 * text_distance[source_tweet_index,:])\n",
    "\n",
    "sorted_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mw9GHt6s6-SQ"
   },
   "source": [
    "Теперь распечаем \"схожие\" тексты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Pd1j_I27D3H"
   },
   "outputs": [],
   "source": [
    "for content_index in sorted_similarity[:4]:\n",
    "  print(ocr_dataset_df.iloc[content_index].text)\n",
    "  print('-------------\\n-------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gMpcMke_8ErE"
   },
   "source": [
    "## Препроцессинг текста\n",
    "\n",
    "Устанавливаем NLTK. Для начала готовим директорию для данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EGOL2Tzv-IcW"
   },
   "outputs": [],
   "source": [
    "nltk_data_dir = os.path.join(ROOT_DIR, 'MyDrive', 'hse_nlp_2021', 'nltk_data')\n",
    "if not os.path.exists(nltk_data_dir):\n",
    "  os.makedirs(nltk_data_dir)\n",
    "  logger.info('Директория %s создана', nltk_data_dir)\n",
    "logs_dir = os.path.join(ROOT_DIR, 'MyDrive', 'hse_nlp_2021', 'logs')\n",
    "if not os.path.exists(logs_dir):\n",
    "  os.makedirs(logs_dir)\n",
    "logger.info('Подготовили директорию для nltk %s', nltk_data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ovlUmPem-Yld"
   },
   "source": [
    "Обязательно фиксируем версию! Иначе будут косяки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BbqOSM7H-enl"
   },
   "outputs": [],
   "source": [
    "!pip install nltk==3.6.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ELNptAEW-tGQ"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt', download_dir=nltk_data_dir)\n",
    "nltk.data.path.append(nltk_data_dir) # тут почему-то корневую надо указывать ¯\\_(ツ)_/¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hG4GNJT38Unx"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "sample_str = ocr_dataset_df.text.values[0]\n",
    "\n",
    "print('== Исходный текст== \\n%s\\n\\n' % sample_str)\n",
    "\n",
    "tokenized_str = nltk.word_tokenize(sample_str)\n",
    "print('== Токенизированный текст==\\n%s' % tokenized_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6Egvl76_2C6"
   },
   "source": [
    "Видим знаки препинания, надо их отфильтровать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NgYYdKmP_8ty"
   },
   "outputs": [],
   "source": [
    "import string  # стандартный модуль\n",
    "\n",
    "tokens = [i.lower() for i in tokenized_str if ( i not in string.punctuation )]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6JNqTumjAKAg"
   },
   "source": [
    "Стоп-слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iREyrgcZALhF"
   },
   "outputs": [],
   "source": [
    "stop_words = [\n",
    "    'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\",\n",
    "    'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers',\n",
    "    'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which',\n",
    "    'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been',\n",
    "    'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if',\n",
    "    'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between',\n",
    "    'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out',\n",
    "    'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why',\n",
    "    'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not',\n",
    "    'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'shold',\n",
    "    \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\",\n",
    "    'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\",\n",
    "    'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\",\n",
    "    'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"\n",
    "]\n",
    "\n",
    "filtered_tokens = [i for i in tokens if ( i not in stop_words )]\n",
    "\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3SkOknHoAVaF"
   },
   "source": [
    "Соединяем в одну функцию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mN8_tDcSAZ_p"
   },
   "outputs": [],
   "source": [
    "def tokenize_text(raw_text: str):\n",
    "    \"\"\"Функция для токенизации текста\n",
    "    \n",
    "    :param raw_text: исходная текстовая строка\n",
    "    \"\"\"\n",
    "    filtered_tokens = []\n",
    "    \n",
    "    filtered_tokens = [i.lower() for i in raw_text.split() if ( i not in string.punctuation )]\n",
    "    filtered_tokens = [i for i in filtered_tokens if ( i not in stop_words )]\n",
    "    filtered_tokens = [i for i in filtered_tokens if ( len(i) > 2 )]\n",
    "    \n",
    "    return filtered_tokens\n",
    "\n",
    "# применяем функцию в датафрейму с помощью метода .apply()\n",
    "tokenized_tweets= ocr_dataset_df.text.apply(tokenize_text)\n",
    "\n",
    "# добавляем новую колонку в исходный датафрейм\n",
    "ocr_dataset_df = ocr_dataset_df.assign(\n",
    "    tokenized=tokenized_tweets\n",
    ")\n",
    "\n",
    "ocr_dataset_df.tokenized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tnJK9SOU7y24"
   },
   "source": [
    "## Эмбеддинги Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9jPa2NDu73Q7"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import logging\n",
    "\n",
    "texts = ocr_dataset_df.tokenized.values\n",
    "\n",
    "model = Word2Vec(texts, size=10, window=7, min_count=2, workers=4, iter=10, sg=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j6Tlis5AJ7Kp"
   },
   "source": [
    "Эмбеддинги обучились, посмотрим как они выглядят"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PbgavG2ZKFi7"
   },
   "outputs": [],
   "source": [
    "model.wv.get_vector('android')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NHXMsjNGBxHT"
   },
   "source": [
    "Проверяем эмбеддинги - ищем эмбеддинг, самый похожий на эмбеддинг слова Biden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1YgKqeLkB934"
   },
   "outputs": [],
   "source": [
    "# biden\n",
    "model.wv.most_similar('biden')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N1JAQPnEtrUr"
   },
   "source": [
    "Включаем GPU\n",
    "\n",
    "Подробнее о модели [тут](https://pytorch.org/hub/pytorch_vision_resnet/)\n",
    "\n",
    "![gpu_on_colab](img/gpu_on_colab_gui.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I90TPsOdOzoW"
   },
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18\n",
    "\n",
    "TORCH_MODELS_DIR = os.path.join(ROOT_DIR, 'MyDrive', 'hse_nlp_2021', 'torch_models')\n",
    "try:\n",
    "  os.mkdir(TORCH_MODELS_DIR)\n",
    "except FileExistsError as e:\n",
    "  logger.info(e)\n",
    "\n",
    "os.environ['TORCH_HOME'] = TORCH_MODELS_DIR # TORCH_MODEL_ZOO is deprecated\n",
    "rn18 = resnet18(pretrained=True)\n",
    "\n",
    "# запускаем вычисления на GPU\n",
    "# rn18 = rn18.to('cuda:0')\n",
    "logger.info('Модель загружена')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ajlIo1veOAh"
   },
   "source": [
    "# Эмбеддинги картинок\n",
    "\n",
    "Для начала посмотрим, какие слои есть в сети\n",
    "\n",
    "Кроме `.modules` можно было воспользоваться `.named_children()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JsS8ABYWjZPe"
   },
   "outputs": [],
   "source": [
    "for layer in rn18._modules:\n",
    "  print(layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OR0Y-xXhjcvj"
   },
   "source": [
    "Нам нужен слой `avgpool`\n",
    "\n",
    "Каждый слой это по сути массив c весами модели - нам нужно оставить все слои ДО того слоя, который нас интересует"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O0vT7v73V9cd"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, output_layer, torch_model):\n",
    "        super().__init__()\n",
    "        self.output_layer = output_layer\n",
    "        self.pretrained = torch_model\n",
    "        self.children_list = []\n",
    "        for n,c in self.pretrained.named_children():\n",
    "            self.children_list.append(c)\n",
    "            if n == self.output_layer:\n",
    "                logger.info('final layer archived: %s', output_layer)\n",
    "                break\n",
    "\n",
    "        self.net = nn.Sequential(*self.children_list)\n",
    "        self.pretrained = None\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vnR8wWKSpRhV"
   },
   "source": [
    "Создаём объект-экстрактор с выборанным слоем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NAiIQvreOvL-"
   },
   "outputs": [],
   "source": [
    "resnet_extractor = FeatureExtractor(output_layer='avgpool', torch_model=rn18)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IO1wL1d4Fs2E"
   },
   "source": [
    "Проверяем директорию с картинками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sJXkiao5FtGl"
   },
   "outputs": [],
   "source": [
    "ROOT_MEMES_DIR = os.path.join(ROOT_DIR, 'MyDrive', 'hse_nlp_2021', 'memes')\n",
    "\n",
    "os.listdir(ROOT_MEMES_DIR)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cTVdn7QPNG18"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "filename = os.path.join(ROOT_MEMES_DIR, '7f3ywc.jpeg')\n",
    "\n",
    "input_image = Image.open(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UDjz-L5gyur_"
   },
   "outputs": [],
   "source": [
    "type(input_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jwJLliIoV9fm"
   },
   "outputs": [],
   "source": [
    "from torch import no_grad, cuda\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# transform = transforms.ToTensor()\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "image_tensor = preprocess(input_image)\n",
    "input_batch = image_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
    "\n",
    "# move the input and model to GPU for speed if available\n",
    "if cuda.is_available():\n",
    "    input_batch = input_batch.to('cuda')\n",
    "    resnet_extractor.to('cuda')\n",
    "\n",
    "with no_grad():\n",
    "    output = resnet_extractor(input_batch)\n",
    "    numpy_vector = output.reshape(-1).cpu().numpy()  # flatten(output)\n",
    "    print(type(output), output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fXbdwdq4kc1M"
   },
   "outputs": [],
   "source": [
    "numpy_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q7srLON2V9jL"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from PIL.JpegImagePlugin import JpegImageFile\n",
    "from torch import no_grad, cuda\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "\n",
    "def img2embedding(input_meme_filename: str) -> np.array:\n",
    "  OUTPUT_SHAPE = 512\n",
    "  numpy_vector = np.zeros(OUTPUT_SHAPE)\n",
    "  preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "  ])\n",
    "\n",
    "  try:\n",
    "    input_img = Image.open(input_meme_filename) # type: JpegImageFile\n",
    "  except UnidentifiedImageError:\n",
    "    return numpy_vector\n",
    "  try:\n",
    "    image_tensor = preprocess(input_img)\n",
    "  except RuntimeError:\n",
    "    #  logger.info('error with %s meme', meme_filename.split('/')[-1])\n",
    "    return numpy_vector\n",
    "  input_batch = image_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
    "\n",
    "  # move the input and model to GPU for speed if available\n",
    "  if cuda.is_available():\n",
    "      input_batch = input_batch.to('cuda')\n",
    "      resnet_extractor.to('cuda')\n",
    "\n",
    "  with no_grad():\n",
    "      output = resnet_extractor(input_batch)\n",
    "      numpy_vector = output.reshape(-1).cpu().numpy()  # flatten(output)\n",
    "  return numpy_vector\n",
    "\n",
    "\n",
    "if os.path.exists(os.path.join(ROOT_MEMES_DIR, 'embed.npy')):\n",
    "  embeds_matrix = np.load(os.path.join(ROOT_MEMES_DIR, 'embed.npy'))\n",
    "  with open(os.path.join(ROOT_MEMES_DIR, 'file_index.pkl'), 'rb') as f:\n",
    "    file_index = pickle.load(f)\n",
    "    logger.info('files loaded from dump')\n",
    "else:\n",
    "  res = []  # тут основная информация о контенте\n",
    "  file_index = {}\n",
    "  TOP = 3092\n",
    "  error_files = []\n",
    "  logger.info('Processing started')\n",
    "  dense_index = 0\n",
    "  for f_name in os.listdir(ROOT_MEMES_DIR)[:TOP]:\n",
    "    meme_filename = os.path.join(ROOT_MEMES_DIR, f_name)\n",
    "    img_embed = img2embedding(meme_filename)\n",
    "    if img_embed.sum() == 0:\n",
    "      error_files.append(meme_filename)\n",
    "    # сохраняяем эмбеддинг (их потом схлопнем в матрицу) и отдельно индекс файла в матрице\n",
    "    res.append(img_embed)\n",
    "    file_index[dense_index] = {'f_name': f_name}\n",
    "    dense_index += 1\n",
    "    \n",
    "  if len(error_files) > 0:\n",
    "    logger.info('num errors %d', len(error_files))\n",
    "    for i in error_files:\n",
    "      os.remove(i)\n",
    "\n",
    "  embeds_matrix = np.vstack(res)\n",
    "\n",
    "logger.info(embeds_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MOg4T8CSy6mE"
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pickle\n",
    "\n",
    "# np.save(os.path.join(ROOT_MEMES_DIR, 'embed.npy') , embeds_matrix)\n",
    "# with open(os.path.join(ROOT_MEMES_DIR, 'file_index.pkl'), 'wb') as f:\n",
    "#   pickle.dump(file_index, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m1oNoJ7r4NrQ"
   },
   "source": [
    "Устанавливаем umap \n",
    "\n",
    "пример из [официальной документации](https://umap-learn.readthedocs.io/en/latest/basic_usage.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vIB3Gio54bEH"
   },
   "outputs": [],
   "source": [
    "!pip install umap-learn==0.5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xkKLwLXjV9pN"
   },
   "outputs": [],
   "source": [
    "import umap\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "reducer = umap.UMAP()\n",
    "\n",
    "scaled_memes_data = StandardScaler().fit_transform(embeds_matrix)\n",
    "low_rank_matrix = reducer.fit_transform(scaled_memes_data)\n",
    "logger.info('low rank matrix shape %s', low_rank_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IohdafZC4BGE"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.scatter(\n",
    "    low_rank_matrix[:, 0],\n",
    "    low_rank_matrix[:, 1],\n",
    ")\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "plt.title('UMAP projection of the memes dataset', fontsize=24)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RH_jWch79kwL"
   },
   "source": [
    "Получился один \"кластер\"\n",
    "\n",
    "Выполняем кластеризацию в низкоразмерном пространстве с помощью DBScan чтобы выделить метки кластеров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jnd8NpVU82a5"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "clstr = DBSCAN(eps=0.10, min_samples=4)\n",
    "classes = clstr.fit_predict(low_rank_matrix)\n",
    "logger.info('num classes %s', np.unique(classes).size)\n",
    "\n",
    "plt.scatter(\n",
    "    low_rank_matrix[:, 0],\n",
    "    low_rank_matrix[:, 1],\n",
    "    c=classes,\n",
    "    cmap='rainbow',\n",
    "    alpha=0.7,\n",
    ")\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "plt.title('UMAP projection of the memes dataset', fontsize=24)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hDMlsmsH95Wp"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "clusters_filename = os.path.join(ROOT_MEMES_DIR, 'dbscan_clusters.csv')\n",
    "if os.path.exists(clusters_filename):\n",
    "  memes_df = pd.read_csv(clusters_filename)\n",
    "  logger.info('loaded_from %s', clusters_filename)\n",
    "else:\n",
    "  df_rows = []\n",
    "  for meme_index in range(low_rank_matrix.shape[0]):\n",
    "    df_rows.append((file_index[meme_index]['f_name'], classes[meme_index]))\n",
    "  memes_df = pd.DataFrame(df_rows, columns=['f_name', 'dbscan_cluster'])\n",
    "logger.info('%s', memes_df['dbscan_cluster'].value_counts().head(10).to_dict())\n",
    "memes_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KQCktlEc_kDg"
   },
   "outputs": [],
   "source": [
    "memes_df.to_csv(os.path.join(ROOT_MEMES_DIR, 'dbscan_clusters.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PgXRufB2wqcy"
   },
   "source": [
    "Видно, что есть кластер с индексом `0` где большая часть контента и меньшие по можности кластера. ДЛя сравнения визуализируем кластер c индексом `3` и кластер с индексом `4`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FF2rdTNY-00U"
   },
   "outputs": [],
   "source": [
    "# import shutil\n",
    "\n",
    "# for f_name in os.listdir(ROOT_MEMES_DIR):\n",
    "#   if f_name not in ('file_index.pkl', 'embed.npy'):\n",
    "#     os.rename(os.path.join(ROOT_MEMES_DIR, f_name), os.path.join(ROOT_MEMES_DIR, f_name.split('.')[0]+'.jpeg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lBYpBbhsnLyt"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image as NotebookImage\n",
    "from IPython.display import display\n",
    "\n",
    "def visualise_cluster(cluster_id: int, top=10):\n",
    "  for _, row in memes_df.query(f'dbscan_cluster == {cluster_id}').head(top).iterrows():\n",
    "    tmp_file_path = os.path.join(ROOT_MEMES_DIR, row['f_name']+'.jpeg')\n",
    "    pil_img = NotebookImage(filename=tmp_file_path, width=200)\n",
    "    display(pil_img)\n",
    "\n",
    "visualise_cluster(cluster_id=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vo-c3xLGxuUB"
   },
   "outputs": [],
   "source": [
    "visualise_cluster(cluster_id=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ihMWhvMCCaas"
   },
   "outputs": [],
   "source": [
    "visualise_cluster(cluster_id=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Y_DGvqiEvK_"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNMt3yMRVzhz1TbSAJt0gs9",
   "collapsed_sections": [],
   "name": "Part 9: feature engineering.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
