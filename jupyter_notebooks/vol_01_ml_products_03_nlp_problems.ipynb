{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gzAchHUV1UAC"
   },
   "source": [
    "# Text data\n",
    "\n",
    "как обработка естественного языка (англ. Natural Language Processing, NLP). NLP изучает проблемы компьютерного анализа естественных языков - т.е. языков, которые для общения используют люди (а не придуманных искусственно (например, азбука Морзе - язык, придуманный искусственно). Поговорим подробнее о том, зачем нужен NLP и где именно возникает задача обработки естественного языка.\n",
    "\n",
    "Тексты - один из самых доступных и объёмных источников данных: легко собирать и просто хранить\n",
    "\n",
    "Например, если у вас интернет-магазин, то для анализа доступны\n",
    "\n",
    "* текстовые описания товаров\n",
    "* пользовательские комментарии\n",
    "* диалоги с продавцом-консультантом в чатике\n",
    "\n",
    "Текстовую информацию просто хранить, поэтому проекты накапливают огромные наборы данных такого рода и очень хотят извлекать из этих объёмов полезную информацию.\n",
    "\n",
    "Как специалист по ML в начале карьеры вы, скорее всего, встретите ряд “классических” задач - например, определение тональности (настроения) текста или классификации сообщений spam/not spam - для таких задач используются подходы, основанные на подсчёте статистик по встречающимся в тексте словам.\n",
    "Однако, есть и другие, более сложные задачи.\n",
    "\n",
    "Для решения применяются различные архитектуры нейросетей (RNN, LSTM) - это мощные инструменты, которые позволяют решать сложные задачи, например:\n",
    "\n",
    "* извлечения именованных сущностей ([NER](https://habr.com/ru/post/414175/), Named-Entity Recognizing)\n",
    "* автоматизированного перевода (например, сервис *google translate* производит перевод с помощью глубоких сетей)\n",
    "* Speech Recognition - распознавание речи, трансляция из аудио в текстовый вид\n",
    "* Natural Language Generation - генерация текстов, например можно генерировать подписи к картинкам\n",
    "\n",
    "У обработки естественного языка есть ряд особенностей:\n",
    "\n",
    "* необходимо размечать большой объём данных для обучения с учителем. Допустим, хотим отделять спам-сообщения от не спама. Вам нужно найти людей, которые прочитают все смс, которые удалось собрать и отметят те из них, которые являются спамом - текстов обычно очень много и разметка данных может оказаться дорогим удовольствием\n",
    "* модель, обученную на одном языке невозможно использовать для другого языка\n",
    "* важен как синтаксис, так и семантика (смысл). Например, во фразе: «Вот списки студентов, которые сдали зачет по физике» определение «которые сдали зачет по физике» относится к студентам, а в предложении: «Вот списки студентов, которые лежали в шкафу у декана»  структура фразы (тот самый синтаксис) такая же, как и предыдущей - но определение уже относится не к студентам, а к листкам бумаги. От компьютера мы хотим добиться, чтобы смыл обеих фраз был “понят” одинаково хорошо.\n",
    "\n",
    "Кроме того, для текстов на естественном языке довольно сложно проводить предобработку данных, этот этап сильно зависит от задачи, которую вы  решаете. Так, например, для задачи анализа тональности текста знаки препинания, скорее всего, не важны. Однако, для задачи извлечения именованных сущностей (именованная сущность - это имя собственное - например название организации или географического объекта) удалять знаки препинания не рекомендуется - это может привести к потере важной информации. Например если из фразы `Мы пошли обедать в “Берёзку”` если удалить все знаки препинания (кавычки) и заглавную букву в названии заведения то станет сложнее понять, что речь идёт о кафе.\n",
    "\n",
    "Обработка текста складывается из двух этапов\n",
    "\n",
    "* предварительная обработка текста\n",
    "* векторизация текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Предварительная обработка текста\n",
    "\n",
    "Перед тем, как обучать модель, данные следует специальным образом подготовить. Подготовка данных включает в себя несколько обязательных этапов\n",
    "\n",
    "* удалить все нерелевантные символы (например, любые символы, не относящиеся к цифро-буквенным).\n",
    "* токенизировать текст, разделив его на индивидуальные слова (токены)\n",
    "* удалить нерелевантные слова — например, упоминания в Twitter или URL-ы.\n",
    "* перевести все символы в нижний регистр для того, чтобы слова «hello», «Hello» и «HELLO» были схлопнуты в один токен\n",
    "* исправление ошибок (\"молоко\" и \"молако\" - одно слово, но разные токены, не надо так)  \n",
    "* лемматизация - перевод слова в нормальную (словарную) форму (например, «машина» вместо «машиной»). Существительные должны быть приведены к единственному числу именительного падежа, глаголы - инфинитив и т.д.\n",
    "* стемминг - процедура, когда от слова переходим к его корню (\"помыть\" и \"мытый\" - корень \"мыт\"). То есть все \"помытые\" заменяем на \"мыт\".\n",
    "\n",
    "Все эти приёмы нужно применять с осторожностью и внимательно следить за тем, как тот или иной приём, применённый к исходному тексту, влияет на качество решения задачи (например, выявлению спама)\n",
    "\n",
    "Для демонстрации всех этих приёмов загрузим корпус (набор текстов) с твитами о продуктах. Для каждого твита размечена эмоциональная окраска - позитивная, нейтральная или негативная. Примечание: для  обработки текста воспользуемся библиотекой nltk, которая [доступна в anaconda](https://anaconda.org/anaconda/nltk)\n",
    "\n",
    "Загружаем датасет с результатами модерации контента"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rnsz6ngY1UKK",
    "outputId": "a8bffe84-269f-4c90-bef4-29f49ded6a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Содержимое директории /Users/adzhumurat/PycharmProjects/ai_product_engineer/data: ['client_segmentation.csv', 'messages.db', 'labeled_data_corpus.csv', 'chroma', 'content_description.csv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "run_env = os.getenv('RUN_ENV', 'COLLAB')\n",
    "if run_env == 'COLLAB':\n",
    "  from google.colab import drive\n",
    "  ROOT_DIR = '/content/drive'\n",
    "  drive.mount(ROOT_DIR)\n",
    "  print('Google drive connected')\n",
    "  DRIVE_DATA_DIR = 'ml_course_data'\n",
    "  root_data_dir = os.path.join(ROOT_DIR, 'MyDrive', DRIVE_DATA_DIR)\n",
    "else:\n",
    "  root_data_dir = os.getenv('DATA_DIR', '/srv/data')\n",
    "\n",
    "if not os.path.exists(root_data_dir):\n",
    "  raise RuntimeError('Отсутствует директория с данными')\n",
    "else:\n",
    "  print('Содержимое директории %s: %s' % (root_data_dir, os.listdir(root_data_dir)[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Препроцессинг текста\n",
    "\n",
    "Устанавливаем NLTK - библиотеку для обработки тестов. Для начала готовим директорию для справочников"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Подготовили директорию для nltk /Users/adzhumurat/PycharmProjects/ai_product_engineer/data/nltk_data\n"
     ]
    }
   ],
   "source": [
    "nltk_data_dir = os.path.join(root_data_dir, 'nltk_data')\n",
    "if not os.path.exists(nltk_data_dir):\n",
    "  os.makedirs(nltk_data_dir)\n",
    "  print(f'Директория {nltk_data_dir} создана')\n",
    "logs_dir = os.path.join(root_data_dir, 'logs')\n",
    "if not os.path.exists(logs_dir):\n",
    "  os.makedirs(logs_dir)\n",
    "print(f'Подготовили директорию для nltk {nltk_data_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если работаете в коллабе надо будет установить пакет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Установили NLTK\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "if run_env == 'COLLAB':\n",
    "    !pip install nltk==3.6.2\n",
    "    # !pip install git+https://github.com/openai/CLIP.git\n",
    "    clear_output()\n",
    "print('Установили NLTK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/adzhumurat/PycharmProj\n",
      "[nltk_data]     ects/ai_product_engineer/data/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt', download_dir=nltk_data_dir)\n",
    "nltk.data.path.append(nltk_data_dir) # тут почему-то корневую надо указывать ¯\\_(ツ)_/¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Исходный текст== \n",
      "SoftBank Vision Fund 2 is leading the round, a Series C, with iPod “father” and Nest co-founder Tony Fadell (by way of Future Shape), Blisce, French entrepreneur Xavier Niel, Mirabaud, Cassius and Evolution — all previous backers — also participating. (Previous investors in the company also include DeepMind co-founders Mustafa Suleyman and Demis Hassabis, notable given the company’s early focus on data science and recommendation algorithms.) Prior to this round Dice had raised around $45 million, according to PitchBook estimates.\n",
      "\n",
      "\n",
      "== Токенизированный текст==\n",
      "['SoftBank', 'Vision', 'Fund', '2', 'is', 'leading', 'the', 'round', ',', 'a', 'Series', 'C', ',', 'with', 'iPod', '“', 'father', '”', 'and', 'Nest', 'co-founder', 'Tony', 'Fadell', '(', 'by', 'way', 'of', 'Future', 'Shape', ')', ',', 'Blisce', ',', 'French', 'entrepreneur', 'Xavier', 'Niel', ',', 'Mirabaud', ',', 'Cassius', 'and', 'Evolution', '—', 'all', 'previous', 'backers', '—', 'also', 'participating', '.', '(', 'Previous', 'investors', 'in', 'the', 'company', 'also', 'include', 'DeepMind', 'co-founders', 'Mustafa', 'Suleyman', 'and', 'Demis', 'Hassabis', ',', 'notable', 'given', 'the', 'company', '’', 's', 'early', 'focus', 'on', 'data', 'science', 'and', 'recommendation', 'algorithms', '.', ')', 'Prior', 'to', 'this', 'round', 'Dice', 'had', 'raised', 'around', '$', '45', 'million', ',', 'according', 'to', 'PitchBook', 'estimates', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "sample_text = \"\"\"SoftBank Vision Fund 2 is leading the round, a Series C, with iPod “father” and Nest co-founder Tony Fadell (by way of Future Shape), Blisce, French entrepreneur Xavier Niel, Mirabaud, Cassius and Evolution — all previous backers — also participating. (Previous investors in the company also include DeepMind co-founders Mustafa Suleyman and Demis Hassabis, notable given the company’s early focus on data science and recommendation algorithms.) Prior to this round Dice had raised around $45 million, according to PitchBook estimates.\"\"\"\n",
    "\n",
    "print('== Исходный текст== \\n%s\\n\\n' % sample_text)\n",
    "\n",
    "tokenized_str = nltk.word_tokenize(sample_text)\n",
    "\n",
    "print('== Токенизированный текст==\\n%s' % tokenized_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отфильтруем знаки пунктуации, токены приведём к нижнему регистру"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['softbank', 'vision', 'fund', '2', 'is', 'leading', 'the', 'round', 'a', 'series', 'c', 'with', 'ipod', '“', 'father', '”', 'and', 'nest', 'co-founder', 'tony', 'fadell', 'by', 'way', 'of', 'future', 'shape', 'blisce', 'french', 'entrepreneur', 'xavier', 'niel', 'mirabaud', 'cassius', 'and', 'evolution', '—', 'all', 'previous', 'backers', '—', 'also', 'participating', 'previous', 'investors', 'in', 'the', 'company', 'also', 'include', 'deepmind', 'co-founders', 'mustafa', 'suleyman', 'and', 'demis', 'hassabis', 'notable', 'given', 'the', 'company', '’', 's', 'early', 'focus', 'on', 'data', 'science', 'and', 'recommendation', 'algorithms', 'prior', 'to', 'this', 'round', 'dice', 'had', 'raised', 'around', '45', 'million', 'according', 'to', 'pitchbook', 'estimates']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "tokens = [i.lower() for i in tokenized_str if ( i not in string.punctuation )]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удаляем стоп-слова, список которых для русского языка можно получить как `stop_words = nltk.corpus.stopwords.words('russian')`. Стоп-слова это \"мусорные\" слова которые встречаются чрезычайно часто (в каждом предложении) поэтому не несут в себе никакой информации. Такие слова, вобщем-то, нужны только для красивой речи и поэтому можем их смело удалять из текста. Например, этот список стоп-слов я нагуглил в интернете."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['softbank', 'vision', 'fund', '2', 'leading', 'round', 'series', 'c', 'ipod', '“', 'father', '”', 'nest', 'co-founder', 'tony', 'fadell', 'way', 'future', 'shape', 'blisce', 'french', 'entrepreneur', 'xavier', 'niel', 'mirabaud', 'cassius', 'evolution', '—', 'previous', 'backers', '—', 'also', 'participating', 'previous', 'investors', 'company', 'also', 'include', 'deepmind', 'co-founders', 'mustafa', 'suleyman', 'demis', 'hassabis', 'notable', 'given', 'company', '’', 'early', 'focus', 'data', 'science', 'recommendation', 'algorithms', 'prior', 'round', 'dice', 'raised', 'around', '45', 'million', 'according', 'pitchbook', 'estimates']\n"
     ]
    }
   ],
   "source": [
    "stop_words = [\n",
    "    'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\",\n",
    "    'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers',\n",
    "    'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which',\n",
    "    'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been',\n",
    "    'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if',\n",
    "    'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between',\n",
    "    'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out',\n",
    "    'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why',\n",
    "    'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not',\n",
    "    'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'shold',\n",
    "    \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\",\n",
    "    'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\",\n",
    "    'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\",\n",
    "    'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"\n",
    "]\n",
    "\n",
    "filtered_tokens = [i for i in tokens if ( i not in stop_words )]\n",
    "\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3904, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "3  @sxsw I hope this year's festival isn't as cra...   \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...   \n",
       "\n",
       "  emotion_in_tweet_is_directed_at  \\\n",
       "0                          iPhone   \n",
       "1              iPad or iPhone App   \n",
       "2                            iPad   \n",
       "3              iPad or iPhone App   \n",
       "4                          Google   \n",
       "\n",
       "  is_there_an_emotion_directed_at_a_brand_or_product  \n",
       "0                                   Negative emotion  \n",
       "1                                   Positive emotion  \n",
       "2                                   Positive emotion  \n",
       "3                                   Negative emotion  \n",
       "4                                   Positive emotion  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "import pandas as pd\n",
    "\n",
    "# # дополнительный словарь со знаками пунктуации\n",
    "# nltk.download('punkt', download_dir='.')\n",
    "\n",
    "df = pd.read_csv(os.path.join(root_data_dir, 'brand_tweets.csv'), sep=',', encoding='utf8')\n",
    "# удаляем строки, в которых отсутствует текст твита\n",
    "df.drop(df[df['tweet_text'].isnull()].index, inplace=True)\n",
    "print(df.shape)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуем пайплайн в виде функции, при помощи которой обработаем все текстовые описания. Для каждого описания\n",
    "* проводим токенизацию\n",
    "* удаляем пунктуацию\n",
    "* приводим к нижнему регистру\n",
    "* удаляем стоп-слова\n",
    "\n",
    "\n",
    "Примените процедуру токенизации к файлу brand_tweets.csv\n",
    "\n",
    "Сколько уникальных токенов получилось?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [.@wesley83, i, have, a, 3g, iphone., after, 3...\n",
       "1    [@jessedee, know, about, @fludapp, awesome, ip...\n",
       "2    [@swonderlin, can, not, wait, for, #ipad, 2, a...\n",
       "3    [@sxsw, i, hope, this, year's, festival, isn't...\n",
       "4    [@sxtxstate, great, stuff, on, fri, #sxsw:, ma...\n",
       "Name: tokenized, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_text(raw_text: str):\n",
    "    \"\"\"Функция для токенизации текста\n",
    "\n",
    "    :param raw_text: исходная текстовая строка\n",
    "    \"\"\"\n",
    "    filtered_tokens = []\n",
    "    # -- ВАШ КОД ТУТ --\n",
    "\n",
    "    filtered_tokens = [\n",
    "        i.lower() for i in raw_text.split() if ( i not in string.punctuation )\n",
    "    ]\n",
    "\n",
    "    # TODO: фильтрация стоп-слов\n",
    "    # TODO: удаляем короткие токены меньше трех символов\n",
    "\n",
    "    # -----------------\n",
    "    return filtered_tokens\n",
    "\n",
    "# применяем функцию в датафрейму с помощью метода .apply()\n",
    "tokenized_tweets= df['tweet_text'].apply(tokenize_text)\n",
    "\n",
    "# добавляем новую колонку в исходный датафрейм\n",
    "df = df.assign(\n",
    "    tokenized=tokenized_tweets\n",
    ")\n",
    "\n",
    "df['tokenized'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Следующий шаг - привести слово в нормальную (словарную) форму. Для русского языка можно проводить нормализацию можно с помощью модуля **pyMorphy**\n",
    "\n",
    "```python\n",
    "import pymorphy2\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "parsed_token = morph.parse(word)\n",
    "normal_form = parsed_token[0].normal_form\n",
    "```\n",
    "\n",
    "В силу того, что наши твиты на английском языке, то этап нормализации не слишком актуален.\n",
    "\n",
    "\n",
    "**NOTE**: изучите самостоятельно `jupyter_notebooks/vol_04_deep_dive_00_probability_hw_2_naive_bayes.ipynb` - этот демо-файл показывает, как использовать простые пайплайны препроцессинга текстов для создания наивного байесовского классификатора\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Векторизация текста: Bag of Words\n",
    "\n",
    "\n",
    "Мы умеем подготавливать текст к обработке: приводить слова к начальным формам, разделять текст на токены, удалять \"мусорные\" токены (стоп-слова). Однако, мы знаем, что нейросети работают не с текстом, а с числами. Давайте разбираться, как переводить токены в числа, то есть с тем, как работает векторизация\n",
    "\n",
    "Bag of Words - это способ перейти от набора токенов к численному вектору. Алгоритм векторизации текста по модели BoW:\n",
    "\n",
    "1. определяем количество $N$ различных токенов во всех доступных текста - так называемый \"словарь\"\n",
    "1. присваиваем каждому токену случайный номер от $0$ до $N$\n",
    "1. для каждого документа $i$ формируем вектор размерности $N$ - ставим на позицию $j$ количество вхождений токена с номером $j$, которые содержатся в тексте $i$.\n",
    "\n",
    "Каждый токен мы по сути представляем в виде вектора размерности $N$, который состоит из нулей и всего одной единицы, такое кодирование называется *One-Hot encoding*. А каждый документ это \"сумма\" всех one-hot векторов входящих в него токенов\n",
    "\n",
    "Такой подход хорошо иллюстрируется картинкой:\n",
    "\n",
    "![bow](img/bow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для каждого элемента получаем вектор из нулей и единиц. При этом размерность словаря обычно составляет несколько десятков тысяч, а количество токенов в одном документе несколько десятков - то есть нулей значительно больше, чем единиц - такие данные называются *разреженными*.\n",
    "\n",
    "В таком виде данные уже пригодны для работы с нейросетью или любым другим алгоритмом ML, однако есть несколько довольно простых и полезных вещей, которые мы можем сделать и без нейросетей. Давайте сначала разберем их, а потом вернемся к нейросетям. Такое представление текста позволяет решать интересные задачи - например, находить самые похожие друг на друга тексты. Чтобы как-то формализовать понятие \"схожести\" текстов, вводится понятие *косинусного расстояния* между двумя векторами текстов $a$ и $b$ размерности $N$. С этой метрикой вы [можете познакомиться в Википедии](https://ru.wikipedia.org/wiki/Векторная_модель#Косинусное_сходство ), формула такая для двух векторов $a$ и $b$ с координатами $a_i$ и $b_i$ соответственно:\n",
    "$$\n",
    "\\text{similarity} = \\cos (\\theta) = 1 - \\frac{\\sum_{i=1}^{N}a_ib_i}{\\sqrt{\\sum_{i=1}^{N}(a_i)^2}\\sqrt{\\sum_{i=1}^{N}(b_i)^2}}\n",
    "$$\n",
    "\n",
    "Интуитивное объяснение для простого случая: два документа полностью совпадают, тогда единички в них стоят на одних и тех же местах - расстояние между ними будет нулевым. Если два текста совершенно не пересекаются, то единички будут стоять на разных местах - расстояние в этом случае равно единице. Самостоятельно реализовывать функцию не нужно - есть готовая реализация в [scipy.spatial.distance.cosine](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.spatial.distance.cosine.html)\n",
    "\n",
    "Векторизуем наш корпус (набор текстов) с помощью класса `CountVectorizer()` (то есть превращаем наборы токенов в наборы векторов)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adzhumurat/PycharmProjects/ai_product_engineer/.venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:523: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
       "\twith 64190 stored elements and shape (3904, 10795)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# инициализируем объект, который токенизирует наш текст\n",
    "# в качестве единственного аргимента передаём функцию, которую мы написали в Уроке 2\n",
    "# на разбивает каждый документ на токены\n",
    "vectorizer = CountVectorizer(tokenizer=tokenize_text)\n",
    "# применяем наш объект-токенизатор к датафрейму с твитами\n",
    "document_matrix = vectorizer.fit_transform(df['tweet_text'].values)\n",
    "# результат - матрица, в которой находятся числа, строк в мастрице столько, сколько документов\n",
    "# а столбцов столько, сколько токенов\n",
    "document_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Класс `sklearn.feature_extraction.text.CountVectorizer` реализует алгоритм преобразования массива текстовых документов в разреженную матрицу такую, что\n",
    "\n",
    "* число строк совпадает с количеством документов в исходном датафрейме\n",
    "* количество столбцов совпадает с количеством различных токенов\n",
    "* объект `CountVectorizer()` содержит в себе разные вспомогательные элементы - например, словарь соответствия токена и его номера\n",
    "\n",
    "Полученные вектора можно использовать в алгоритмах второго уровня - например, в задаче классификации отзывов.\n",
    "\n",
    "Пользуясь матрицей, найдем твит, который максимально похож на первый твит из набора\n",
    "\n",
    "Вычисляем попарные схожести между элементами разреженной матрицы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3904, 3904)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "tweet_distance = 1 - pairwise_distances(document_matrix, metric=\"cosine\")\n",
    "\n",
    "tweet_distance.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы получили квадратную матрицy, которая содержит столько строк и столбцов, сколько документов в нашем  корпусе  (наборе текстов)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 827,  929, 1523, ..., 1190, 1103, 2850], shape=(3904,))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "TARGET_INDEX = 827\n",
    "\n",
    "\n",
    "# отсортируем твиты по “похожести” - чем похожее на source_tweet_index,\n",
    "# тем ближе к началу списка sorted_similarity\n",
    "sorted_similarity = np.argsort(-tweet_distance[TARGET_INDEX,:])\n",
    "\n",
    "sorted_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы получили квадратную матрицy, которая содержит столько строк и столбцов, сколько документов в нашем  корпусе  (наборе текстов)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 827,  929, 1523, ..., 1190, 1103, 2850], shape=(3904,))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# отсортируем твиты по “похожести” - чем похожее на source_tweet_index,\n",
    "# тем ближе к началу списка sorted_similarity\n",
    "sorted_similarity = np.argsort(-tweet_distance[TARGET_INDEX,:])\n",
    "\n",
    "sorted_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы получили вектор \"схожестей\", который содержит индексы похожих твитов, расположенных по убыванию схожести. Больше всего твит похож сам на себя, поэтому возьмём индекс второго по схожести элемента (и далее)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Google launching its own social network? Definitely not at ##sxsw. But rumor still has it...{link}  #googlecircles\n",
      "-------------\n",
      "Google is NOT launching any products at #SXSW {link}\n",
      "-------------\n",
      "@mention ‰ЫП@mention Google is NOT launching any products at #SXSW {link}\n",
      "-------------\n",
      "Google to launch social network Google Circles  at some point - but not at SXSW today.  :-(   {link}   #sxsw\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(df.iloc[TARGET_INDEX].tweet_text)\n",
    "print('-------------')\n",
    "print(df.iloc[sorted_similarity[1]].tweet_text)\n",
    "print('-------------')\n",
    "print(df.iloc[sorted_similarity[2]].tweet_text)\n",
    "print('-------------')\n",
    "print(df.iloc[sorted_similarity[3]].tweet_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы получили мощный инструмент для анализа текстов - например, мы случайно нашли дубликат твита\n",
    "\n",
    "Кроме простого подхода, когда мы вычисляем счётчик вхождения токена, можно вычислять более сложную метрику TF-IDF (term frequency - inverse document frequency), которая вычисляется по следующей формуле для токена $t$ и документа $d$:\n",
    "$$\n",
    "\\text{tf-idf}(t,d) = \\text{tf}(t,d)\\cdot\\text{idf}(t)\n",
    "$$\n",
    "\n",
    "Где $\\text{tf}(t,d)$ - элемент матрицы, полученной из `CountVectorizer()`, который мы умножаем на величину $\\text{idf}(t)$.\n",
    "\n",
    "Этот класс тоже реализован в sklearn, его предлагаю использовать в домашней работе\n",
    "```python\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    lowercase=True,\n",
    "    token_pattern=r'\\b[\\w\\d]{3,}\\b',\n",
    "    min_df=0.001\n",
    ")\n",
    "\n",
    "```\n",
    "\n",
    "Эта величина показывает количество документов в корпусе  (наборе текстов), в которых был встречен токен $t$:\n",
    "$$\n",
    "\\text{idf}(t) = \\log\\frac{1+N}{1+\\text{df(t)}} + 1\n",
    "$$\n",
    "\n",
    "где $\\text{df}(t)$ - количество документов корпуса, в которых был встречен токен $t$. Таким образом мы понижаем веса у слов, которые встречаются почти во всех документах - такие токены являются неинформативными и мусорными, алгоритм понижает их \"важность\" для анализа.\n",
    "\n",
    "Алгоритм TF-IDF лучше подходит для анализа текстов и даёт более высокое качество, но более затратен по вычислениям. Как выбрать между этими алгоритмами?\n",
    "\n",
    "* если токенов менее 10000 используйте TF-IDF\n",
    "* если токенов более 10000 то *попробуйте* использовать TF-IDF, если не получится - возвращайтесь к CountVectorizer\n",
    "\n",
    "**Недостатки BoW подхода** Используя алгоритмы вроде Вag of Words, мы теряем порядок слов в тексте, а значит, тексты \"i have no cows\" и \"no, i have cows\" будут идентичными после векторизации, хотя и противоположными семантически. Чтобы избежать этой проблемы, можно сделать шаг назад и изменить подход к токенизации: например, использовать N-граммы (комбинации из N последовательных токенов). Обычно по корпусу  (набору текстов) формируются биграммы (последовательности из двух слов) или триграммы (последовательности из трёх слов)\n",
    "\n",
    "Кроме того, текст можно разбивать не на слова, а на последовательности букв (биграммы, триграммы) - при таком подходе опечатки будут автоматически учитываться.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Другие подходы к векторизации текста: Word2Vec\n",
    "\n",
    "Это более новый алгоритм, чем BoW.\n",
    "\n",
    "Алгоритм предполагает, что мы  уже разбили документы на токены и готовы скормить наши токены нейросети, которая сделает из каждого токена плотный вектор-эмбеддинг.\n",
    "\n",
    "При подходе BoW мы представляем каждый документ в виде разреженного вектора-строки, где размерность вектора соответствует количеству токенов в словаре.\n",
    "\n",
    "Нетрудно заметить, что при таком подходе игнорируется контекст, в котором находится слово. Например, в двух предложениях \"король издал указ\" и  \"правитель издал указ\" слова *король* и *правитель* являются синонимами, потому что используются в одинаковом контекста. Подход *BoW* не сможет уловить отношение синонимии.\n",
    "\n",
    "Эту проблему решает подход `Word Embedding`, при котором каждое слово представляет собой вектор большой размерности (обычно несколько сотен). В отличие от подхода BoW, при котором каждое слово представляет собой разреженный вектор, *word embedding* - это \"плотный\" вектор. Классическим алгоритмом, вычисляющим эмбеддинги (то есть \"плотные\" вектора) слов, является Word2Vec, предложена чешским аспирантом Томашем Миколовым в 2013 году. Эта модель позволяет формировать векторы, которые отражают взаимоотношения между словами: \"король\" относится к \"королеве\" так же как \"женщина\" к \"мужчине\"\n",
    "\n",
    "![word_vectors](img/word_vectors.png)\n",
    "\n",
    "Подход Word2Vec основан на интуитивно понятной гипотезе, которая называется гипотезой локальности — \"слова, которые встречаются в одинаковых окружениях, имеют близкие значения\". Эта гипотеза приводит к двум способам тренировки моделей: *Continious Bag of Words* (когда по контексту предсказываем слово) и *Skip Gram* - когда по слову пытаемся предсказать его контекст. Эмбеддинги, полученные с помощью обоих подходов оказываются идентичными - можно применять любой из них.\n",
    "\n",
    "Пример контекста:\n",
    "\n",
    "*Машинное обучение это* **класс** *методов искусственного интеллекта*\n",
    "\n",
    "Мы видим, что из текста вырезается окно текста, слово в центре окна мы хотим предсказать, используя слова по краям \"окна\" (тот самый *контекст*).\n",
    "\n",
    "На схеме представлены оба подхода:\n",
    "\n",
    "![word2vec](img/word2vec.png)\n",
    "\n",
    "На картинке представлен алгоритм тренировки *W2V* в виде простой нейросети:\n",
    "![w2v_net](img/w2v_net.png)\n",
    "\n",
    "На схеме слева-направо:\n",
    "\n",
    "* Входной вектор $(x_1,\\ldots,x_v)$ - слово из словаря, закодированное One-Hot\n",
    "* $W_{V\\times N}$ - матрица *word input* -  это эмбеддинги, которые мы обучаем\n",
    "* Эмбеддинг слова контекста $(h_1,\\ldots,h_N)$\n",
    "* $W`_{N\\times V}$ - матрица *word output* -  это тоже эмбеддинги но уже другие (они тоже обучаются в процессе)\n",
    "* Выходной вектор $(y_1,\\ldots,y_V)$ - скор для каждого слова из словаря размерности $V$\n",
    "\n",
    "Мы видим два матричных перемножения - на самом деле W2V представляет собой очень простую нейронную сеть прямого распространения, *feed forward*.\n",
    "\n",
    "На схеме видны две матрицы-скрытые слои. На самом деле это эмбеддинги контента, которые мы обучаем, каждая строка - эмбеддинг размерности N. Матрица эмбеддингов размером (ЧИСЛО СЛОВ В СЛОВАРЕ) X (РАЗМЕРНОСТЬ ЭМБЕДДИНГА) в начале обучения инициализируется рандомными числами, которые “превращаются” в осмысленные эмбеддинги, пока сеть обучается методом обратного распространения ошибки\n",
    "\n",
    "На последнем слое мы получаем скоры для каждого слова из словаря. Скор (от англ score) с индексом i - это “уверенность” сети в том, что слово i может быть в контексте слова, которое мы прокидываем через сеть. То есть мы “кормим” сеть контекстом и уменьшаем лосс в случае, когда по контексту правильно удалось распознать слово внутри контекста. Слово с максимальным скором - это предсказание нашей сети. Зная \"истинное\" слово, которое мы предсказываем и то, что предсказала сеть, мы будем \"подкручивать\" веса эмбеддингов таким образом, чтобы лосс уменьшался и начинаем все лучше предсказывать слово по контексту.\n",
    "\n",
    "Ниже показано, как работает, модификация *CBOW* - через нашу \"сеть\" пропускается каждое слово из контекста, мы пытаемся спрогнозировать слово \"внутри\" контекста:\n",
    "\n",
    "![cbow](img/cbow.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "В питоне существует модуль `gensim` который включает в себя библиотеки для обучения W2V.\n",
    "\n",
    "Давайте применим алгоритм CBOW к нашему тексту:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-26 09:13:44,064 : INFO : collecting all words and their counts\n",
      "2025-12-26 09:13:44,065 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2025-12-26 09:13:44,071 : INFO : collected 10795 word types from a corpus of 67165 raw words and 3904 sentences\n",
      "2025-12-26 09:13:44,071 : INFO : Creating a fresh vocabulary\n",
      "2025-12-26 09:13:44,076 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=2 retains 3843 unique words (35.60% of original 10795, drops 6952)', 'datetime': '2025-12-26T09:13:44.076005', 'gensim': '4.4.0', 'python': '3.12.11 (main, Jun  3 2025, 15:41:47) [Clang 17.0.0 (clang-1700.0.13.3)]', 'platform': 'macOS-15.0-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2025-12-26 09:13:44,076 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=2 leaves 60213 word corpus (89.65% of original 67165, drops 6952)', 'datetime': '2025-12-26T09:13:44.076275', 'gensim': '4.4.0', 'python': '3.12.11 (main, Jun  3 2025, 15:41:47) [Clang 17.0.0 (clang-1700.0.13.3)]', 'platform': 'macOS-15.0-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2025-12-26 09:13:44,082 : INFO : deleting the raw counts dictionary of 10795 items\n",
      "2025-12-26 09:13:44,082 : INFO : sample=0.001 downsamples 54 most-common words\n",
      "2025-12-26 09:13:44,082 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 42481.39821289723 word corpus (70.6%% of prior 60213)', 'datetime': '2025-12-26T09:13:44.082617', 'gensim': '4.4.0', 'python': '3.12.11 (main, Jun  3 2025, 15:41:47) [Clang 17.0.0 (clang-1700.0.13.3)]', 'platform': 'macOS-15.0-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2025-12-26 09:13:44,093 : INFO : estimated required memory for 3843 words and 10 dimensions: 2228940 bytes\n",
      "2025-12-26 09:13:44,093 : INFO : resetting layer weights\n",
      "2025-12-26 09:13:44,094 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-12-26T09:13:44.094250', 'gensim': '4.4.0', 'python': '3.12.11 (main, Jun  3 2025, 15:41:47) [Clang 17.0.0 (clang-1700.0.13.3)]', 'platform': 'macOS-15.0-arm64-arm-64bit', 'event': 'build_vocab'}\n",
      "2025-12-26 09:13:44,094 : INFO : Word2Vec lifecycle event {'msg': 'training model with 4 workers on 3843 vocabulary and 10 features, using sg=0 hs=0 sample=0.001 negative=5 window=7 shrink_windows=True', 'datetime': '2025-12-26T09:13:44.094405', 'gensim': '4.4.0', 'python': '3.12.11 (main, Jun  3 2025, 15:41:47) [Clang 17.0.0 (clang-1700.0.13.3)]', 'platform': 'macOS-15.0-arm64-arm-64bit', 'event': 'train'}\n",
      "2025-12-26 09:13:44,107 : INFO : EPOCH 0: training on 67165 raw words (42452 effective words) took 0.0s, 4952837 effective words/s\n",
      "2025-12-26 09:13:44,117 : INFO : EPOCH 1: training on 67165 raw words (42654 effective words) took 0.0s, 5061788 effective words/s\n",
      "2025-12-26 09:13:44,127 : INFO : EPOCH 2: training on 67165 raw words (42600 effective words) took 0.0s, 5002202 effective words/s\n",
      "2025-12-26 09:13:44,136 : INFO : EPOCH 3: training on 67165 raw words (42473 effective words) took 0.0s, 5961960 effective words/s\n",
      "2025-12-26 09:13:44,149 : INFO : EPOCH 4: training on 67165 raw words (42538 effective words) took 0.0s, 4083452 effective words/s\n",
      "2025-12-26 09:13:44,158 : INFO : EPOCH 5: training on 67165 raw words (42542 effective words) took 0.0s, 6299291 effective words/s\n",
      "2025-12-26 09:13:44,166 : INFO : EPOCH 6: training on 67165 raw words (42467 effective words) took 0.0s, 6285665 effective words/s\n",
      "2025-12-26 09:13:44,175 : INFO : EPOCH 7: training on 67165 raw words (42538 effective words) took 0.0s, 6385170 effective words/s\n",
      "2025-12-26 09:13:44,184 : INFO : EPOCH 8: training on 67165 raw words (42404 effective words) took 0.0s, 6347628 effective words/s\n",
      "2025-12-26 09:13:44,193 : INFO : EPOCH 9: training on 67165 raw words (42461 effective words) took 0.0s, 5919869 effective words/s\n",
      "2025-12-26 09:13:44,193 : INFO : Word2Vec lifecycle event {'msg': 'training on 671650 raw words (425129 effective words) took 0.1s, 4307623 effective words/s', 'datetime': '2025-12-26T09:13:44.193305', 'gensim': '4.4.0', 'python': '3.12.11 (main, Jun  3 2025, 15:41:47) [Clang 17.0.0 (clang-1700.0.13.3)]', 'platform': 'macOS-15.0-arm64-arm-64bit', 'event': 'train'}\n",
      "2025-12-26 09:13:44,193 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=3843, vector_size=10, alpha=0.025>', 'datetime': '2025-12-26T09:13:44.193488', 'gensim': '4.4.0', 'python': '3.12.11 (main, Jun  3 2025, 15:41:47) [Clang 17.0.0 (clang-1700.0.13.3)]', 'platform': 'macOS-15.0-arm64-arm-64bit', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "texts = df.tokenized.values\n",
    "\n",
    "model = Word2Vec(texts, vector_size=10, window=7, min_count=2, workers=4, epochs=10, sg=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы обучили эмбеддинги слов. Давайте проверим, какой вектор обучился для слова `android`\n",
    "\n",
    "Мы видим набор цифр - это вектор длины 10. Давайте найдём, какие слова соответствуют максимально похожим векторам\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.24677023 -0.41371143  1.2445815   0.56448156  1.7247869   1.844483\n",
      "  2.8947337   1.9079707  -2.724034   -1.2670892 ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('version,', 0.9756479263305664),\n",
       " ('iphone,', 0.9746665358543396),\n",
       " ('share/gather', 0.9744194149971008),\n",
       " ('spots', 0.9653379917144775),\n",
       " ('chrome', 0.9650068283081055),\n",
       " ('elusive', 0.9646620750427246),\n",
       " ('blackberry', 0.963772177696228),\n",
       " ('our', 0.9613988995552063),\n",
       " ('&amp;', 0.9607152342796326),\n",
       " ('check', 0.9594240784645081)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.wv.get_vector('android'))\n",
    "model.wv.most_similar('android')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Мы видим, что модель обучила похожие вектора для слов `blackberry`, `iphone`  - это всё названия телефонов, то есть модель работает!\n",
    "\n",
    "На основе векторизованных слов можно строить векторное описание целого предложения - такой алгоритм называется `doc2vec`.\n",
    "\n",
    "Вывод: [модель W2V](http://www.1-4-5.net/~dmm/ml/how_does_word2vec_work.pdf) которая позволяет превращать схожие слова в \"близкие\" векторы, ориентируясь на контекст"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Прикладная задача - NER\n",
    "\n",
    "\n",
    "\n",
    "**NOTE**: зарегистрируйтесь на HuggingFace, это классная платформа для публикации моделей, датасетов: [More models](https://huggingface.co/models?library=transformers&sort=trending&search=bert) - тут можно найти открытые модель\n",
    "\n",
    "Важно: используется кеширование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loading started...\n",
      "Model loading finished!\n",
      "SoftBank Vision Fund 2 is leading the round, a Series C, with iPod “father” and Nest co-founder Tony Fadell (by way of Future Shape), Blisce, French entrepreneur Xavier Niel, Mirabaud, Cassius and Evolution — all previous backers — also participating. (Previous investors in the company also include DeepMind co-founders Mustafa Suleyman and Demis Hassabis, notable given the company’s early focus on data science and recommendation algorithms.) Prior to this round Dice had raised around $45 million, according to PitchBook estimates.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'ORG',\n",
       "  'score': np.float32(0.98425573),\n",
       "  'word': 'Soft',\n",
       "  'start': 0,\n",
       "  'end': 4},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': np.float32(0.9850637),\n",
       "  'word': '##B',\n",
       "  'start': 4,\n",
       "  'end': 5},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': np.float32(0.9770343),\n",
       "  'word': '##an',\n",
       "  'start': 5,\n",
       "  'end': 7},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': np.float32(0.8805127),\n",
       "  'word': '##k Vision Fund 2',\n",
       "  'start': 7,\n",
       "  'end': 22},\n",
       " {'entity_group': 'MISC',\n",
       "  'score': np.float32(0.61475396),\n",
       "  'word': 'Series C',\n",
       "  'start': 47,\n",
       "  'end': 55}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import logging\n",
    "\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer, pipeline\n",
    "\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "print('Model loading started...')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"dslim/distilbert-NER\",\n",
    "    cache_dir=os.path.join(root_data_dir, \"models\")\n",
    ")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"dslim/distilbert-NER\",\n",
    "    cache_dir=os.path.join(root_data_dir, \"models\")\n",
    ")\n",
    "\n",
    "ner = pipeline(\n",
    "    'ner', model=model, tokenizer=tokenizer,\n",
    "    aggregation_strategy=\"simple\"\n",
    ")\n",
    "print('Model loading finished!')\n",
    "\n",
    "print(sample_text)\n",
    "print()\n",
    "ner(sample_text)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метрики оценки\n",
    "\n",
    "### BLEU vs ROUGE: в чём разница?\n",
    "\n",
    "**BLEU** (для translation) фокусируется на **precision**:\n",
    "> \"Сколько из сгенерированного правильно?\"\n",
    "\n",
    "**ROUGE** (для summarization) фокусируется на **recall**:\n",
    "> \"Сколько из важного было найдено?\"\n",
    "\n",
    "#### Пример\n",
    "```\n",
    "Reference: \"Apple announced record earnings of $90B\"\n",
    "\n",
    "Candidate A: \"Apple announced record earnings\"\n",
    "BLEU: High ✅ (всё правильно, но короткое → brevity penalty)\n",
    "ROUGE: Low ❌ (пропущена важная инфа \"$90B\")\n",
    "\n",
    "Candidate B: \"Apple Inc. announced very strong record earnings of $90B\"\n",
    "BLEU: Low ❌ (много лишних слов)\n",
    "ROUGE: High ✅ (вся важная информация есть)\n",
    "```\n",
    "\n",
    "**Формулы:**\n",
    "```\n",
    "BLEU = BP × exp(Σ wₙ log pₙ)\n",
    "где pₙ = n-gram precision\n",
    "\n",
    "ROUGE-N Recall = matched n-grams / n-grams в reference\n",
    "```\n",
    "\n",
    "**Типичные значения:**\n",
    "\n",
    "| Метрика | Summarization | Translation |\n",
    "|---------|---------------|-------------|\n",
    "| BLEU | 0.2-0.4 | 0.4-0.6 |\n",
    "| ROUGE-1 | 0.3-0.5 | — |\n",
    "| ROUGE-2 | 0.1-0.25 | — |\n",
    "\n",
    "📚 **Источники:**\n",
    "- [BLEU: a Method for Automatic Evaluation of Machine Translation (Papineni et al., 2002)](https://aclanthology.org/P02-1040.pdf)\n",
    "- [ROUGE: A Package for Automatic Evaluation of Summaries (Lin, 2004)](https://aclanthology.org/W04-1013/)\n",
    "- [BERTScore: Evaluating Text Generation with BERT (Zhang et al., 2020)](https://arxiv.org/abs/1904.09675)\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Perplexity: что это и зачем?\n",
    "\n",
    "### Интуиция\n",
    "\n",
    "**Perplexity** = насколько модель \"удивлена\" правильным ответом\n",
    "```\n",
    "Предложение: \"The cat sat on the ___\"\n",
    "\n",
    "Хорошая модель:\n",
    "  P(mat) = 0.80 → Perplexity ≈ 1.25 ✅ (почти уверена)\n",
    "\n",
    "Плохая модель:\n",
    "  P(mat) = 0.05 → Perplexity = 20 ❌ (сильно удивлена)\n",
    "```\n",
    "\n",
    "### Математика\n",
    "```\n",
    "Perplexity = 2^(Cross-Entropy)\n",
    "           = 2^(-1/N × Σ log₂ P(wᵢ | context))\n",
    "```\n",
    "\n",
    "**Интерпретация:**\n",
    "\n",
    "Perplexity = K означает:\n",
    "> \"Модель в среднем так же не уверена, как если бы выбирала из K равновероятных вариантов\"\n",
    "\n",
    "**Типичные значения:**\n",
    "```\n",
    "Perplexity = 1       → Идеальная (overfitting!)\n",
    "Perplexity = 10-30   → Отличная модель (GPT-3)\n",
    "Perplexity = 50-100  → Хорошая модель\n",
    "Perplexity = 200+    → Плохая модель\n",
    "```\n",
    "\n",
    "📚 **Источники:**\n",
    "- [A Mathematical Theory of Communication (Shannon, 1948)](https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf) — основы энтропии\n",
    "- [Speech and Language Processing (Jurafsky & Martin)](https://web.stanford.edu/~jurafsky/slp3/) — глава 3 про N-gram LM и perplexity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next level: transformers for embeddings evaluation\n",
    "\n",
    "`Transformer` - архитектура которая позволяет генерировать эмбеддинги с помощью нейросетей\n",
    "\n",
    "Архитектура стала следующим большим шагом после LSTM используется повсеместно для обработки последовательностей\n",
    "\n",
    "Рекомендую для просмотра: [Transfromers united](https://www.youtube.com/playlist?list=PLoROMvodv4rNiJRchCzutFw5ItR_Z27CM) от Stanford.\n",
    "\n",
    "```shell\n",
    "hf auth login\n",
    "```\n",
    "\n",
    "For local env run in console\n",
    "\n",
    "```shell\n",
    "SCRIPT=run_ner_pipeline.py make run-script\n",
    "```\n",
    "\n",
    "Помните: трансформеры это не серебрянная пуля, правило 'мусор на входе - мусор на выходе' остаётся актуальным. То есть препроцуссинг никто не отменял"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next next level: ollama embeddings\n",
    "\n",
    "```shell\n",
    "curl http://localhost:11434/api/embeddings \\\n",
    "  -d '{\n",
    "    \"model\": \"granite4:350m\",\n",
    "    \"prompt\": \"Your text to embed goes here\"\n",
    "  }' \\\n",
    "  -H \"Content-Type: application/json\"\n",
    "```\n",
    "\n",
    "python\n",
    "\n",
    "```python\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Endpoint and model configuration\n",
    "url = \"http://localhost:11434/api/embeddings\"\n",
    "data = {\n",
    "    \"model\": \"granite4:350m\",\n",
    "    \"prompt\": \"Your text to embed goes here\"\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=data)\n",
    "embedding = response.json().get(\"embedding\")\n",
    "```\n",
    "\n",
    "## Google AI studio\n",
    "\n",
    "[docs](https://ai.google.dev/gemini-api/docs/embeddings)\n",
    "\n",
    "this is not works in Russia >>>> __try to get Gemini keys at [google AI studio](https://aistudio.google.com/)__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
