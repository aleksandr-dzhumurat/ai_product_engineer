{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z0Wp3AQGfDxy"
   },
   "source": [
    "# Градиентное обучение на примере линейной регрессии\n",
    "\n",
    "## Исходные данные\n",
    "\n",
    "Для этого занятия нам понадобится файл `non_linear.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2XhdLAtqfEKG",
    "outputId": "7cfffc79-9846-4756-a9d2-6ea96732b064"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Содержимое директории /Users/adzhumurat/PycharmProjects/ai_product_engineer/data: ['client_segmentation.csv', 'messages.db', 'labeled_data_corpus.csv', 'chroma', 'content_description.csv']\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)  # гарантируем воспроизводимость\n",
    "\n",
    "run_env = os.getenv('RUN_ENV', 'COLLAB')\n",
    "if run_env == 'COLLAB':\n",
    "  from google.colab import drive\n",
    "  ROOT_DIR = '/content/drive'\n",
    "  drive.mount(ROOT_DIR)\n",
    "  print('Google drive connected')\n",
    "  root_data_dir = os.path.join(ROOT_DIR, 'MyDrive', 'ml_course_data')\n",
    "  sys.path.append(os.path.join(ROOT_DIR, 'MyDrive', 'src'))\n",
    "else:\n",
    "  root_data_dir = os.getenv('DATA_DIR', '/srv/data')\n",
    "\n",
    "if not os.path.exists(root_data_dir):\n",
    "  raise RuntimeError('Отсутствует директория с данными')\n",
    "else:\n",
    "  print('Содержимое директории %s: %s' % (root_data_dir, os.listdir(root_data_dir)[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_nNQLgELvYCw"
   },
   "source": [
    "Прочитаем `non_linear.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "Xo_PT1EFlosg",
    "outputId": "55834641-9beb-4736-b028-a6245459eb5a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_train</th>\n",
       "      <th>y_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.182421</td>\n",
       "      <td>1.860341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.251605</td>\n",
       "      <td>1.878928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.270474</td>\n",
       "      <td>2.430015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.402553</td>\n",
       "      <td>2.327856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.427711</td>\n",
       "      <td>2.203649</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    x_train   y_train\n",
       "5  1.182421  1.860341\n",
       "6  1.251605  1.878928\n",
       "7  1.270474  2.430015\n",
       "8  1.402553  2.327856\n",
       "9  1.427711  2.203649"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "raw_data_file = os.path.join(root_data_dir, 'non_linear.csv')\n",
    "data = pd.read_csv(raw_data_file, sep=',')\n",
    "\n",
    "data = data[\n",
    "            (data.x_train > 1) & (data.x_train < 5)\n",
    "].copy()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l7rxP1_Cvht7"
   },
   "source": [
    "Визуализируем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "t1YNLriavgou",
    "outputId": "1a85b6a8-6e7b-4ddc-e48c-f8c7a1acb26c"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.scatter(\n",
    "    data.x_train, data.y_train,\n",
    "    s=40, c='g', marker='o', alpha=0.8\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CoyqHUCEPeKT"
   },
   "source": [
    "Создадим две отдельные переменные\n",
    "\n",
    "* $y$ для целевой переменной из столбца `y_train`. Удалим из исходного датафрейма с помощью `.drop()`\n",
    "* Все оставшиеся после удаления столбцы - это матрица объекты-признаки $X$\n",
    "\n",
    "Для матрицы объекты-признаки добавляем \"дефолтный\" признак из единиц."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "2t7ZXPhBPcnK"
   },
   "outputs": [],
   "source": [
    "num_examlpes = data.shape[0]\n",
    "num_features = data.shape[0] - 1  # вычитаем единичку т.к. убрали столбец y\n",
    "X = np.zeros((num_examlpes, num_features + 1))  # размерность увеличилась на дефолтный столбец\n",
    "\n",
    "#-------- ВАШ КОД ТУТ -------------------\n",
    "\n",
    "y = np.zeros(num_examlpes)\n",
    "\n",
    "\n",
    "#-----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QA3w5X68b3tN"
   },
   "source": [
    "Создаём отложенную выборку для валидации - см. предыдущий семинар"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "7X2_WHqrb3-D"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GZAjRY0ow4tt"
   },
   "source": [
    "### Аналитическое решение\n",
    "\n",
    "Для начала напишем код для аналитического вычисления коэффициентов линейной регрессии по формуле $\\overline{w} = \\left(X^TX\\right)^{-1}X^T\\overline{y}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "id": "JIRf6h0XmnbC",
    "outputId": "14ce12c2-5623-4377-ba9c-6df1e2dcd6ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Аналитически определённые коэффициенты [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "from numpy.linalg import inv\n",
    "import numpy as np\n",
    "\n",
    "# вычисляем к-ты линейной регрессии\n",
    "w_analytic = np.zeros(num_features)\n",
    "#-------- ВАШ КОД ТУТ -------------------\n",
    "\n",
    "\n",
    "\n",
    "#-----------------------------------------\n",
    "\n",
    "print(f'Аналитически определённые коэффициенты {w_analytic}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SnWsfBHb9XrW"
   },
   "source": [
    "$$\n",
    "y = x_0*2.98 + x*(-0.67)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wu0YN9WjZOQz"
   },
   "source": [
    "Плучите коэффициенты из класса `LinearRegression` - мы уже делали так на прошлом семинаре"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X9FXb6uxZOg4",
    "outputId": "e998ef9e-6880-42f9-e551-0175b308d013"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "#-------- ВАШ КОД ТУТ -------------------\n",
    "\n",
    "\n",
    "\n",
    "#----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DL2lZQjPZJLu"
   },
   "source": [
    "Сравните по евклидовому расстоянию коэффициенты из класса `LinearRegression` и полученные аналитическим способом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6LX9cURsvqsx"
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import inv, norm\n",
    "\n",
    "#-------- ВАШ КОД ТУТ -------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6lYoS48UaNAi"
   },
   "source": [
    "Проверка на правильность - пишем юнит-тест!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ch2c16IbaEpg"
   },
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "\n",
    "class TestNotebook(unittest.TestCase):\n",
    "    def test_task(self):\n",
    "        self.assertAlmostEqual(linalg_norm, 0.000000, places=6)\n",
    "\n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Критика аналитического решения\n",
    "\n",
    "почему не всегда работает - проверим на примере\n",
    "\n",
    "В уроке про варидацию столкнулись с ворненгом **LinAlgWarning** что это за ошибка? Почему она возникает? Как её побороть? На эти вопросы даст ответ градиентный спуск - способ обучения моделей, **не использующий матричное умножение**.\n",
    "\n",
    "## Основные понятия SGD\n",
    "\n",
    "Мы помним, что в случае многомерной линейной регрессии (когда количество фичей $m$) аналитическое решение имеет вид\n",
    "$$\n",
    "\\overline{w} = \\left(X^TX\\right)^{-1}X^T\\overline{y}\n",
    "$$\n",
    "\n",
    "Где $X$ - т.н. матрица объекты-признаки размености *количество объектов* x *количество признаков*.\n",
    "\n",
    "У аналитического решения есть ряд недостатков\n",
    "* вычислительная сложность из-за матричного перемножения $O(n^3)$, где $n$ - размерность матрицы. При увеличении размерности матрицы в 10 раз сложность вычислений увеличивается в $10^3=1000$ раз\n",
    "* **неустойчивость вычислений** - пытаемся найти обратную матрицу, которая может не существовать, в этом случае в алгоритме нахождения обратной матрицы возникает деление на ноль\n",
    "\n",
    "С неустойчивостью вычислений, например, связано предупреждение **LinAlgWarning:** которое мы видели в первом уроке. Пример такой матрицы:\n",
    "$$\n",
    "X^TX =\n",
    "\\left[\n",
    "\\begin{array}{cc}\n",
    "5 & 25 \\\\\n",
    "2 & 10\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "Допустим, хотим вычислить коэффициенты аналитически. Если попытаемся найти обратную матрицу $(X^TX)^{-1}$, мы получим сообщение об ошибке:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# матрица из примера\n",
    "X = np.array([\n",
    "    [5, 25],\n",
    "    [2, 10]\n",
    "])\n",
    "# пытаемся найти обратную\n",
    "np.linalg.inv(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это очень печально - значит, мы не всегда можем применять аналитическую формулу $\\overline{w} = \\left(X^TX\\right)^{-1}X^T\\overline{y}$ для нахождения коэффициентов $\\overline{w}$ Как же быть, если задачу решать все равно надо?\n",
    "\n",
    "Решение этих проблем нашли математики - давайте вычислять коэффициенты линейной регрессии не аналитически, а с помощью приближённых численных методов. Тогда не надо будет перемножать матрицы или находить обратные матрицы. Самый простой и эффективный из этих методов называется методом *градиентного спуска*. Суть метода состоит в обновлении параметров модели $w$ по маленьким шажкам (вместо того, чтобы находить их сразу) - это и есть градиентный спуск.\n",
    "\n",
    "Каждый такой шажок назвается \"итерация\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы знаем, что коэффициенты обучаются при помощи минимизации функции ошибок:\n",
    "\n",
    "$$\n",
    "L(y,w) = \\sum_{i=1}^{N}\\left(y_i - \\hat{y_i}\\right)^2\n",
    "$$\n",
    "\n",
    "Эта функция квадратичная - следовательно, имеет форму параболы. Минимум параболы соответствует минимуму ошибки - давайте как-то понемногу \"подкручивать\" параметры, чтобы по шажкам спуститься в точку, где ошибка будет минимальной - в этой точке и находятся параметры $w$, которые мы ищем. Правила обновления весов должны быть очень простыми и не содержать матричных перемножений\n",
    "\n",
    "![grad_descent_single_measure](img/grad_descent_single_measure.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В трёхмерном случае картинка более красивая - мы движемся как бы ландшафту и хотим найти самую нижнюю точку на этом ландшафте:\n",
    "\n",
    "![grad_descent_single_measure](img/grad_descent_multi_measure.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В в библиотеке sklearn уже есть класс, в котором реализована логика такого путешествия - это класс `sklearn.linear_model.SGDRegressor`. Давайте посмотрим, как он работает на примере нашей выборки из прошлого урока - начнём с загрузки исходных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Trz0jpmbHN04"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# для регрессии\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.spatial import distance\n",
    "%matplotlib inline\n",
    "\n",
    "data = pd.read_csv('data/non_linear.csv', sep=',')\n",
    "data = data[(data.x_train > 1) & (data.x_train < 5)].copy()\n",
    "\n",
    "y = data['y_train'].reshape(-1, 1)  # таргет\n",
    "X = data.drop('y_train', axis=1)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sgAXuGzgcfcG"
   },
   "source": [
    "## Градиентный спуск из sklearn\n",
    "\n",
    "Используем готовую реализацию и функцию [.fit_partial()](https://scikit-learn.org/0.15/modules/scaling_strategies.html#incremental-learning)\n",
    "\n",
    "Для градиентного спуска мы используем готовый класс [sklearn.linear_model.SGDRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html). Класс принимает следующие параметры\n",
    "\n",
    "* `learning_rate='constant'` используем самую простую модификацию спуска из нескольких возможных (см. доументацию)\n",
    "* `eta0=0.009` - шаг градиентного спуска, в формуле мы его обозначали буквой $\\eta$\n",
    "* `fit_intercept=True` - чтобы обучить коэффициент при \"свободном члене\" $w_0$ линейной регрессии (см. лекцию)\n",
    "* `random_state=RANDOM_SEED` - этот параметр встречали ранее в этом модуле, он нужен для воспроизводимости вычислений.\n",
    "\n",
    "Псевдокод решения (нужно закодить самостоятельно, если возникнут сложности - подсмотреть в лекции):\n",
    "\n",
    "* инициализируем `w_current` рандомом, `weight_evolution` и `rmse_evolution` пустыми списками, и критерий остановки $\\varepsilon=0.0001$\n",
    "* задаём максимальное число шагов `800` и далее на каждом шаге\n",
    "  * вызываем `.partial_fit`\n",
    "  * `.coef_` содержит текущие веса - применяем [from scipy.spatial.distance.euclidean](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.euclidean.html#scipy.spatial.distance.euclidean) и сверяем с критерием остановки. Если критерий не выполняется - обновляем rmse с помощью [sklearn.metrics.mean_squared_error](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html) на `y_valid` и предикт на `X_valid` и переназначаем `w_current` на атрибут `.coef_.copy()`\n",
    "\n",
    "Когда критерий остановки выполнился - визуализируем `rmse_evolution`, мы должны увидеть т.н. кривую обучения с помощью `.plot(range(step), rmse_evolution)`\n",
    "\n",
    "Новая библиотека в нашем арсенале  - scipy, изучите её документацию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "id": "WD_RpbQjcjdP",
    "outputId": "f5200b17-f5d7-4a84-c85d-2cd786b49ad6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-24 17:55:49,213 : INFO : Обучение закончилось\n",
      "2022-01-24 17:55:49,216 : INFO : Количество пройденных итераций 160\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f5378c89910>]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAfXUlEQVR4nO3deXRc5Z3m8e+vqrQvlmRJXiQZ2UYGjLENyA57CAmJIcGkmyxm0gmkkybpDFma9ALJTHJC98yZkO5s00wSQsgwWSCGkOCmnQZCk7AEbMtg4w1jeZXkTbYsWdZeVe/8UVd2Wci2bJd0Vbeezzk6de97X1X9zmvXU1fvvXWvOecQEZH0F/K7ABERSQ0FuohIQCjQRUQCQoEuIhIQCnQRkYCI+PXC5eXlrra21q+XFxFJS6tXrz7gnKsYbptvgV5bW0tDQ4NfLy8ikpbMbOeJtmnKRUQkIBToIiIBoUAXEQkIBbqISEAo0EVEAkKBLiISEAp0EZGASLtAb9jRxjf/40102V8RkeOlXaCvb+ngB3/YSuuRPr9LEREZV9Iu0OsmFQHQuO+Iz5WIiIwv6RfolYUAbNmvQBcRSZZ2gV5RlENxboQt+zv9LkVEZFxJu0A3M86tLKRRe+giIsdJu0AHqKssUqCLiAyRnoE+qZADR/pp6+r3uxQRkXEjLQP9XO/AqPbSRUSOSctAHzx1UQdGRUSOSctAnzohl4LssPbQRUSSpGWgmxkzdaaLiMhx0jLQITGP/tY+TbmIiAxK20A/b1IR+w730d6tM11ERCCNA/38KcUAbNqjvXQREUjjQL9gSuJMlzf3Hva5EhGR8WFEgW5mi8xss5k1mtndJ+jzETPbaGYbzOyXqS3z7SoKc5hYkM2mPQp0ERGAyKk6mFkYuB+4HmgGVpnZMufcxqQ+dcA9wJXOuUNmVjlaBSe9JudPKeLNvZpyERGBke2hLwQanXPbnHP9wKPAzUP6/BVwv3PuEIBzbn9qyxzeBZOL2by3k2gsPhYvJyIyro0k0KuApqT1Zq8t2Sxglpm9bGavmtmi4Z7IzO4wswYza2htbT2zipOcP6WYvmicHQe7z/q5RETSXaoOikaAOuBa4Fbgx2ZWMrSTc+4B51y9c66+oqLirF/0/Mk6MCoiMmgkgd4C1CStV3ttyZqBZc65AefcduAtEgE/quomFRIOmQ6MiogwskBfBdSZ2XQzywaWAMuG9Pktib1zzKycxBTMthTWOaycSJiZFQW8qXPRRUROHejOuShwJ/A0sAlY6pzbYGb3mtlir9vTwEEz2wg8D/ydc+7gaBWd7IIpxWzUHrqIyKlPWwRwzi0Hlg9p+1rSsgPu8n7G1IVTi3lyzW7auvopK8ge65cXERk30vabooMunDoBgA27O3yuRETEXwEI9MQ1Xda3aNpFRDJb2gd6SX42VSV52kMXkYyX9oEOMKeqmI27tYcuIpktEIF+4dQJbDvQxZG+qN+liIj4JhCBPqdq8Nro2ksXkcwViEAfPNNlfYvm0UUkcwUi0CuLcigvzGGD5tFFJIMFItDNjDlVxdpDF5GMFohAB5hbNYEt+4/Q0x/zuxQREV8EJ9CrS4jFnc5HF5GMFaBATxwYXdusQBeRzBSYQK8szmVycS7rmtv9LkVExBeBCXRI7KW/oT10EclQgQv0bQe66OgZ8LsUEZExF7BAT9zGVKcvikgmCligJw6MatpFRDJRoAK9JD+baWX5rG3SgVERyTyBCnSA+TUlrFGgi0gGClygXzKthL2He9nT0eN3KSIiYypwgX7xtFIAXt+lvXQRySyBC/QLphSTHQnx+q5DfpciIjKmAhfo2ZEQF1VN4DXtoYtIhglcoANcXFPCupYO+qNxv0sRERkzwQz0aaX0R+O6JZ2IZJSABnriG6OaRxeRTDKiQDezRWa22cwazezuYbbfbmatZrbG+/l06ksduakleUwuzmW15tFFJINETtXBzMLA/cD1QDOwysyWOec2Dun6K+fcnaNQ4xm5tLaU1Tva/C5DRGTMjGQPfSHQ6Jzb5pzrBx4Fbh7dss7egnNK2d3RS0u7vmAkIplhJIFeBTQlrTd7bUPdYmZvmNnjZlYz3BOZ2R1m1mBmDa2trWdQ7sjV15YB0KC9dBHJEKk6KPpvQK1zbi7wLPDwcJ2ccw845+qdc/UVFRUpeunhnT+5iILsMA07dGBURDLDSAK9BUje46722o5yzh10zvV5qw8Cl6amvDMXCYe45JxSVmkPXUQyxEgCfRVQZ2bTzSwbWAIsS+5gZlOSVhcDm1JX4pmrP6eMzfs6OdyrOxiJSPCdMtCdc1HgTuBpEkG91Dm3wczuNbPFXrcvmNkGM1sLfAG4fbQKPh31taU4B6/t1LSLiATfKU9bBHDOLQeWD2n7WtLyPcA9qS3t7M2vKSEcMlbtaOPa8yr9LkdEZFQF8puigwpyIsypmsDK7ZpHF5HgC3SgA1w2o4w1Te309Mf8LkVEZFRlQKBPZCDmdF0XEQm8wAd6/TmlhAxe3XbQ71JEREZV4AO9KDeLi6om8Krm0UUk4AIf6ADvmDGRNbva6R3QPLqIBFdGBPplM8roj8V142gRCbSMCPT62jJCBq9sPeB3KSIioyYjAr04N4v5NSW82KhAF5HgyohAB7iqroK1Te10dOu6LiISTBkT6NfUlRN38CdNu4hIQGVMoM+rKaEoJ6JpFxEJrIwJ9KxwiMtmTuSFt1pxzvldjohIymVMoENi2qX5UA87D3b7XYqISMplVKBfVZe47Z2mXUQkiDIq0Gsn5lNdmseLb43uDapFRPyQUYFuZlxdV8ErWw8SjcX9LkdEJKUyKtABrq4rp7MvytpmXQZARIIl4wL9ipkTCRm88Jbm0UUkWDIu0Evys5lbXcKLWzSPLiLBknGBDolplzVN7XT06DIAIhIcGRroFcQdvLRF0y4iEhwZGeiXTCuhJD+L32/a53cpIiIpk5GBHgmHuO78Sv7zzf0M6PRFEQmIjAx0gPfOnkRHzwANOw75XYqISEpkbKBfXVdBdiTEsxs17SIiwTCiQDezRWa22cwazezuk/S7xcycmdWnrsTRUZAT4cqZE3l2015dfVFEAuGUgW5mYeB+4AZgNnCrmc0epl8R8EVgRaqLHC3Xz55MU1sPm/d1+l2KiMhZG8ke+kKg0Tm3zTnXDzwK3DxMv38Evgn0prC+UfWeCyoBeHaDpl1EJP2NJNCrgKak9Wav7SgzuwSocc79+8meyMzuMLMGM2tobfX/m5qVxbnMrynR6YsiEghnfVDUzELAt4Evn6qvc+4B51y9c66+oqLibF86Ja6fPYm1zR3sO5w2f1iIiAxrJIHeAtQkrVd7bYOKgDnAH8xsB3AZsCwdDoxCItABne0iImlvJIG+Cqgzs+lmlg0sAZYNbnTOdTjnyp1ztc65WuBVYLFzrmFUKk6xuspCzpmYr0AXkbR3ykB3zkWBO4GngU3AUufcBjO718wWj3aBo83MuP6CSbyy9SCdvbpYl4ikrxHNoTvnljvnZjnnZjrn/ofX9jXn3LJh+l6bLnvng264aDL9sbj20kUkrWXsN0WTXTKtlKqSPJat3e13KSIiZ0yBTmLa5aZ5U3lpywHauvr9LkdE5Iwo0D2L500lGncsX7fH71JERM6IAt1zwZQizq0s1LSLiKQtBbrHzFg8byqrdrSxp6PH73JERE6bAj3J4nlTcQ6eWqtpFxFJPwr0JLXlBcytnqBpFxFJSwr0IRbPm8q6lg62H+jyuxQRkdOiQB/iA3OnYgbL1mgvXUTSiwJ9iMkTcllYW8aTa1t0JyMRSSsK9GHcckk121q7eG2XbiAtIulDgT6M98+dQkF2mEdXNp26s4jIOKFAH0ZBToTF86fy1Bt7dAVGEUkbCvQT+OiCafQMxHQKo4ikDQX6CcyrnsD5k4s07SIiaUOBfgJmxpIFNaxr6WB9S4ff5YiInJIC/SQ+eHEV2ZEQSxu0ly4i458C/SRK8rO5cc5kfvN6Cz39Mb/LERE5KQX6KXx0wTQ6e6P8br0u2CUi45sC/RQum1FG7cR8frFil9+liIiclAL9FMyMj19ey+qdh1jb1O53OSIiJ6RAH4GP1FdTmBPhJy9t97sUEZETUqCPQFFuFh9dUMPydXvY3a67GYnI+KRAH6Hbr6gl7hwPv7LD71JERIalQB+hmrJ8Fs2ZzCMrdtHVF/W7HBGRt1Ggn4ZPXTWdw71Rfv1as9+liIi8zYgC3cwWmdlmM2s0s7uH2f5ZM1tnZmvM7CUzm536Uv13ybRS5tWU8NOXdxCP6+YXIjK+nDLQzSwM3A/cAMwGbh0msH/pnLvIOTcfuA/4dsorHQfMjE9fNZ3tB7p47s39fpcjInKckeyhLwQanXPbnHP9wKPAzckdnHOHk1YLgMDuvt4wZzJVJXn84A+NukWdiIwrIwn0KiD56lTNXttxzOy/mtlWEnvoXxjuiczsDjNrMLOG1tbWM6nXd5FwiL++diav7WrnxS0H/C5HROSolB0Udc7d75ybCfwD8N9O0OcB51y9c66+oqIiVS895j5cX82UCbl877kt2ksXkXFjJIHeAtQkrVd7bSfyKPDBsylqvMuJhPncu85l9c5DvNSovXQRGR9GEuirgDozm25m2cASYFlyBzOrS1p9P7AldSWOTx8Z3Ev/vfbSRWR8OGWgO+eiwJ3A08AmYKlzboOZ3Wtmi71ud5rZBjNbA9wF3DZqFY8TOZEwn7t2Jg07D/Fy40G/yxERwfzau6yvr3cNDQ2+vHaq9EVjvPO+P1BTlsfSz1yOmfldkogEnJmtds7VD7dN3xQ9C4m59Jms2qG9dBHxnwL9LH2kvoaqkjz+5/JNxPTtURHxkQL9LOVmhfn7Reexcc9hntA1XkTERwr0FLhp7lTm1ZTwz89sprtfV2IUEX8o0FMgFDL++/svYN/hPn78gu5qJCL+UKCnSH1tGTdeNJkf/nEr+w73+l2OiGQgBXoK/cOi84nG4/zLM5v9LkVEMpACPYXOmVjA7VfU8tjqZta3dPhdjohkGAV6it15XR3lhTnc88Q6orG43+WISAZRoKfYhLwsvn7TbNa1dPDwKzv9LkdEMogCfRS8/6IpvOu8Cv7lmc20tPf4XY6IZAgF+igwM+69eQ7Owdd+u15XYxSRMaFAHyU1Zfncdf0snntzP79bv9fvckQkAyjQR9Enr6zlwqnFfH3ZBtq7+/0uR0QCToE+iiLhEN+8ZS6Huvr5ym/WaepFREaVAn2UzamawF3vncXydXv59Wsnu3OfiMjZUaCPgc9cM5OF08v4+pPr2XWw2+9yRCSgFOhjIBwyvvPR+YRCxpd+9bq+cCQio0KBPkaqSvL4pw/O4bVd7fzr841+lyMiAaRAH0M3z6/izy6u4vvPbeHFLa1+lyMiAaNAH2P/9ME51FUW8flHXqepTfPpIpI6CvQxVpAT4Ucfv5RY3PHZn6+mdyDmd0kiEhAKdB/UlhfwvSXz2bjnMF95Queni0hqKNB9ct35k/jSu2fxxOst/N8/7fC7HBEJAAW6jz5/3blcP3sS//jURp7ZoOu9iMjZUaD7KBQyvrdkPhdVl/D5R15n9c5DfpckImlsRIFuZovMbLOZNZrZ3cNsv8vMNprZG2b2nJmdk/pSgyk/O8JDt9UzZUIun354FVtbj/hdkoikqVMGupmFgfuBG4DZwK1mNntIt9eBeufcXOBx4L5UFxpkEwtzePgvFxIy47aHVrK/s9fvkkQkDY1kD30h0Oic2+ac6wceBW5O7uCce945N3hS9atAdWrLDL5zJhbw0O0LOHikn48/uJKDR/r8LklE0sxIAr0KaEpab/baTuRTwO+G22Bmd5hZg5k1tLbqm5JDzasp4Se31bOzrYuPPbhCoS4ipyWlB0XN7C+AeuBbw213zj3gnKt3ztVXVFSk8qUD44pzy/nJbQvYfiAR6m1dujGGiIzMSAK9BahJWq/22o5jZu8Bvgosds5p1/IsXKlQF5EzMJJAXwXUmdl0M8sGlgDLkjuY2cXAj0iE+f7Ul5l5rqor58efqGdb6xE+9MM/0XxI130RkZM7ZaA756LAncDTwCZgqXNug5nda2aLvW7fAgqBx8xsjZktO8HTyWm4ZlYFP//0OzjQ2cef/58/sWnPYb9LEpFxzPy6jkh9fb1raGjw5bXTzea9ndz20Eq6+qP8+BP1XDZjot8liYhPzGy1c65+uG36pmgaOG9yEb/+3BVMKs7lEw+t5Nerm/0uSUTGIQV6mqgqyePxz17OpdNK+fJja7n33zbqVnYichwFehopyc/m/31qIZ+8spaHXt7OJx5aySGdASMiHgV6mskKh/j6TRfyrQ/NpWHnIW7615d4bZcu6iUiCvS09eH6GpZ+5nKcgw//8BXuf76RWFw3yhDJZAr0NDa/poTlX7yaG+ZM5ltPb+YvHlzB3g5d2EskUynQ09yEvCz+960Xc98tc1nT1M77vvsCj69u1m3tRDKQAj0AzIyPLKjh379wFXWVhfztY2u57aer9O1SkQyjQA+QGRWFLP3M5Xxj8YU07Gjjfd95gYde2s6ATm8UyQgK9IAJhYzbrqjlmb+5hktry7j3qY28//sv8nLjAb9LE5FRpkAPqOrSfB7+5AJ+9PFL6RmI8bEHV/DZn61m10FNw4gEVcTvAmT0mBnvu3Ay75xVwYMvbuP+57fy+037uHXhND5/3blUFuf6XaKIpJAuzpVB9h3u5fvPbeFXq5qIhI1PXjmdO66eQWlBtt+licgIneziXAr0DLTzYBffefYtnly7m7ysMLcunMZfXT2DyRO0xy4y3inQZVhb9nXygz9u5ck1uwkZ/PnF1Xzq6unMmlTkd2kicgIKdDmpprZuHnhhG0sbmuiLxrny3IncfsV0rju/knDI/C5PRJIo0GVE2rr6eXTVLn72yk72dPRSXZrHhy6t5pZLqqkpy/e7PBFBgS6nKRqL8/SGfTyychcvbz2Ac3DFzIl8uL6aRRdOIS877HeJIhlLgS5nrPlQN0+81sLjq5vZ1dZNYU6ERXMmc+NFk7ny3HJyIgp3kbGkQJez5pxj5fY2HlvdzNMb9tLZG6UoJ8K7L6hk0ZwpXHteBblZCneR0aZAl5Tqj8Z5eesBfrduD89s3Ed79wD52WGuPLecd86q4J2zKjTnLjJKFOgyagZicVZsa+M/Nuzh+TdbaWnvAWBmRQHvnFXJNbPKWVBbRkGOvpQskgoKdBkTzjm2Hejij5tb+eNbrby67SB90TjhkDGnagKXTS9j4fQy6mvLmJCX5Xe5ImlJgS6+6B2IsWpHGyu2tbFyextrmtrpj8Uxg/MmFTGvuoS5NROYV13CeZOLyArrWnEip6JAl3GhdyDGmqZ2Vm5vo2HnId5obqe9ewCA7EiI2VOKmVc9gbnVJVxYVcyM8kKyIwp5kWQKdBmXnHM0tfWwtrmdN5rbWdvcwfqWDrr7YwBEQsaMigJmTSrivElFzJqceKwpy9c3WCVjnSzQR3SkyswWAd8DwsCDzrn/NWT7NcB3gbnAEufc42dXsmQCM2PaxHymTcznpnlTAYjFHVtbj7Bpz2He2tfJ5r1HeKO5g6fe2HP093KzQtROLKB2YgHnlOczfWIBteUFTC8voLIoBzOFvWSmUwa6mYWB+4HrgWZglZktc85tTOq2C7gd+NvRKFIyRzhkzJpU9LYLhHX1RWncf4TNezt5a18nOw52sWV/J8+9uY+B2LG/MvOywpwzMZ+qkjymHv3Jpbo0sVxZlKu9ewmskeyhLwQanXPbAMzsUeBm4GigO+d2eNt080oZFQU5EebVlDCvpuS49ljcsbu9h+0HuthxsIvtB7poauumpb2Xhp2H6OgZOK5/JGRMKs6lqjSPScW5VBTmUFmcc+yxKLFcmp9NSMEvaWYkgV4FNCWtNwPvOJMXM7M7gDsApk2bdiZPIXKccMioKcunpiyfa6h42/YjfVH2tPfQ0t7D7vZednvLLe09rG/pYP/hXrq8OftkkZBRXpgI+JL8LMoKsinNzz66XJKfTWl+FqX52ZQWJJbzssKa7hFfjem3PZxzDwAPQOKg6Fi+tmSmwpwIdZOKqDvJNd67+qK0dvbReqSP1s4+9h/upfVIH/sP93HgSB+HugfY1dbNoa5+DvdGT/g8OZEQxXlZFOVGKMrNojg3kljOyaJwcDk3sb04abkwJ0J+doS87DD52WGdvilnbCSB3gLUJK1Xe20igVCQE6EgJ0JtecEp+0Zjcdp7Bmjv7qeta4BD3f0c6urnUHdi+XDPAJ29UTr7onT2DrCno5fO3kRb9zB/CQwnK2zkZYXJz46Qnx0mLztMQVLgDz7mZ0fIywqTmxUmJxIiOxIiJxIix1tP/ISPtudmJdaHtmtqKThGEuirgDozm04iyJcA/2VUqxIZpyLhEOWFOZQX5pz270ZjcY70RensjXLYC/nO3ihdfYmw7+6P0tMfo3sgRk9/LNHuLXf3R2nv7md3e4zu/hg9A4m23oGzP2yVFbajQR8JG5FQ4sMhEjIi4RBZYTu6nB0+1icrfGx7VijRnhUOJfVNPEa87eGQEQ4ZoZARNiMcgpDZsXZLvM6x7ceWQyGOa4t4/ZN/N5zcN+k5wiHDMMx7PcN7NDBLLA+2J9rS9wPulIHunIua2Z3A0yROW3zIObfBzO4FGpxzy8xsAfAboBS4ycy+4Zy7cFQrF0kzkXCIkvzE/HuqxOOO/licvmicvmiMvoHEcv/getTbNhBLak/aNpBY7o/G6Y3GiMYcAzHHQCxONB5nIOaIxuJH23oGYkR740l9vEdvfbAtGkvUlY7MOBr6IW8lZMN9GNix9qR1w3v02kP29vUvvrvu6Km6qTSiOXTn3HJg+ZC2ryUtryIxFSMiYygUMnJDYe/SxePr+jjOOWJxdzT0Y/HEesw54nG8x+S2RN9Y3BF3yY8c1xZzjljs2O/EhvRNbhv8cV49zkHcJdbj3rpzjrh7+7rjWDveY9xx7DmSnmewPfG7g6/DMH0S20brWka6BJ6IjAozS0zPhNG18seIDqeLiASEAl1EJCAU6CIiAaFAFxEJCAW6iEhAKNBFRAJCgS4iEhAKdBGRgPDtFnRm1grsPMNfLwcOpLCcVBqvtamu0zNe64LxW5vqOn1nUts5zrm3XysaHwP9bJhZw4nuqee38Vqb6jo947UuGL+1qa7Tl+raNOUiIhIQCnQRkYBI10B/wO8CTmK81qa6Ts94rQvGb22q6/SltLa0nEMXEZG3S9c9dBERGUKBLiISEGkX6Ga2yMw2m1mjmd3tYx01Zva8mW00sw1m9kWvvczMnjWzLd5jqU/1hc3sdTN7ylufbmYrvHH7lZml7j5op1dXiZk9bmZvmtkmM7t8PIyZmf2N9++43sweMbNcP8bMzB4ys/1mtj6pbdjxsYTve/W9YWaX+FDbt7x/yzfM7DdmVpK07R6vts1m9r6xrCtp25fNzJlZubc+ZmN2orrM7PPemG0ws/uS2s9+vNzgrZTS4IfEPU23AjOAbGAtMNunWqYAl3jLRcBbwGzgPuBur/1u4Js+1XcX8EvgKW99KbDEW/4h8Nc+1fUw8GlvORso8XvMgCpgO5CXNFa3+zFmwDXAJcD6pLZhxwe4EfgdiVtgXgas8KG29wIRb/mbSbXN9t6fOcB0730bHqu6vPYaEvdC3gmUj/WYnWC83gX8Hsjx1itTOV5j9qZJ0QBdDjydtH4PcI/fdXm1PAlcD2wGpnhtU4DNPtRSDTwHXAc85f3nPZD0xjtuHMewrglecNqQdl/HzAv0JqCMxG0ZnwLe59eYAbVDQmDY8QF+BNw6XL+xqm3Itj8DfuEtH/fe9IL18rGsC3gcmAfsSAr0MR2zYf4tlwLvGaZfSsYr3aZcBt94g5q9Nl+ZWS1wMbACmOSc2+Nt2gtM8qGk7wJ/Dwzedn0i0O6ci3rrfo3bdKAV+Kk3HfSgmRXg85g551qAfwZ2AXuADmA142PM4MTjM97eD39JYu8XfK7NzG4GWpxza4ds8nvMZgFXe1N5fzSzBamsK90Cfdwxs0Lg18CXnHOHk7e5xEftmJ4XamYfAPY751aP5euOUITEn6A/cM5dDHSRmEI4yqcxKwVuJvGBMxUoABaNZQ0j5cf4jISZfRWIAr8YB7XkA18BvuZ3LcOIkPhL8DLg74ClZmapevJ0C/QWEvNig6q9Nl+YWRaJMP+Fc+4Jr3mfmU3xtk8B9o9xWVcCi81sB/AoiWmX7wElZhbx+vg1bs1As3Nuhbf+OImA93vM3gNsd861OucGgCdIjON4GDM48fiMi/eDmd0OfAD4mPeBA/7WNpPEh/Na731QDbxmZpN9rgsS74EnXMJKEn9Fl6eqrnQL9FVAnXf2QTawBFjmRyHep+pPgE3OuW8nbVoG3OYt30Zibn3MOOfucc5VO+dqSYzPfzrnPgY8D3zIr7q82vYCTWZ2ntf0bmAjPo8ZiamWy8ws3/t3HazL9zHznGh8lgGf8M7cuAzoSJqaGRNmtojE9N5i51x30qZlwBIzyzGz6UAdsHIsanLOrXPOVTrnar33QTOJExj24v+Y/ZbEgVHMbBaJEwMOkKrxGq2DAaN4kOFGEmeUbAW+6mMdV5H40/cNYI33cyOJ+erngC0kjmaX+VjjtRw7y2WG9x+kEXgM7yi7DzXNBxq8cfstUDoexgz4BvAmsB74GYmzDcZ8zIBHSMzjD5AIok+daHxIHOy+33svrAPqfaitkcTc7+B74IdJ/b/q1bYZuGEs6xqyfQfHDoqO2ZidYLyygZ97/89eA65L5Xjpq/8iIgGRblMuIiJyAgp0EZGAUKCLiASEAl1EJCAU6CIiAaFAFxEJCAW6iEhA/H9jlVXh3mfRtAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "from scipy.spatial.distance import euclidean\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# можно установить None для эксперимента\n",
    "rand_state = 42\n",
    "\n",
    "sgd_regressor = SGDRegressor(\n",
    "    learning_rate='constant',\n",
    "    eta0=0.009,\n",
    "    fit_intercept=True,\n",
    "    random_state=rand_state\n",
    ")\n",
    "\n",
    "# инициализация весов случайным образом\n",
    "w_current = np.random.random(2)\n",
    "epsilon = 0.0001\n",
    "\n",
    "# изменения весов и ошибка на валидации\n",
    "weight_evolution, rmse_evolution = [], []\n",
    "\n",
    "for step in list(range(800)):\n",
    "    # шаг градиентного спуска\n",
    "    sgd_regressor = sgd_regressor.partial_fit(X_train, y_train)\n",
    "    # отслеживаем изменения весов\n",
    "    weight_evolution.append(\n",
    "        distance.euclidean(w_current, sgd_regressor.coef_)\n",
    "    )\n",
    "    # проверяем критерий остановки\n",
    "    if weight_evolution[-1] < epsilon:\n",
    "        print(\"Итарации остановлены на шаге %d\" % step); break\n",
    "    rmse_evolution.append(\n",
    "        mean_squared_error(y_valid, sgd_regressor.predict(X_valid))\n",
    "    )\n",
    "    # обновление весов регрессии\n",
    "    w_current = sgd_regressor.coef_.copy()\n",
    "plt.plot(range(step), rmse_evolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H4FbEsKOe_hi"
   },
   "source": [
    "Визуализируйте решение на графике\n",
    "\n",
    "Примерный результат который следует ожидать\n",
    "\n",
    "![fitteg_reg](img/linear_regression_fitted.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Coy40Ffbe_2b"
   },
   "outputs": [],
   "source": [
    "x_linspace = np.linspace(data['x_train'].min(), data['x_train'].max(), num=100)\n",
    "\n",
    "y_linspace= sgd_regressor.predict(x_linspace.reshape(-1,1))\n",
    "\n",
    "plt.plot(x_linspace, y_linspace)\n",
    "plt.scatter(data.x_train, data.y_train, 40, 'g', 'o', alpha=0.8, label='data')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Практическое задание** очевидно, что чем больше шаг градиентного спуска (параметр *eta0* класса *SGDRegressor*), тем быстрее мы придём к оптимальным значениям. Используя под выше, поиграйтесь с параметром *eta0* и добейтесь , чтобы градиентный спуск закончился быстрее, чем за 200 шагов.\n",
    "\n",
    "Сколько шагов у вас получилось? Какое качество *RMSE* у Вашего решения?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------- ВАШ КОД ТУТ -------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5qohl7GmfcLD"
   },
   "source": [
    "Готово! Мы получили решение задачи линейной регрессии, обучив параметры линейной регресии небольшими \"шажками\". Мы не использовали сложных матричных вычислений - тем не менее получили линию регрессии, которая лежит в середине облака точек. Когда стоит использовать градиентный спуск в реальных задачах?\n",
    "\n",
    "* когда данных очень много - в этом случае компьютер может не справится с перемножением матриц\n",
    "* когда нужно контролировать точность обучения - остановить итерации можно в любой момент (не дожидаясь, пока дойдем до \"идеальных\" значений.\n",
    "\n",
    "Когда не стоит применять градиентный спуск? Когда данных мало - в этом случае лучше воспользоваться точным решением"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UApF2WKHeq3z"
   },
   "source": [
    "**Практическое задание** очевидно, что чем больше шаг градиентного спуска (параметр *eta0* класса *SGDRegressor*), тем быстрее мы придём к оптимальным значениям. Используя под выше, поиграйтесь с параметром *eta0* и добейтесь , чтобы градиентный спуск закончился быстрее, чем за 200 шагов.\n",
    "\n",
    "Сколько шагов у вас получилось? Какое качество *RMSE* у Вашего решения на валидационной выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3X9HVuNUerDM"
   },
   "outputs": [],
   "source": [
    "#-------- ВАШ КОД ТУТ -------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tNrWHNTUe8Kt"
   },
   "source": [
    "## Реализация GD на python\n",
    "\n",
    "\n",
    "\"Маленькие шажки\", которыми мы двигаемся к оптимальному решению в виде формулы выглядят следующим образом:\n",
    "$$\n",
    "w^{k+1} = w^k - \\eta\\nabla L(w)\n",
    "$$\n",
    "Переменная $\\eta$ в формуле - т.н. *шаг градиентного спуска*.\n",
    "\n",
    "Где $\\nabla L(w)$ - вектор градиента функции. Этот вектор обладает следующими свойствами:\n",
    "\n",
    "* имеет размерность вектора параметров. Если два параметра $[w_1,w_0]$ - в векторе будет два элемента\n",
    "* элемент градиента под номером $i$ - это частная производная (вспоминаем математику за 11 класс и [смотрим в Википедию](https://ru.wikipedia.org/wiki/Производная_функции ) ) функции потерь $L(y, w)$ по параметру $w_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вектор антиградиента всегда направлен в сторону уменьшения функции - в этом и есть всё волшебство! Мы будем двигаться в сторону минимума функции ошибки, потому что знаем как туда попасть - надо следовать по антиградиенту.\n",
    "\n",
    "На картинке одномерный случай. Синяя стрелка - градиент, красная - антиградиент. Видно, что если двигаться по вектору антиградиента, то свалимся в минимум функции за конечное число шагов\n",
    "![grad_vector](img/grad_vector.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритм визуально выглядит довольно интуитивно - (1) у тебя есть вектор-стрелочка (2) шагай по стрелочке, пока не попадёшь на дно \"оврага\", который представляет собой целевую функцию . В трёхмерном случае оптимальное значение функции находится в центре концентрицеских эллипсов (эллипс - проекция трёхмерной фигуры функции потерь на плоскость):\n",
    "\n",
    "![grad_descent](img/grad_descent_intuit.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как туда добраться, двигаясь маленькими шажками?\n",
    "\n",
    "1. Стартуем алгоритм в случайной начальной точке $x^0$  \n",
    "1. Вычисляем направление антиградиента $-f'(x^0)$ (буквально: производная со знаком \"минус\")\n",
    "1. Перемещаемся по направлению градиента в точку $x^1 = x^0 - \\eta f'(x^0)$\n",
    "1. Повторяем шаги (1-3) для попадания в точку $x^2$\n",
    "1. $\\ldots$\n",
    "1. Profit! Достигли оптимальной точки $x^*$\n",
    "\n",
    "Алгоритм выше - универсальный, он позволяет найти точку минимума любой функции $f(x)$. А как же нам найти минимум функции качества линейной регрессии $L$?\n",
    "\n",
    "$$\n",
    "L(y,w) = \\sum_{i=1}^{N}\\left(y_i - \\hat{y_i}\\right)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы реализовать алгоритм градиентного спуска, выпишем частные прозводные функции качества линейной регрессии $L$ для параметров $\\overline{w} = [w_1,\\ldots,w_m]$ в простейшем случае $n=1$, то есть для одного обучающего примера (одного наблюдения):\n",
    "$$\n",
    "\\left\\{\n",
    "\\begin{array}{cc}\n",
    "\\frac{\\partial L}{\\partial w_0} = 2\\cdot(-1)\\cdot1\\cdot (y_1 - (w_0x_0^1 + \\ldots+w_mx_m^1)) &\\\\\n",
    "\\frac{\\partial L}{\\partial w_k} = 2\\cdot(-1)\\cdot x_1^1 \\cdot (y_1 - (w_0x_0^1 + \\ldots+w_mx_m^1)) &  k\\neq 0\\\\\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "В формуле все обозначения вам известны\n",
    "\n",
    "* $w_0, \\ldots, w_m$ - коэффициенты линейной регрессиии $m$ - количество фичей\n",
    "* $x_0, \\ldots, x_m$ - фичи."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эту формулу с частными производными можно легко обобщить на случай, когда в обучающей выборке не один объект, а $n$ - просто добавляем сумму по всем объектам от 1 до $n$:\n",
    "$$\n",
    "\\left\\{\n",
    "\\begin{array}{cc}\n",
    "\\frac{\\partial L}{\\partial w_0} = \\frac{2}{n}\\cdot(-1)\\cdot \\sum_{i=1}^{n} 1\\cdot \\left(y_i - \\sum_{j=1}^{m}w_jx_j^i\\right) &\\\\\n",
    "\\frac{\\partial L}{\\partial w_k} = \\frac{2}{n}\\cdot(-1)\\cdot \\sum_{i=1}^{n} x_k^i \\cdot\\left(y_i - \\sum_{j=1}^{m}w_jx_j^i\\right) & k\\neq 0 \\\\\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "Что значат переменные в этой формуле?\n",
    "* $n$ - число точек в обучающей выборке\n",
    "* $m$ - число фичей в датасете\n",
    "* $k$ - номер коэффициента в списке параметров линейной регрессии $w = [w_0,\\ldots,w_m]$\n",
    "* $y_i$ - значение целевой переменной на объекте обучающей выборки под номером $i$\n",
    "* $x_j^i$ - значение фичи под номером $j$ на объекте обучающей выборки под номером $i$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритм градиентного спуска следующий:\n",
    "\n",
    "1. Вычислить градиент $\\nabla L(w)$\n",
    "1. Вычислить новый вектор  $w^{k+1} = w^k - \\eta\\nabla L(w)$\n",
    "1. Повторять пункты до тех пор, пока $w^{k+1}$ и $w^{k}$ не сойдутся вместе, то есть вектор весов перестанет обновляться\n",
    "\n",
    "Чтобы лучше понять этот метод, давайте реализуем его \"с нуля\" на языке Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xHX2RxBNe7Ei"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gradient(X, y, w, alpha=0) -> np.array:\n",
    "    \"\"\"Вычисляем градиент в точке\"\"\"\n",
    "    # количество обучающих примеров в выборке\n",
    "    n = X.shape[0]\n",
    "    # считаем прогноз\n",
    "    y_hat = X.dot(w.T)\n",
    "    # вычисляем ошибку прогноза\n",
    "    error = y - y_hat\n",
    "    # дальше pointwise перемножение - умножаем каждую из координат на ошибку\n",
    "    pointwise_errors = np.multiply(X, error) + X\n",
    "    # вычисляем градиент и ошибку\n",
    "    grad = pointwise_errors.sum(axis=0)*(-1.0)*2.0 / n\n",
    "    return grad, error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pd_ZB5TBjwhg"
   },
   "source": [
    "Делаем шаг градиентного спуска"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IM40X0QPe1Yl"
   },
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "def eval_w_next(X, y, eta, w_current):\n",
    "    \"\"\"Делаем шаг градиентного спуска\"\"\"\n",
    "    # вычисляем градиент\n",
    "    grad, error = gradient(X, y, w_current)\n",
    "    # делаем шаг градиентного спуска\n",
    "    w_next = w_current - eta*grad\n",
    "    # проверяем условие сходимости\n",
    "    weight_evolution = distance.euclidean(w_current, w_next)\n",
    "    return (w_next, weight_evolution, grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W6-FWVBVlBZ4"
   },
   "source": [
    "Повторяем шаги (1,2) до сходимости"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vxuc9S74lB8_"
   },
   "outputs": [],
   "source": [
    "def gradient_descent(X: np.array, y: np.array, eta=0.01, epsilon=0.001) -> np.array:\n",
    "    m = X.shape[1] # количество фичей\n",
    "    # инициализируем рандомом веса\n",
    "    w = np.random.random(m).reshape(1, -1)\n",
    "    w_next, weight_evolution, grad = eval_w_next(X, y, eta, w)\n",
    "    step = 0\n",
    "    # повторяем до сходимости вектора весов\n",
    "    while weight_evolution > epsilon:\n",
    "        w = w_next\n",
    "        w_next, weight_evolution, grad = eval_w_next(X, y, eta, w)\n",
    "        step += 1\n",
    "        if step % 100 ==0:\n",
    "            logger.info(\"step %s |w-w_next|=%.5f, grad=%s\", step, weight_evolution, grad)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4KC41Nu-lRQM"
   },
   "source": [
    "Запускаем обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "06DCGLtBlRce"
   },
   "outputs": [],
   "source": [
    "# трансформируем плоский массив X в вектор-столбец\n",
    "X = data['x_train'].values.reshape(-1, 1)\n",
    "n = X.shape[0]\n",
    "# добавляем тривиальный признак w_0, столбец из единиц. См. прошлый урок, почему так\n",
    "X = np.hstack([\n",
    "    np.ones(n).reshape(-1,1),\n",
    "    X\n",
    "])\n",
    "w = gradient_descent(\n",
    "    X,\n",
    "    data['y_train'].values.reshape(-1, 1),\n",
    "    eta=0.008\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "102VDJxOlaSn"
   },
   "source": [
    "У нас произошло несколько сотен итераций, на каждой итерации мы:\n",
    "* вычисляем вектор весов\n",
    "* смотрим расстояние между новым вектором весов и векторов весов с предыдущего шага\n",
    "* если изменения в векторе весов небольшие (скажем, четвёртый знак поcле запятой) - останавливаем итерации\n",
    "\n",
    "Когда вектор перестаёт меняться - говорят, что алгоритм \"сошёлся\" (имеется в виду сходимость к оптимальной точке) - это значит, что итерации можно останавливать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HWCXrJmila2v"
   },
   "outputs": [],
   "source": [
    "support = np.linspace(X.min(), X.max(), num=100)\n",
    "# делаем предикт - считаем предсказания модели в каждой точке обучающей выборке в виде y=X*w\n",
    "y_hat = np.hstack([\n",
    "    np.ones(support.size).reshape(-1, 1),\n",
    "    support.reshape(-1, 1)\n",
    "]).dot(w.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PghJGwPEllda"
   },
   "source": [
    "визуализируем результаты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c_bTZVbBld8D"
   },
   "outputs": [],
   "source": [
    "# строим график\n",
    "plt.plot(support, y_hat, 'b--', alpha=0.5, label='manifold')\n",
    "plt.scatter(data['x_train'], data['y_train'], 40, 'g', 'o', alpha=0.8, label='data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Особенности градиентного спуска**\n",
    "\n",
    "1. Нужно подбирать параметр $\\eta$. Еcли выбрать параметр слишком малым, то обучение регрессии будет происходить слишком медленно. Если слишком большим - вычисления не сойдутся к оптимуму. Вариант решения - адаптивный выбор величины шага\n",
    "1. Долгие вычисления, если размер выборки $n$ становится большим. В этом случае мы можем вычислять градиент не по всей выборке за один шаг, а по одному случайному элементу выборки - в этом случае вычислений значительно меньше.\n",
    "\n",
    "Кроме того, градиент можно считать не только по одному объекту, но и по случайной подвыборке (батчу). Такая модификация алгоритма называется градиентным спуском по мини-батчам.\n",
    "\n",
    "\n",
    "Хорошая теория [тут](http://www.machinelearning.ru/wiki/images/6/68/voron-ML-Lin.pdf). Неплохая статья с разбором [формул обновления весов](https://medium.com/@lachlanmiller_52885/machine-learning-week-1-cost-function-gradient-descent-and-univariate-linear-regression-8f5fe69815fd). Видео от [специализации ШАД на coursera](https://ru.coursera.org/lecture/supervised-learning/gradiientnyi-spusk-dlia-linieinoi-rieghriessii-adARX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
